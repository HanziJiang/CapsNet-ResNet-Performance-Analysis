{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CapsNet.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "zKPXAofhnYhK"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PiQpTflFkuqi"
      },
      "source": [
        "# Capsule Neural Network (CapsNet) Implementation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QDxJ_A_0Gnt9"
      },
      "source": [
        "Implementation of the paper [Dynamic Routing Between Capsules](https://arxiv.org/pdf/1710.09829.pdf) by Sara Sabour, Nicholas Frosst, and Geoffrey E. Hinton. Used [jindongwang/Pytorch-CapsuleNet](https://github.com/jindongwang/Pytorch-CapsuleNet) and [laubonghaudoi/CapsNet_guide_PyTorch](https://github.com/laubonghaudoi/CapsNet_guide_PyTorch) to clarify some confusions, and borrowed some code."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_pkx4McDDItG"
      },
      "source": [
        "## Setup PyTorch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "peSdN_P8CElk",
        "outputId": "c8ef56c4-15e9-4bdf-dd89-adc9f8f9508d"
      },
      "source": [
        "!pip install torch torchvision\n",
        "!pip install matplotlib\n",
        "\n",
        "%mkdir -p /content/project/\n",
        "%cd /content/project/"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (1.8.1+cu101)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (0.9.1+cu101)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch) (1.19.5)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch) (3.7.4.3)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.7/dist-packages (from torchvision) (7.1.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (3.2.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (1.3.1)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (2.4.7)\n",
            "Requirement already satisfied: numpy>=1.11 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (1.19.5)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (0.10.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (2.8.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from cycler>=0.10->matplotlib) (1.15.0)\n",
            "/content/project\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rsZEHMrIEVz6"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XcYm7y8rECqg"
      },
      "source": [
        "## CapsNet Modules"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fC4zxyWWeF-k"
      },
      "source": [
        "def squash(s):\n",
        "    '''Non-linear \"squashing\" function to ensure that short vectors get shrunk \n",
        "    to almost zero length and long vectors get shrunk to a length slightly \n",
        "    below 1. Equation (1) in the paper.\n",
        "    \n",
        "    Input:\n",
        "      s: \ttotal input vector\n",
        "    \n",
        "    Output:\n",
        "      squashed output vector\n",
        "    '''\n",
        "    norm_sqrd = torch.sum(s**2, dim=2, keepdim=True)\n",
        "    return (norm_sqrd / (1 + norm_sqrd)) * (s / (torch.sqrt(norm_sqrd) + 1e-8))"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VdGH3V4jJPmQ"
      },
      "source": [
        "class PrimaryCapsules(nn.Module):\n",
        "  '''The layer after Conv1. Section 4 of the paper.\n",
        "  '''\n",
        "\n",
        "  def __init__(self):\n",
        "    super(PrimaryCapsule, self).__init__()\n",
        "    self.capsules = nn.ModuleList(\n",
        "        [nn.Conv2d(in_channels=256,\n",
        "                   out_channels=8,\n",
        "                   kernel_size=9,\n",
        "                   stride=2)\n",
        "        for i in range(32)]\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    '''Section 4 of the paper.\n",
        "\n",
        "    Input:\n",
        "      x: outut of the ReLUConv1 layer, of shape (batch_size x 256 x 20 x 20)\n",
        "\n",
        "    Output:\n",
        "      squashed PrimaryCapsules output tensor, of shape (batch_size x 1152 x 8)\n",
        "    '''\n",
        "    batch_size = x.size(0)\n",
        "    all_u = []\n",
        "    for cap in self.capsules:\n",
        "        u = cap(x)  # (batch_size x 8 x 6 x 6)\n",
        "        u = u.view(batch_size, 8, 36, 1)  # (batch_size x 8 x 36 x 1)\n",
        "        u.append(u)\n",
        "    all_u = torch.cat(all_u, dim=3)  # (batch_size x 8 x 36 x 32)\n",
        "    all_u = all_u.view(batch_size, 8, -1)  # (batch_size x 8 x 1152)\n",
        "    all_u = torch.transpose(all_u, 1, 2)  # (batch_size x 1152 x 8)\n",
        "    all_u = squash(all_u)  # (batch_size x 1152 x 8)\n",
        "    return all_u"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xz1tFdzkaHyP"
      },
      "source": [
        "class DoodleCapsules(nn.Module):\n",
        "  '''The layer after PrimaryCapsules. Section 4 of the paper.\n",
        "  '''\n",
        "\n",
        "  def __init__(self):\n",
        "    super(DigitCaps, self).__init__()\n",
        "    self.opt = opt\n",
        "    self.W = nn.Parameter(torch.randn(1, 1152, opt.n_classes, 8, 16))\n",
        "  \n",
        "  def forward(self, u):\n",
        "    '''Equation (2) and Procedure 1 in the paper.\n",
        "\n",
        "    Input:\n",
        "      u: output of the PrimaryCapsules layer, of shape (batch_size x 8)\n",
        "\n",
        "    Output:\n",
        "      output tensor of the DoodleCapsules layer, of shape (batch_size x 1152 x 10 x 1)\n",
        "    '''\n",
        "    batch_size = u.size(0)\n",
        "    u = torch.unsqueeze(u, dim=2)  # (batch_size x 1 x 8)\n",
        "    u = torch.unsqueeze(u, dim=2)  # (batch_size x 1 x 1 x 8)\n",
        "    u_hat = torch.matmul(u, self.W).squeeze()  # (batch_size x 1152 x n_classes x 16)\n",
        "\n",
        "    b = Variable(torch.zeros(batch_size, 1152, self.opt.n_classes, 1))  # (batch_size x 1152 x n_classes x 1)\n",
        "    if self.opt.use_cuda & torch.cuda.is_available(): b = b.cuda()\n",
        "\n",
        "    for i in range(self.opt.iterations):\n",
        "        c = F.softmax(b, dim=2)  # (batch_size x 1152 x n_classes x 1)\n",
        "        s = torch.sum(u_hat * c, dim=1)  # (batch_size x n_classes x 16)\n",
        "        v = squash(s)  # (batch_size x n_classes x 16)\n",
        "        b = b + torch.sum(u_hat * v.unsqueeze(1), dim=3, keepdim=True)  # (batch_size x 1152 x n_classes x 1)\n",
        "    return v  # (batch_size x n_classes x 16)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q-k05aXUmDs8"
      },
      "source": [
        "class DoodleDecoder(nn.Module):\n",
        "  '''Decoder structure to reconstruct the doodle from the output of the DoodleCapsules layer.\n",
        "  Section 4 of the paper. For an illustrative explaination, see Figure 2 of the paper.\n",
        "  '''\n",
        "\n",
        "  def __init__(self, opt):\n",
        "    super(DoodleDecoder, self).__init__()\n",
        "    self.opt = opt\n",
        "    self.layers = nn.Sequential(\n",
        "      nn.Linear(opt.n_classes * 16, 512),\n",
        "      nn.ReLU(inplace=True),\n",
        "      nn.Linear(512, 1024),\n",
        "      nn.ReLU(inplace=True),\n",
        "      nn.Linear(1024, opt.height * opt.width),\n",
        "      nn.Sigmoid()\n",
        "    )\n",
        "\n",
        "  def forward(self, v, target):\n",
        "    '''Takes a 16-dimensional vector v from the *correct* DoodleCapsules, and \n",
        "    learns to decode it into an image of a doodle. Mask out the other (n_classes - 1) classes.\n",
        "    Section 4 of the paper. For an illustrative explaination, see Figure 2 of the paper.\n",
        "\n",
        "    Input:\n",
        "      v: output of DoodleCapsules, of shape (batch_size x n_classes x 16)\n",
        "      target: one-hot targets, of shape (batch_size, n_classes)\n",
        "\n",
        "    Output:\n",
        "      decoder constructed images, of shape (batch_size x 784)\n",
        "    '''\n",
        "    \n",
        "    # TODO: the true target or the most probable?\n",
        "    # correct = torch.sqrt((v ** 2).sum(2))  # (batch_size x n_classes)\n",
        "    # correct = F.softmax(correct, dim=0)  # (batch_size x n_classes)\n",
        "    # correct = correct.max(dim=1)[1]  # (batch_size)\n",
        "\n",
        "    # Create the mask, which is 1 only for the correct class and 0 otherwise\n",
        "    mask = target.type(torch.FloatTensor).unsqueeze(-1)  # (batch_size x n_classes x 1)\n",
        "    mask = torch.repeat_interleave(mask, 16, dim=2)  # (batch_size x n_classes x 16)\n",
        "    if self.opt.use_cuda & torch.cuda.is_available(): mask = mask.cuda()\n",
        "\n",
        "    masked = v * mask  # (batch_size x n_classes x 16)\n",
        "    return self.layers(masked)  # (batch_size x 784)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CH6pouLtXQnx"
      },
      "source": [
        "class CapsuleNetwork(nn.Module):\n",
        "  '''Consists of a ReLU Convolution layer, a PrimaryCapsules layer, a DoodleCapsules\n",
        "  layer, and a Decoder layer. Section 4 of the paper.\n",
        "  '''\n",
        "\n",
        "  def __init__(self, opt):\n",
        "    super(CapsuleNetwork, self).__init__()\n",
        "    self.opt = opt\n",
        "    self.ReLUConv1 = nn.Sequential(\n",
        "        nn.Conv2d(in_channels=1, out_channels=256, kernel_size=9),\n",
        "        F.relu()\n",
        "    )\n",
        "    self.PrimaryCapsules = PrimaryCaps()\n",
        "    self.DoodleCapsules = DoodleCapsules(opt)\n",
        "    self.DoodleDecoder = DoodleDecoder(opt)\n",
        "  \n",
        "  def forward(self, x):\n",
        "    '''Section 4 of the paper.\n",
        "\n",
        "    Input:\n",
        "      the input to the network, of shape (batch_size x 1 x 28 x 28)\n",
        "\n",
        "    Output:\n",
        "      the output of the network, of shape (batch_size x n_classes x 16)\n",
        "    '''\n",
        "    v = self.ReLUConv1(x)  # (batch_size x 256 x 20 x 20)\n",
        "    v = self.PrimaryCaps(v)  # (batch_size x 1152 x 8)\n",
        "    v = self.DoodleCapsules(v)  # (batch_size x n_classes x 16)\n",
        "    return v  # (batch_size x n_classes x 16)\n",
        "\n",
        "  def marginal_loss(self, v, target):\n",
        "    '''Section 3, Equation (4) of the paper.\n",
        "\n",
        "    Input:\n",
        "      v: the output of the network, of shape (batch_size x n_classes x 16)\n",
        "      target: the one-hot target, of shape (batch_size x n_classes)\n",
        "      lambd: a scalor for down-weighting of the loss for absent doodle classes\n",
        "    \n",
        "    Output:\n",
        "      marginal loss (a scalor summed over all batches and classes)\n",
        "    '''\n",
        "    v_norm = torch.sqrt((v ** 2).sum(dim=2))  # (batch_size x n_classes)\n",
        "    zeros = torch.zeros(norm.size())  # (batch_size x n_classes)\n",
        "    if self.opt.use_cuda & torch.cuda.is_available(): zeros = zeros.cuda()\n",
        "    max1 = torch.max(zeros, 0.9 - norm) ** 2  # (batch_size x n_classes)\n",
        "    max2 = torch.max(zeros, norm - 0.1) ** 2  # (batch_size x n_classes) \n",
        "    loss = target * max1 + (1 - target) * 0.5 * max2  # (batch_size x n_classes)\n",
        "    return torch.sum(loss)  # scalor\n",
        "\n",
        "  def reconstruction_loss(self, data, reconstruction):\n",
        "    '''Reconstruction for regularization. Ecourages the doodle capsules to \n",
        "    encode the instantiation parameters of the input doodle. Section 4.1 of the paper.\n",
        "\n",
        "    Input:\n",
        "      data: the real image, of shape (batch_size, 784)\n",
        "      reconstruction: the reconstructed image, of shape (batch_size, 784)\n",
        "\n",
        "    Output:\n",
        "      reconstruction loss (a scalor summed over all batches and classes)\n",
        "    '''\n",
        "    return torch.sum((reconstruction - image) ** 2)\n",
        "\n",
        "  def loss(self, v, data, target):\n",
        "    '''Loss is marginal loss + 0.0005 * reconstruction loss. 0.0005 to ensure\n",
        "    the reconstruction loss does not dominate the training. Section 4.1 of the paper.\n",
        "\n",
        "    Input:\n",
        "      v: output of the network, of shape (batch_size x 10 x 16)\n",
        "      target: one-hot target, of shape (batch_size x n_classes)\n",
        "      data: the input to the network (the image), of shape (batch_size, 784)\n",
        "    \n",
        "    Output:\n",
        "      averaged loss (a scalor) over batches \n",
        "    '''\n",
        "    batch_size = data.size(0)\n",
        "    marginal_loss = self.marginal_loss(v, target)  # scalor\n",
        "    reconstruction = self.DoodleDecoder(v, target)  # (batch_size, 784)\n",
        "    reconstruction_loss = self.reconstruction_loss(data, reconstruction)  # scalor\n",
        "    return (marginal_loss + 0.0005 * reconstruction_loss)/batch_size  # scalor"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zKPXAofhnYhK"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Holn-KGHnvZH"
      },
      "source": [
        "# opt = {\n",
        "#     iterations: 3,\n",
        "#     lr: 0.005,\n",
        "#     width: 28,\n",
        "#     height: 28,\n",
        "#     n_classes: 10\n",
        "# }\n",
        "\n",
        "# model = CapsuleNetwork(opt)\n",
        "# if opt.use_cuda & torch.cuda.is_available(): model.cuda()\n",
        "\n",
        "# train(model, opt, train_loader, test_loader)"
      ],
      "execution_count": 17,
      "outputs": []
    }
  ]
}