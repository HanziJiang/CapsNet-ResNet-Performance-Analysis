{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "guide_big.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PiQpTflFkuqi"
      },
      "source": [
        "# Capsule Neural Network (CapsNet) Implementation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QDxJ_A_0Gnt9"
      },
      "source": [
        "Implementation of the paper [Dynamic Routing Between Capsules](https://arxiv.org/pdf/1710.09829.pdf) by Sara Sabour, Nicholas Frosst, and Geoffrey E. Hinton. Used [jindongwang/Pytorch-CapsuleNet](https://github.com/jindongwang/Pytorch-CapsuleNet) and [laubonghaudoi/CapsNet_guide_PyTorch](https://github.com/laubonghaudoi/CapsNet_guide_PyTorch) to clarify some confusions, and borrowed some code."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_pkx4McDDItG"
      },
      "source": [
        "## Setup PyTorch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "peSdN_P8CElk",
        "outputId": "5d02b9c4-de9e-4bfa-91bb-bf1513f22ece"
      },
      "source": [
        "!pip install torch torchvision\n",
        "!pip install matplotlib\n",
        "!pip install import-ipynb\n",
        "!pip install tqdm\n",
        "!pip install pytorch_extras"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (1.8.1+cu101)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (0.9.1+cu101)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch) (1.19.5)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch) (3.7.4.3)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.7/dist-packages (from torchvision) (7.1.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (3.2.2)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (2.4.7)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (0.10.0)\n",
            "Requirement already satisfied: numpy>=1.11 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (1.19.5)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (1.3.1)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (2.8.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from cycler>=0.10->matplotlib) (1.15.0)\n",
            "Requirement already satisfied: import-ipynb in /usr/local/lib/python3.7/dist-packages (0.1.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (4.41.1)\n",
            "Requirement already satisfied: pytorch_extras in /usr/local/lib/python3.7/dist-packages (0.1.3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YMdBumXa75tw",
        "outputId": "cff3e1ca-4168-4025-810b-97d5039c29a3"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"mnt\")"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at mnt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wlYHtO737_dA",
        "outputId": "f4491647-abfd-44c7-c7f5-3f3dc793c1cb"
      },
      "source": [
        "%cd \"mnt/My Drive\""
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/mnt/My Drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rsZEHMrIEVz6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ada3abb3-2e13-4151-952f-39987c0d6042"
      },
      "source": [
        "import torch\n",
        "from scipy.linalg import sqrtm\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "import torch_extras\n",
        "import torch.nn as nn\n",
        "import torchvision.utils as tv_utils\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "import import_ipynb\n",
        "import load_DrawData_with_transform as loader\n",
        "import numpy as np\n",
        "import argparse\n",
        "%matplotlib inline\n",
        "# %mkdir -p /content/project/\n",
        "# %cd /content/project/"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "importing Jupyter notebook from load_DrawData_with_transform.ipynb\n",
            "Requirement already satisfied: ndjson in /usr/local/lib/python3.7/dist-packages (0.3.1)\n",
            "Requirement already satisfied: cairocffi in /usr/local/lib/python3.7/dist-packages (1.2.0)\n",
            "Requirement already satisfied: cffi>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from cairocffi) (1.14.5)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.1.0->cairocffi) (2.20)\n",
            "Requirement already satisfied: imutils in /usr/local/lib/python3.7/dist-packages (0.5.4)\n",
            "batch_size: 8\n",
            "total training samples: 15000\n",
            "total validatoin samples: 5000\n",
            "total test samples: 5000\n",
            "mkdir: cannot create directory ‘quickDrawData’: File exists\n",
            "Copying gs://quickdraw_dataset/full/simplified/triangle.ndjson...\n",
            "- [1/1 files][ 30.5 MiB/ 30.5 MiB] 100% Done                                    \n",
            "Operation completed over 1 objects/30.5 MiB.                                     \n",
            "Copying gs://quickdraw_dataset/full/simplified/square.ndjson...\n",
            "| [1/1 files][ 32.0 MiB/ 32.0 MiB] 100% Done                                    \n",
            "Operation completed over 1 objects/32.0 MiB.                                     \n",
            "Copying gs://quickdraw_dataset/full/simplified/mushroom.ndjson...\n",
            "\\ [1/1 files][ 53.5 MiB/ 53.5 MiB] 100% Done                                    \n",
            "Operation completed over 1 objects/53.5 MiB.                                     \n",
            "Copying gs://quickdraw_dataset/full/simplified/crown.ndjson...\n",
            "- [1/1 files][ 47.7 MiB/ 47.7 MiB] 100% Done                                    \n",
            "Operation completed over 1 objects/47.7 MiB.                                     \n",
            "Copying gs://quickdraw_dataset/full/simplified/envelope.ndjson...\n",
            "- [1/1 files][ 41.7 MiB/ 41.7 MiB] 100% Done                                    \n",
            "Operation completed over 1 objects/41.7 MiB.                                     \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XcYm7y8rECqg"
      },
      "source": [
        "## CapsNet Modules"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VdGH3V4jJPmQ"
      },
      "source": [
        "class PrimaryCapsules(nn.Module):\n",
        "    '''\n",
        "    The `PrimaryCaps` layer consists of 32 capsule units. Each unit takes\n",
        "    the output of the `Conv1` layer, which is a `[256, 20, 20]` feature\n",
        "    tensor (omitting `batch_size`), and performs a 2D convolution with 8\n",
        "    output channels, kernel size 9 and stride 2, thus outputing a [8, 6, 6]\n",
        "    tensor. In other words, you can see these 32 capsules as 32 paralleled 2D\n",
        "    convolutional layers. Then we concatenate these 32 capsules' outputs and\n",
        "    flatten them into a tensor of size `[1152, 8]`, representing 1152 8D\n",
        "    vectors, and send it to the next layer `DigitCaps`.\n",
        "    As indicated in Section 4, Page 4 in the paper, *One can see PrimaryCaps\n",
        "    as a Convolution layer with Eq.1 as its block non-linearity.*, outputs of\n",
        "    the `PrimaryCaps` layer are squashed before being passed to the next layer.\n",
        "    Reference: Section 4, Fig. 1\n",
        "    '''\n",
        "\n",
        "    def __init__(self):\n",
        "        '''\n",
        "        We build 8 capsule units in the `PrimaryCaps` layer, each can be\n",
        "        seen as a 2D convolution layer.\n",
        "        '''\n",
        "        super(PrimaryCapsules, self).__init__()\n",
        "\n",
        "        self.capsules = nn.ModuleList([\n",
        "            nn.Conv2d(in_channels=256,\n",
        "                      out_channels=8,\n",
        "                      kernel_size=9,\n",
        "                      stride=2)\n",
        "            for i in range(32)\n",
        "        ])\n",
        "\n",
        "    def forward(self, x):\n",
        "        '''\n",
        "        Each capsule outputs a [batch_size, 8, 6, 6] tensor, we need to\n",
        "        flatten and concatenate them into a [batch_size, 8, 6*6, 32] size\n",
        "        tensor and flatten and transpose into `u` [batch_size, 1152, 8], \n",
        "        where each [batch_size, 1152, 1] size tensor is the `u_i` in Eq.2. \n",
        "        #### Dimension transformation in this layer(ignoring `batch_size`):\n",
        "        [256, 20, 20] --> [8, 6, 6] x 32 capsules --> [1152, 8]\n",
        "        Note: `u_i` is one [1, 8] in the final [1152, 8] output, thus there are\n",
        "        1152 `u_i`s.\n",
        "        '''\n",
        "        batch_size = x.size(0)\n",
        "\n",
        "        u = []\n",
        "        for i in range(32):\n",
        "            # Input: [batch_size, 256, 20, 20]\n",
        "\n",
        "            u_i = self.capsules[i](x)\n",
        "            # u_i: [batch_size, 8, 6, 6]\n",
        "            u_i = u_i.view(batch_size, 8, -1, 1)\n",
        "            # u_i: [batch_size, 8, 36]\n",
        "            u.append(u_i)\n",
        "\n",
        "        # u: [batch_size, 8, 36, 1] x 32\n",
        "        u = torch.cat(u, dim=3)\n",
        "        # u: [batch_size, 8, 36, 32]\n",
        "        u = u.view(batch_size, 8, -1)\n",
        "        # u: [batch_size, 8, 1152]\n",
        "        u = torch.transpose(u, 1, 2)\n",
        "        # u: [batch_size, 1152, 8]\n",
        "        #assert u.data.size() == torch.Size([batch_size, 1152, 8])\n",
        "\n",
        "        # Squash before output\n",
        "        u_squashed = self.squash(u)\n",
        "\n",
        "        return u_squashed\n",
        "\n",
        "    def squash(self, u):\n",
        "        '''\n",
        "        Args:\n",
        "            `u`: [batch_size, 1152, 8]\n",
        "        Return:\n",
        "            `u_squashed`: [batch_size, 1152, 8]\n",
        "        In CapsNet, we use the squash function after the output of both \n",
        "        capsule layers. Squash functions can be seen as activating functions\n",
        "        like sigmoid, but for capsule layers rather than traditional fully\n",
        "        connected layers, as they squash vectors instead of scalars.\n",
        "        v_j = (norm(s_j) ^ 2 / (1 + norm(s_j) ^ 2)) * (s_j / norm(s_j))\n",
        "        Reference: Eq.1 in Section 2.\n",
        "        '''\n",
        "        batch_size = u.size(0)\n",
        "\n",
        "        # u: [batch_size, 1152, 8]\n",
        "        square = u ** 2\n",
        "\n",
        "        # square_sum for u: [batch_size, 1152]\n",
        "        square_sum = torch.sum(square, dim=2)\n",
        "\n",
        "        # norm for u: [batch_size, 1152]\n",
        "        norm = torch.sqrt(square_sum)\n",
        "\n",
        "        # factor for u: [batch_size, 1152]\n",
        "        factor = norm ** 2 / (norm * (1 + norm ** 2))\n",
        "\n",
        "        # u_squashed: [batch_size, 1152, 8]\n",
        "        u_squashed = factor.unsqueeze(2) * u\n",
        "        assert u_squashed.size() == torch.Size([batch_size, 1152, 8])\n",
        "\n",
        "        return u_squashed"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xz1tFdzkaHyP"
      },
      "source": [
        "class DoodleCapsules(nn.Module):\n",
        "    '''\n",
        "    The `DigitCaps` layer consists of 10 16D capsules. Compared to the traditional\n",
        "    scalar output neurons in fully connected networks(FCN), the `DigitCaps` layer\n",
        "    can be seen as an FCN with ten 16-dimensional output neurons, which we call\n",
        "    these neurons \"capsules\".\n",
        "    In this layer, we take the input `[1152, 8]` tensor `u` as 1152 [8,] vectors\n",
        "    `u_i`, each `u_i` is a 8D output of the capsules from `PrimaryCaps` (see Eq.2\n",
        "    in Section 2, Page 2) and sent to the 10 capsules. For each capsule, the tensor\n",
        "    is first transformed by `W_ij`s into [1152, 16] size. Then we perform the Dynamic\n",
        "    Routing algorithm to get the output `v_j` of size [16,]. As there are 10 capsules,\n",
        "    the final output is [16, 10] size.\n",
        "    #### Dimension transformation in this layer(ignoring `batch_size`):\n",
        "    [1152, 8] --> [1152, 16] --> [1, 16] x 10 capsules --> [10, 16] output\n",
        "    Note that in our codes we have vectorized these computations, so the dimensions\n",
        "    above are just for understanding, actual dimensions of tensors are different.\n",
        "    '''\n",
        "\n",
        "    def __init__(self, opt):\n",
        "        '''\n",
        "        There is only one parameter in this layer, `W` [1, 1152, 10, 16, 8], where\n",
        "        every [8, 16] is a weight matrix W_ij in Eq.2, that is, there are 11520\n",
        "        `W_ij`s in total.\n",
        "        The the coupling coefficients `b` [64, 1152, 10, 1] is a temporary variable which\n",
        "        does NOT belong to the layer's parameters. In other words, `b` is not updated\n",
        "        by gradient back-propagations. Instead, we update `b` by Dynamic Routing\n",
        "        in every forward propagation. See the docstring of `self.forward` for details.\n",
        "        '''\n",
        "        super(DoodleCapsules, self).__init__()\n",
        "        self.opt = opt\n",
        "\n",
        "        self.W = nn.Parameter(torch.randn(1, 1152, opt.n_classes, 8, 16))\n",
        "\n",
        "    def forward(self, u):\n",
        "        '''\n",
        "        Args:\n",
        "            `u`: [batch_size, 1152, 8]\n",
        "        Return:\n",
        "            `v`: [batch_size, 10, 16]\n",
        "        In this layer, we vectorize our computations by calling `W` and using\n",
        "        `torch.matmul()`. Thus the full computaion steps are as follows.\n",
        "            1. Expand `W` into batches and compute `u_hat` (Eq.2)\n",
        "            2. Line 2: Initialize `b` into zeros\n",
        "            3. Line 3: Start Routing for `r` iterations:\n",
        "                1. Line 4: c = softmax(b)\n",
        "                2. Line 5: s = sum(c * u_hat)\n",
        "                3. Line 6: v = squash(s)\n",
        "                4. Line 7: b += u_hat * v\n",
        "        The coupling coefficients `b` can be seen as a kind of attention matrix\n",
        "        in the attentional sequence-to-sequence networks, which is widely used in\n",
        "        Neural Machine Translation systems. For tutorials on  attentional seq2seq\n",
        "        models, see https://arxiv.org/abs/1703.01619 or\n",
        "        http://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html\n",
        "        Reference: Section 2, Procedure 1\n",
        "        '''\n",
        "        batch_size = u.size(0)\n",
        "\n",
        "        # First, we need to expand the dimensions of `W` and `u` to compute `u_hat`\n",
        "        #assert u.size() == torch.Size([batch_size, 1152, 8])\n",
        "        # u: [batch_size, 1152, 1, 1, 8]\n",
        "        u = torch.unsqueeze(u, dim=2)\n",
        "        u = torch.unsqueeze(u, dim=2)\n",
        "        # Now we compute u_hat in Eq.2\n",
        "        # u_hat: [batch_size, 1152, 10, 16]\n",
        "        u_hat = torch.matmul(u, self.W).squeeze()\n",
        "\n",
        "        # Line 2: Initialize b into zeros\n",
        "        # b: [batch_size, 1152, 10, 1]\n",
        "        b = Variable(torch.zeros(batch_size, 1152, opt.n_classes, 1))\n",
        "        if self.opt.use_cuda & torch.cuda.is_available():\n",
        "            b = b.cuda()\n",
        "\n",
        "        # Start Routing\n",
        "        for r in range(self.opt.r):\n",
        "            # Line 4: c_i = softmax(b_i)\n",
        "            # c: [b, 1152, 10, 1]\n",
        "            c = F.softmax(b, dim=2)\n",
        "            #assert c.size() == torch.Size([batch_size, 1152, opt.n_classes, 1])\n",
        "\n",
        "            # Line 5: s_j = sum_i(c_ij * u_hat_j|i)\n",
        "            # u_hat: [batch_size, 1152, 10, 16]\n",
        "            # s: [batch_size, 10, 16]\n",
        "            s = torch.sum(u_hat * c, dim=1)\n",
        "\n",
        "            # Line 6: v_j = squash(s_j)\n",
        "            # v: [batch_size, 10, 16]\n",
        "            v = self.squash(s)\n",
        "            #assert v.size() == torch.Size([batch_size, opt.n_classes, 16])\n",
        "\n",
        "            # Line 7: b_ij += u_hat * v_j\n",
        "            # u_hat: [batch_size, 1152, 10, 16]\n",
        "            # v: [batch_size, 10, 16]\n",
        "            # a: [batch_size, 10, 1152, 16]\n",
        "            a = u_hat * v.unsqueeze(1)\n",
        "            # b: [batch_size, 1152, 10, 1]\n",
        "            b = b + torch.sum(a, dim=3, keepdim=True)\n",
        "\n",
        "        return v\n",
        "\n",
        "    def squash(self, s):\n",
        "        '''\n",
        "        Args:\n",
        "            `s`: [batch_size, 10, 16]\n",
        "        v_j = (norm(s_j) ^ 2 / (1 + norm(s_j) ^ 2)) * (s_j / norm(s_j))\n",
        "        Reference: Eq.1 in Section 2.\n",
        "        '''\n",
        "        batch_size = s.size(0)\n",
        "\n",
        "        # s: [batch_size, 10, 16]\n",
        "        square = s ** 2\n",
        "\n",
        "        # square_sum for v: [batch_size, 10]\n",
        "        square_sum = torch.sum(square, dim=2)\n",
        "\n",
        "        # norm for v: [batch_size, 10]\n",
        "        norm = torch.sqrt(square_sum)\n",
        "\n",
        "        # factor for v: [batch_size, 10]\n",
        "        factor = norm ** 2 / (norm * (1 + norm ** 2))\n",
        "\n",
        "        # v: [batch_size, 10, 16]\n",
        "        v = factor.unsqueeze(2) * s\n",
        "        assert v.size() == torch.Size([batch_size, opt.n_classes, 16])\n",
        "\n",
        "        return v"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q-k05aXUmDs8"
      },
      "source": [
        "class DoodleDecoder(nn.Module):\n",
        "    '''\n",
        "    The decoder network consists of 3 fully connected layers. For each\n",
        "    [10, 16] output, we mask out the incorrect predictions, and send\n",
        "    the [16,] vector to the decoder network to reconstruct a [784,] size\n",
        "    image.\n",
        "    Reference: Section 4.1, Fig. 2\n",
        "    '''\n",
        "\n",
        "    def __init__(self, opt):\n",
        "        '''\n",
        "        The decoder network consists of 3 fully connected layers, with\n",
        "        512, 1024, 784 neurons each.\n",
        "        '''\n",
        "        super(DoodleDecoder, self).__init__()\n",
        "        self.opt = opt\n",
        "\n",
        "        self.fc1 = nn.Linear(16, 1024)\n",
        "        self.fc2 = nn.Linear(1024, 2048)\n",
        "        self.fc3 = nn.Linear(2048, opt.image_size*opt.image_size)\n",
        "\n",
        "    def forward(self, v, target):\n",
        "        '''\n",
        "        Args:\n",
        "            `v`: [batch_size, 10, 16]\n",
        "            `target`: [batch_size, 10]\n",
        "        Return:\n",
        "            `reconstruction`: [batch_size, 784]\n",
        "        We send the outputs of the `DigitCaps` layer, which is a\n",
        "        [batch_size, 10, 16] size tensor into the decoder network, and\n",
        "        reconstruct a [batch_size, 784] size tensor representing the image.\n",
        "        '''\n",
        "        batch_size = target.size(0)\n",
        "\n",
        "        target = target.type(torch.FloatTensor)\n",
        "        # mask: [batch_size, 10, 16]\n",
        "        mask = torch.stack([target for i in range(16)], dim=2)\n",
        "        #assert mask.size() == torch.Size([batch_size, opt.n_classes, 16])\n",
        "        if self.opt.use_cuda & torch.cuda.is_available():\n",
        "            mask = mask.cuda()\n",
        "\n",
        "        # v: [bath_size, 10, 16]\n",
        "        v_masked = mask * v\n",
        "        v_masked = torch.sum(v_masked, dim=1)\n",
        "        #assert v_masked.size() == torch.Size([batch_size, 16])\n",
        "\n",
        "        # Forward\n",
        "        v = F.relu(self.fc1(v_masked))\n",
        "        v = F.relu(self.fc2(v))\n",
        "        reconstruction = torch.sigmoid(self.fc3(v))\n",
        "\n",
        "        #assert reconstruction.size() == torch.Size([batch_size, opt.image_size*opt.image_size])\n",
        "        return reconstruction"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CH6pouLtXQnx"
      },
      "source": [
        "class CapsuleNetwork(nn.Module):\n",
        "    '''Consists of a ReLU Convolution layer, a PrimaryCapsules layer, a DoodleCapsules\n",
        "    layer, and a Decoder layer. Section 4 of the paper.\n",
        "    '''\n",
        "\n",
        "    def __init__(self, opt):\n",
        "        super(CapsuleNetwork, self).__init__()\n",
        "        self.opt = opt\n",
        "\n",
        "        self.Conv1 = nn.Conv2d(in_channels=1, out_channels=256, kernel_size=21)\n",
        "        self.PrimaryCaps = PrimaryCapsules()\n",
        "        self.DigitCaps = DoodleCapsules(opt)\n",
        "\n",
        "        self.Decoder = DoodleDecoder(opt)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        '''\n",
        "        Args:\n",
        "            `x`: [batch_size, 1, 28, 28] MNIST samples\n",
        "        \n",
        "        Return:\n",
        "            `v`: [batch_size, 10, 16] CapsNet outputs, 16D prediction vectors of\n",
        "                10 digit capsules\n",
        "        The dimension transformation procedure of an input tensor in each layer:\n",
        "            0. Input: [batch_size, 1, 28, 28] -->\n",
        "            1. `Conv1` --> [batch_size, 256, 20, 20] --> \n",
        "            2. `PrimaryCaps` --> [batch_size, 8, 6, 6] x 32 capsules --> \n",
        "            3. Flatten, concatenate, squash --> [batch_size, 1152, 8] -->\n",
        "            4. `W_ij`s and `DigitCaps` --> [batch_size, 16, 10] -->\n",
        "            5. Length of 10 capsules --> [batch_size, 10] output probabilities\n",
        "        '''\n",
        "        # Input: [batch_size, 1, 28, 28]\n",
        "        x = x.view(-1, 1, opt.image_size, opt.image_size)\n",
        "        x = F.relu(self.Conv1(x))\n",
        "        # PrimaryCaps input: [batch_size, 256, 20, 20]\n",
        "        u = self.PrimaryCaps(x)\n",
        "        # PrimaryCaps output u: [batch_size, 1152, 8]\n",
        "        v = self.DigitCaps(u)\n",
        "        # DigitCaps output v: [batsh_size, 10, 16]\n",
        "        return v\n",
        "\n",
        "    def marginal_loss(self, v, target, l=0.5):\n",
        "        '''\n",
        "        Args:\n",
        "            `v`: [batch_size, 10, 16]\n",
        "            `target`: [batch_size, 10]\n",
        "            `l`: Scalar, lambda for down-weighing the loss for absent digit classes\n",
        "        Return:\n",
        "            `marginal_loss`: Scalar\n",
        "        \n",
        "        L_c = T_c * max(0, m_plus - norm(v_c)) ^ 2 + lambda * (1 - T_c) * max(0, norm(v_c) - m_minus) ^2\n",
        "        \n",
        "        Reference: Eq.4 in Section 3.\n",
        "        '''\n",
        "        batch_size = v.size(0)\n",
        "\n",
        "        square = v ** 2\n",
        "        square_sum = torch.sum(square, dim=2)\n",
        "        # norm: [batch_size, 10]\n",
        "        norm = torch.sqrt(square_sum)\n",
        "\n",
        "        # The two T_c in Eq.4\n",
        "        T_c = target.type(torch.FloatTensor)\n",
        "        zeros = Variable(torch.zeros(norm.size()))\n",
        "        # Use GPU if available\n",
        "        if self.opt.use_cuda & torch.cuda.is_available():\n",
        "            zeros = zeros.cuda()\n",
        "            T_c = T_c.cuda()\n",
        "\n",
        "        # Eq.4\n",
        "        marginal_loss = T_c * (torch.max(zeros, 0.9 - norm) ** 2) + \\\n",
        "            (1 - T_c) * l * (torch.max(zeros, norm - 0.1) ** 2)\n",
        "        marginal_loss = torch.sum(marginal_loss)\n",
        "\n",
        "        return marginal_loss\n",
        "\n",
        "    def reconstruction_loss(self, reconstruction, image):\n",
        "        '''\n",
        "        Args:\n",
        "            `reconstruction`: [batch_size, 784] Decoder outputs of images\n",
        "            `image`: [batch_size, 1, 28, 28] MNIST samples\n",
        "        Return:\n",
        "            `reconstruction_loss`: Scalar Variable\n",
        "        The reconstruction loss is measured by a squared differences\n",
        "        between the reconstruction and the original image. \n",
        "        Reference: Section 4.1\n",
        "        '''\n",
        "        batch_size = image.size(0)\n",
        "        # image: [batch_size, 784]\n",
        "        image = image.view(batch_size, -1)\n",
        "        #assert image.size() == (batch_size, opt.image_size*opt.image_size)\n",
        "        \n",
        "        # Scalar Variable\n",
        "        reconstruction_loss = torch.sum((reconstruction - image) ** 2)\n",
        "        return reconstruction_loss\n",
        "\n",
        "    def cov(self, m, rowvar=False):\n",
        "        if m.dim() > 2:\n",
        "            raise ValueError('m has more than 2 dimensions')\n",
        "        if m.dim() < 2:\n",
        "            m = m.view(1, -1)\n",
        "        if not rowvar and m.size(0) != 1:\n",
        "            m = m.t()\n",
        "        # m = m.type(torch.double)  # uncomment this line if desired\n",
        "        fact = 1.0 / (m.size(1) - 1)\n",
        "        m -= torch.mean(m, dim=1, keepdim=True)\n",
        "        mt = m.t()  # if complex: mt = m.t().conj()\n",
        "        return fact * m.matmul(mt).squeeze()\n",
        "\n",
        "\n",
        "    def loss(self, v, image, target):\n",
        "        '''\n",
        "        Args:\n",
        "            `v`: [batch_size, 10, 16] CapsNet outputs\n",
        "            `target`: [batch_size, 10] One-hot MNIST labels\n",
        "            `image`: [batch_size, 1, 28, 28] MNIST samples\n",
        "        Return:\n",
        "            `L`: Scalar Variable, total loss\n",
        "            `marginal_loss`: Scalar Variable\n",
        "            `reconstruction_loss`: Scalar Variable\n",
        "        The reconstruction loss is scaled down by 5e-4, serving as a\n",
        "        regularization method.\n",
        "        Reference: Section 4.1\n",
        "        '''\n",
        "        batch_size = image.size(0)\n",
        "\n",
        "        marginal_loss = self.marginal_loss(v, target)\n",
        "\n",
        "        # Get reconstructions from the decoder network\n",
        "        reconstruction = self.Decoder(v, target)\n",
        "        reconstruction_loss = self.reconstruction_loss(reconstruction, image)\n",
        "\n",
        "        # Scalar Variable\n",
        "        loss = (marginal_loss + 0.0005 * reconstruction_loss) / batch_size\n",
        "\n",
        "        return loss, marginal_loss / batch_size, reconstruction_loss / batch_size"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zKPXAofhnYhK"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QQCM0T0_1d5W"
      },
      "source": [
        "def evaluate(opt, valid_loader, model, type_data, plot=False):\n",
        "    sum_loss = 0\n",
        "    sum_marginal_loss = 0\n",
        "    sum_reconstruction_loss = 0\n",
        "    correct = 0\n",
        "    num_sample = len(valid_loader.dataset)\n",
        "    num_batch = len(valid_loader)\n",
        "\n",
        "    model.eval()\n",
        "    for data, target in valid_loader:\n",
        "        data = data.to(torch.float32)\n",
        "        target = target.to(torch.int64)\n",
        "        batch_size = data.size(0)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            data, target = Variable(data), Variable(target)\n",
        "        if opt.use_cuda & torch.cuda.is_available():\n",
        "            data, target = data.cuda(), target.cuda()\n",
        "\n",
        "        output = model(data)  # (batch_size, n_classes, 16)\n",
        "        loss, marginal_loss, reconstruction_loss = model.loss(output, data, target)\n",
        "        sum_loss += loss.item()\n",
        "        sum_marginal_loss += marginal_loss.item()\n",
        "        sum_reconstruction_loss += reconstruction_loss.item()\n",
        "\n",
        "        norms = torch.sqrt(torch.sum(output**2, dim=2))  # (batch_size, n_classes)\n",
        "        pred = norms.data.max(1, keepdim=True)[1].type(torch.LongTensor)  # (batch_size, 1)\n",
        "        label = target.max(1, keepdim=True)[1].type(torch.LongTensor)  # (batch_size, 1)\n",
        "        correct += pred.eq(label.view_as(pred)).cpu().sum().item()\n",
        "\n",
        "    if plot:\n",
        "        plt.figure()\n",
        "        recons = model.Decoder(output, target)\n",
        "        recons = recons.view(batch_size, opt.image_size, opt.image_size)\n",
        "        recons = recons[0].cpu()\n",
        "        plt.imshow(recons.detach(), cmap = \"gray\")\n",
        "\n",
        "    sum_loss /= num_batch\n",
        "    sum_marginal_loss /= num_batch\n",
        "    sum_reconstruction_loss /= num_batch\n",
        "    accuracy = correct / float(num_sample)\n",
        "    print('{} loss: {:.4f}   Marginal loss: {:.4f}   Reconstruction loss: {:.4f}'.format(\n",
        "        type_data, sum_loss, sum_marginal_loss, sum_reconstruction_loss))\n",
        "    print('Accuracy: {}/{} {:.4f}'.format(correct, num_sample,\n",
        "        accuracy))\n",
        "    global highest\n",
        "    global model_name\n",
        "    if accuracy > highest:\n",
        "        highest = accuracy\n",
        "        print(\"Highest saved! New accuracy: {:.4f}\".format(accuracy))\n",
        "        torch.save(model, model_name)\n",
        "    model.train()"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jcoOvQfb1bVn"
      },
      "source": [
        "def train(opt, train_loader, valid_loader, model):\n",
        "    num_sample = len(train_loader.dataset)\n",
        "    num_batches = len(train_loader)\n",
        "    train_loss_list = []\n",
        "    loss_val = 0.\n",
        "\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=opt.lr)\n",
        "    scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, opt.gamma)\n",
        "    model.train()\n",
        "    evaluate(opt, train_loader, model, 'initial TRAIN', False) \n",
        "    evaluate(opt, valid_loader, model, 'initial VALID', False)\n",
        "    for epoch in range(opt.epochs):\n",
        "    \n",
        "        for batch_idx, (data, target) in enumerate(train_loader):\n",
        "            optimizer.zero_grad()\n",
        "            data = data.to(torch.float32)\n",
        "            \n",
        "            batch_size = data.size(0)\n",
        "            target = target.to(torch.int64)\n",
        "            #assert target.size() == torch.Size([batch_size, opt.n_classes])\n",
        "\n",
        "            # Use GPU if available\n",
        "            with torch.no_grad():\n",
        "                data, target = Variable(data), Variable(target)\n",
        "            if opt.use_cuda & torch.cuda.is_available():\n",
        "                data, target = data.cuda(), target.cuda()\n",
        "            \n",
        "            output = model(data)\n",
        "            loss, marginal_loss, reconstruction_loss = model.loss(output, data, target)\n",
        "            loss_val = loss.item()\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        print('\\nEpoch: {}'.format(epoch))\n",
        "        print('Learning rate: {:.2e}'.format(scheduler.get_last_lr()[0]))\n",
        "        evaluate(opt, valid_loader, model, 'VALID', False)\n",
        "\n",
        "        scheduler.step()"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Holn-KGHnvZH"
      },
      "source": [
        "def get_opts():\n",
        "    parser = argparse.ArgumentParser(description='CapsuleNetwork')\n",
        "    parser.add_argument('-image_size', type=int, default=40)\n",
        "    parser.add_argument('-batch_size', type=int, default=8)\n",
        "    parser.add_argument('-lr', type=float, default=1e-4)\n",
        "    parser.add_argument('-epochs', type=int, default=20)\n",
        "    parser.add_argument('-n_classes', type=int, default=5)\n",
        "    parser.add_argument('-use_cuda', default=True)\n",
        "    parser.add_argument('-gamma', type=float, default=0.9)\n",
        "    parser.add_argument('-r', type=int, default=1)\n",
        "    opt, _ = parser.parse_known_args()\n",
        "    return opt"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4zXW1i3p1uq8",
        "outputId": "e43fd3c3-85fa-4eec-d779-d6f66dcd23d2"
      },
      "source": [
        "# To keep track of the highest-accuracy model\n",
        "global highest\n",
        "highest = 0.\n",
        "global model_name\n",
        "model_name = 'capsnet.pt'\n",
        "highest = 999 # Important! Make it impossible to override trained model\n",
        "\n",
        "opt = get_opts()\n",
        "model = CapsuleNetwork(opt)\n",
        "if opt.use_cuda & torch.cuda.is_available():\n",
        "    model.cuda()\n",
        "train_loader = loader.train_loader\n",
        "valid_loader = loader.valid_loader\n",
        "test_loader = loader.test_loader\n",
        "train(opt, train_loader, valid_loader, model)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "initial TRAIN loss: 0.9522   Marginal loss: 0.7655   Reconstruction loss: 373.3200\n",
            "Accuracy: 3008/15000 0.2005\n",
            "initial VALID loss: 0.9558   Marginal loss: 0.7691   Reconstruction loss: 373.3200\n",
            "Accuracy: 963/5000 0.1926\n",
            "\n",
            "Epoch: 0\n",
            "Learning rate: 1.00e-04\n",
            "VALID loss: 0.0527   Marginal loss: 0.0262   Reconstruction loss: 52.9385\n",
            "Accuracy: 4875/5000 0.9750\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L1zTaKivpznC"
      },
      "source": [
        "# evaluate(opt, train_loader, model, 'TRAIN', True)\n",
        "# evaluate(opt, valid_loader, model, 'VALID', True)\n",
        "# evaluate(opt, test_loader, model, 'TEST', True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qM8J2BxNrJ1G"
      },
      "source": [
        "# for data, target in valid_loader:\n",
        "#     data = data.to(torch.float32)\n",
        "#     target = target.to(torch.int64)\n",
        "#     batch_size = data.size(0)\n",
        "#     assert target.size() == torch.Size([batch_size, opt.n_classes])\n",
        "\n",
        "#     with torch.no_grad():\n",
        "#         data, target = Variable(data), Variable(target)\n",
        "#     if opt.use_cuda & torch.cuda.is_available():\n",
        "#         data, target = data.cuda(), target.cuda()\n",
        "\n",
        "#     output = model(data)  # (batch_size, n_classes, 16)\n",
        "\n",
        "#     norms = torch.sqrt(torch.sum(output**2, dim=2))  # (batch_size, n_classes)\n",
        "#     pred = norms.data.max(1, keepdim=True)[1].type(torch.LongTensor)  # (batch_size, 1)\n",
        "#     label = target.max(1, keepdim=True)[1].type(torch.LongTensor)  # (batch_size, 1)\n",
        "#     break\n",
        "# recons = model.Decoder(output, target)\n",
        "# recons = recons.view(batch_size, opt.image_size, opt.image_size)\n",
        "# print(label)\n",
        "# for i in range(54):\n",
        "#   plt.figure()\n",
        "#   plt.imshow(recons[i].cpu().detach(), cmap = \"gray\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}