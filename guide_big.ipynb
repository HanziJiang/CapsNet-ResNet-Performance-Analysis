{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "guide big.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PiQpTflFkuqi"
      },
      "source": [
        "# Capsule Neural Network (CapsNet) Implementation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QDxJ_A_0Gnt9"
      },
      "source": [
        "Implementation of the paper [Dynamic Routing Between Capsules](https://arxiv.org/pdf/1710.09829.pdf) by Sara Sabour, Nicholas Frosst, and Geoffrey E. Hinton. Used [jindongwang/Pytorch-CapsuleNet](https://github.com/jindongwang/Pytorch-CapsuleNet) and [laubonghaudoi/CapsNet_guide_PyTorch](https://github.com/laubonghaudoi/CapsNet_guide_PyTorch) to clarify some confusions, and borrowed some code."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_pkx4McDDItG"
      },
      "source": [
        "## Setup PyTorch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "peSdN_P8CElk",
        "outputId": "8dfe150e-6380-4a9e-cb18-0d470fdfa8ad"
      },
      "source": [
        "!pip install torch torchvision\n",
        "!pip install matplotlib\n",
        "!pip install import-ipynb\n",
        "!pip install tqdm\n",
        "!pip install pytorch_extras"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (1.8.1+cu101)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (0.9.1+cu101)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch) (1.19.5)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch) (3.7.4.3)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.7/dist-packages (from torchvision) (7.1.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (3.2.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (0.10.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (1.3.1)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (2.8.1)\n",
            "Requirement already satisfied: numpy>=1.11 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (1.19.5)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (2.4.7)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from cycler>=0.10->matplotlib) (1.15.0)\n",
            "Collecting import-ipynb\n",
            "  Downloading https://files.pythonhosted.org/packages/63/35/495e0021bfdcc924c7cdec4e9fbb87c88dd03b9b9b22419444dc370c8a45/import-ipynb-0.1.3.tar.gz\n",
            "Building wheels for collected packages: import-ipynb\n",
            "  Building wheel for import-ipynb (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for import-ipynb: filename=import_ipynb-0.1.3-cp37-none-any.whl size=2976 sha256=d02e0861c9be47260dd8788a68f6443ed68883088272006fb9b812b303b1d0f5\n",
            "  Stored in directory: /root/.cache/pip/wheels/b4/7b/e9/a3a6e496115dffdb4e3085d0ae39ffe8a814eacc44bbf494b5\n",
            "Successfully built import-ipynb\n",
            "Installing collected packages: import-ipynb\n",
            "Successfully installed import-ipynb-0.1.3\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (4.41.1)\n",
            "Collecting pytorch_extras\n",
            "  Downloading https://files.pythonhosted.org/packages/66/79/42d7d9a78c27eb897b14790c9759dd9a991f67bc987e9e137527a68db9dc/pytorch-extras-0.1.3.tar.gz\n",
            "Building wheels for collected packages: pytorch-extras\n",
            "  Building wheel for pytorch-extras (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pytorch-extras: filename=pytorch_extras-0.1.3-cp37-none-any.whl size=2833 sha256=e9391f6fe4d12945dbfdd2fd2cb29d7fbc5443e7c253c3c9da273ce5b6b96fef\n",
            "  Stored in directory: /root/.cache/pip/wheels/5b/7c/5a/f27d4088adfe722cb280d523a1ed9eeb33be11b8d3a653292a\n",
            "Successfully built pytorch-extras\n",
            "Installing collected packages: pytorch-extras\n",
            "Successfully installed pytorch-extras-0.1.3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YMdBumXa75tw",
        "outputId": "c06b8bf5-9f80-44c0-a138-9f4f14c1e192"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"mnt\")"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at mnt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wlYHtO737_dA",
        "outputId": "3cbfe14c-1a98-4240-c011-75b6f36f2284"
      },
      "source": [
        "%cd \"mnt/My Drive\""
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/mnt/My Drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rsZEHMrIEVz6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cd8d9470-9732-45d2-d7da-4a453e0a030e"
      },
      "source": [
        "import torch\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "import torch_extras\n",
        "import torch.nn as nn\n",
        "import torchvision.utils as tv_utils\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "import import_ipynb\n",
        "import load_DrawData_with_transform as loader\n",
        "import numpy as np\n",
        "import argparse\n",
        "from tqdm import tqdm\n",
        "%matplotlib inline\n",
        "# %mkdir -p /content/project/\n",
        "# %cd /content/project/"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "importing Jupyter notebook from load_DrawData_with_transform.ipynb\n",
            "Collecting ndjson\n",
            "  Downloading https://files.pythonhosted.org/packages/70/c9/04ba0056011ba96a58163ebfd666d8385300bd12da1afe661a5a147758d7/ndjson-0.3.1-py2.py3-none-any.whl\n",
            "Installing collected packages: ndjson\n",
            "Successfully installed ndjson-0.3.1\n",
            "Collecting cairocffi\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/84/ca/0bffed5116d21251469df200448667e90acaa5131edea869b44a3fbc73d0/cairocffi-1.2.0.tar.gz (70kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 3.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: cffi>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from cairocffi) (1.14.5)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.1.0->cairocffi) (2.20)\n",
            "Building wheels for collected packages: cairocffi\n",
            "  Building wheel for cairocffi (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for cairocffi: filename=cairocffi-1.2.0-cp37-none-any.whl size=89548 sha256=c9f1da1755b1cba038b256dc87dcd246a31f6906dd5c744a5ee5a9ec7a51c5ba\n",
            "  Stored in directory: /root/.cache/pip/wheels/40/76/48/f1effadceea83b32e7d957dd0f92db4db8b537d7b72b4ef374\n",
            "Successfully built cairocffi\n",
            "Installing collected packages: cairocffi\n",
            "Successfully installed cairocffi-1.2.0\n",
            "Requirement already satisfied: imutils in /usr/local/lib/python3.7/dist-packages (0.5.4)\n",
            "18000\n",
            "12000\n",
            "total training samples: 18000\n",
            "total validatoin samples: 6000\n",
            "total test samples: 6000\n",
            "mkdir: cannot create directory ‘quickDrawData’: File exists\n",
            "gs://quickdraw_dataset/full/simplified/t-shirt.ndjson\n",
            "Copying gs://quickdraw_dataset/full/simplified/t-shirt.ndjson...\n",
            "- [1/1 files][ 41.3 MiB/ 41.3 MiB] 100% Done                                    \n",
            "Operation completed over 1 objects/41.3 MiB.                                     \n",
            "gs://quickdraw_dataset/full/simplified/rabbit.ndjson\n",
            "Copying gs://quickdraw_dataset/full/simplified/rabbit.ndjson...\n",
            "/ [1/1 files][ 95.0 MiB/ 95.0 MiB] 100% Done                                    \n",
            "Operation completed over 1 objects/95.0 MiB.                                     \n",
            "adding from:  t-shirt\n",
            "adding from:  rabbit\n",
            "building  train  dataset\n",
            "adding from:  t-shirt\n",
            "adding from:  rabbit\n",
            "building  val  dataset\n",
            "adding from:  t-shirt\n",
            "adding from:  rabbit\n",
            "building test dataset\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XcYm7y8rECqg"
      },
      "source": [
        "## CapsNet Modules"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VdGH3V4jJPmQ"
      },
      "source": [
        "class PrimaryCapsules(nn.Module):\n",
        "    '''\n",
        "    The `PrimaryCaps` layer consists of 32 capsule units. Each unit takes\n",
        "    the output of the `Conv1` layer, which is a `[256, 20, 20]` feature\n",
        "    tensor (omitting `batch_size`), and performs a 2D convolution with 8\n",
        "    output channels, kernel size 9 and stride 2, thus outputing a [8, 6, 6]\n",
        "    tensor. In other words, you can see these 32 capsules as 32 paralleled 2D\n",
        "    convolutional layers. Then we concatenate these 32 capsules' outputs and\n",
        "    flatten them into a tensor of size `[1152, 8]`, representing 1152 8D\n",
        "    vectors, and send it to the next layer `DigitCaps`.\n",
        "    As indicated in Section 4, Page 4 in the paper, *One can see PrimaryCaps\n",
        "    as a Convolution layer with Eq.1 as its block non-linearity.*, outputs of\n",
        "    the `PrimaryCaps` layer are squashed before being passed to the next layer.\n",
        "    Reference: Section 4, Fig. 1\n",
        "    '''\n",
        "\n",
        "    def __init__(self):\n",
        "        '''\n",
        "        We build 8 capsule units in the `PrimaryCaps` layer, each can be\n",
        "        seen as a 2D convolution layer.\n",
        "        '''\n",
        "        super(PrimaryCapsules, self).__init__()\n",
        "\n",
        "        self.capsules = nn.ModuleList([\n",
        "            nn.Conv2d(in_channels=256,\n",
        "                      out_channels=8,\n",
        "                      kernel_size=9,\n",
        "                      stride=2)\n",
        "            for i in range(32)\n",
        "        ])\n",
        "\n",
        "    def forward(self, x):\n",
        "        '''\n",
        "        Each capsule outputs a [batch_size, 8, 6, 6] tensor, we need to\n",
        "        flatten and concatenate them into a [batch_size, 8, 6*6, 32] size\n",
        "        tensor and flatten and transpose into `u` [batch_size, 1152, 8], \n",
        "        where each [batch_size, 1152, 1] size tensor is the `u_i` in Eq.2. \n",
        "        #### Dimension transformation in this layer(ignoring `batch_size`):\n",
        "        [256, 20, 20] --> [8, 6, 6] x 32 capsules --> [1152, 8]\n",
        "        Note: `u_i` is one [1, 8] in the final [1152, 8] output, thus there are\n",
        "        1152 `u_i`s.\n",
        "        '''\n",
        "        batch_size = x.size(0)\n",
        "\n",
        "        u = []\n",
        "        for i in range(32):\n",
        "            # Input: [batch_size, 256, 20, 20]\n",
        "            assert x.data.size() == torch.Size([batch_size, 256, 20, 20]), x.data.size()\n",
        "\n",
        "            u_i = self.capsules[i](x)\n",
        "            assert u_i.size() == torch.Size([batch_size, 8, 6, 6])\n",
        "            # u_i: [batch_size, 8, 6, 6]\n",
        "            u_i = u_i.view(batch_size, 8, -1, 1)\n",
        "            # u_i: [batch_size, 8, 36]\n",
        "            u.append(u_i)\n",
        "\n",
        "        # u: [batch_size, 8, 36, 1] x 32\n",
        "        u = torch.cat(u, dim=3)\n",
        "        # u: [batch_size, 8, 36, 32]\n",
        "        u = u.view(batch_size, 8, -1)\n",
        "        # u: [batch_size, 8, 1152]\n",
        "        u = torch.transpose(u, 1, 2)\n",
        "        # u: [batch_size, 1152, 8]\n",
        "        assert u.data.size() == torch.Size([batch_size, 1152, 8])\n",
        "\n",
        "        # Squash before output\n",
        "        u_squashed = self.squash(u)\n",
        "\n",
        "        return u_squashed\n",
        "\n",
        "    def squash(self, u):\n",
        "        '''\n",
        "        Args:\n",
        "            `u`: [batch_size, 1152, 8]\n",
        "        Return:\n",
        "            `u_squashed`: [batch_size, 1152, 8]\n",
        "        In CapsNet, we use the squash function after the output of both \n",
        "        capsule layers. Squash functions can be seen as activating functions\n",
        "        like sigmoid, but for capsule layers rather than traditional fully\n",
        "        connected layers, as they squash vectors instead of scalars.\n",
        "        v_j = (norm(s_j) ^ 2 / (1 + norm(s_j) ^ 2)) * (s_j / norm(s_j))\n",
        "        Reference: Eq.1 in Section 2.\n",
        "        '''\n",
        "        batch_size = u.size(0)\n",
        "\n",
        "        # u: [batch_size, 1152, 8]\n",
        "        square = u ** 2\n",
        "\n",
        "        # square_sum for u: [batch_size, 1152]\n",
        "        square_sum = torch.sum(square, dim=2)\n",
        "\n",
        "        # norm for u: [batch_size, 1152]\n",
        "        norm = torch.sqrt(square_sum)\n",
        "\n",
        "        # factor for u: [batch_size, 1152]\n",
        "        factor = norm ** 2 / (norm * (1 + norm ** 2))\n",
        "\n",
        "        # u_squashed: [batch_size, 1152, 8]\n",
        "        u_squashed = factor.unsqueeze(2) * u\n",
        "        assert u_squashed.size() == torch.Size([batch_size, 1152, 8])\n",
        "\n",
        "        return u_squashed"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xz1tFdzkaHyP"
      },
      "source": [
        "class DoodleCapsules(nn.Module):\n",
        "    '''\n",
        "    The `DigitCaps` layer consists of 10 16D capsules. Compared to the traditional\n",
        "    scalar output neurons in fully connected networks(FCN), the `DigitCaps` layer\n",
        "    can be seen as an FCN with ten 16-dimensional output neurons, which we call\n",
        "    these neurons \"capsules\".\n",
        "    In this layer, we take the input `[1152, 8]` tensor `u` as 1152 [8,] vectors\n",
        "    `u_i`, each `u_i` is a 8D output of the capsules from `PrimaryCaps` (see Eq.2\n",
        "    in Section 2, Page 2) and sent to the 10 capsules. For each capsule, the tensor\n",
        "    is first transformed by `W_ij`s into [1152, 16] size. Then we perform the Dynamic\n",
        "    Routing algorithm to get the output `v_j` of size [16,]. As there are 10 capsules,\n",
        "    the final output is [16, 10] size.\n",
        "    #### Dimension transformation in this layer(ignoring `batch_size`):\n",
        "    [1152, 8] --> [1152, 16] --> [1, 16] x 10 capsules --> [10, 16] output\n",
        "    Note that in our codes we have vectorized these computations, so the dimensions\n",
        "    above are just for understanding, actual dimensions of tensors are different.\n",
        "    '''\n",
        "\n",
        "    def __init__(self, opt):\n",
        "        '''\n",
        "        There is only one parameter in this layer, `W` [1, 1152, 10, 16, 8], where\n",
        "        every [8, 16] is a weight matrix W_ij in Eq.2, that is, there are 11520\n",
        "        `W_ij`s in total.\n",
        "        The the coupling coefficients `b` [64, 1152, 10, 1] is a temporary variable which\n",
        "        does NOT belong to the layer's parameters. In other words, `b` is not updated\n",
        "        by gradient back-propagations. Instead, we update `b` by Dynamic Routing\n",
        "        in every forward propagation. See the docstring of `self.forward` for details.\n",
        "        '''\n",
        "        super(DoodleCapsules, self).__init__()\n",
        "        self.opt = opt\n",
        "\n",
        "        self.W = nn.Parameter(torch.randn(1, 1152, opt.n_classes, 8, 16))\n",
        "\n",
        "    def forward(self, u):\n",
        "        '''\n",
        "        Args:\n",
        "            `u`: [batch_size, 1152, 8]\n",
        "        Return:\n",
        "            `v`: [batch_size, 10, 16]\n",
        "        In this layer, we vectorize our computations by calling `W` and using\n",
        "        `torch.matmul()`. Thus the full computaion steps are as follows.\n",
        "            1. Expand `W` into batches and compute `u_hat` (Eq.2)\n",
        "            2. Line 2: Initialize `b` into zeros\n",
        "            3. Line 3: Start Routing for `r` iterations:\n",
        "                1. Line 4: c = softmax(b)\n",
        "                2. Line 5: s = sum(c * u_hat)\n",
        "                3. Line 6: v = squash(s)\n",
        "                4. Line 7: b += u_hat * v\n",
        "        The coupling coefficients `b` can be seen as a kind of attention matrix\n",
        "        in the attentional sequence-to-sequence networks, which is widely used in\n",
        "        Neural Machine Translation systems. For tutorials on  attentional seq2seq\n",
        "        models, see https://arxiv.org/abs/1703.01619 or\n",
        "        http://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html\n",
        "        Reference: Section 2, Procedure 1\n",
        "        '''\n",
        "        batch_size = u.size(0)\n",
        "\n",
        "        # First, we need to expand the dimensions of `W` and `u` to compute `u_hat`\n",
        "        assert u.size() == torch.Size([batch_size, 1152, 8])\n",
        "        # u: [batch_size, 1152, 1, 1, 8]\n",
        "        u = torch.unsqueeze(u, dim=2)\n",
        "        u = torch.unsqueeze(u, dim=2)\n",
        "        # Now we compute u_hat in Eq.2\n",
        "        # u_hat: [batch_size, 1152, 10, 16]\n",
        "        u_hat = torch.matmul(u, self.W).squeeze()\n",
        "\n",
        "        # Line 2: Initialize b into zeros\n",
        "        # b: [batch_size, 1152, 10, 1]\n",
        "        b = Variable(torch.zeros(batch_size, 1152, opt.n_classes, 1))\n",
        "        if self.opt.use_cuda & torch.cuda.is_available():\n",
        "            b = b.cuda()\n",
        "\n",
        "        # Start Routing\n",
        "        for r in range(self.opt.r):\n",
        "            # Line 4: c_i = softmax(b_i)\n",
        "            # c: [b, 1152, 10, 1]\n",
        "            c = F.softmax(b, dim=2)\n",
        "            assert c.size() == torch.Size([batch_size, 1152, opt.n_classes, 1])\n",
        "\n",
        "            # Line 5: s_j = sum_i(c_ij * u_hat_j|i)\n",
        "            # u_hat: [batch_size, 1152, 10, 16]\n",
        "            # s: [batch_size, 10, 16]\n",
        "            s = torch.sum(u_hat * c, dim=1)\n",
        "\n",
        "            # Line 6: v_j = squash(s_j)\n",
        "            # v: [batch_size, 10, 16]\n",
        "            v = self.squash(s)\n",
        "            assert v.size() == torch.Size([batch_size, opt.n_classes, 16])\n",
        "\n",
        "            # Line 7: b_ij += u_hat * v_j\n",
        "            # u_hat: [batch_size, 1152, 10, 16]\n",
        "            # v: [batch_size, 10, 16]\n",
        "            # a: [batch_size, 10, 1152, 16]\n",
        "            a = u_hat * v.unsqueeze(1)\n",
        "            # b: [batch_size, 1152, 10, 1]\n",
        "            b = b + torch.sum(a, dim=3, keepdim=True)\n",
        "\n",
        "        return v\n",
        "\n",
        "    def squash(self, s):\n",
        "        '''\n",
        "        Args:\n",
        "            `s`: [batch_size, 10, 16]\n",
        "        v_j = (norm(s_j) ^ 2 / (1 + norm(s_j) ^ 2)) * (s_j / norm(s_j))\n",
        "        Reference: Eq.1 in Section 2.\n",
        "        '''\n",
        "        batch_size = s.size(0)\n",
        "\n",
        "        # s: [batch_size, 10, 16]\n",
        "        square = s ** 2\n",
        "\n",
        "        # square_sum for v: [batch_size, 10]\n",
        "        square_sum = torch.sum(square, dim=2)\n",
        "\n",
        "        # norm for v: [batch_size, 10]\n",
        "        norm = torch.sqrt(square_sum)\n",
        "\n",
        "        # factor for v: [batch_size, 10]\n",
        "        factor = norm ** 2 / (norm * (1 + norm ** 2))\n",
        "\n",
        "        # v: [batch_size, 10, 16]\n",
        "        v = factor.unsqueeze(2) * s\n",
        "        assert v.size() == torch.Size([batch_size, opt.n_classes, 16])\n",
        "\n",
        "        return v"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q-k05aXUmDs8"
      },
      "source": [
        "class DoodleDecoder(nn.Module):\n",
        "    '''\n",
        "    The decoder network consists of 3 fully connected layers. For each\n",
        "    [10, 16] output, we mask out the incorrect predictions, and send\n",
        "    the [16,] vector to the decoder network to reconstruct a [784,] size\n",
        "    image.\n",
        "    Reference: Section 4.1, Fig. 2\n",
        "    '''\n",
        "\n",
        "    def __init__(self, opt):\n",
        "        '''\n",
        "        The decoder network consists of 3 fully connected layers, with\n",
        "        512, 1024, 784 neurons each.\n",
        "        '''\n",
        "        super(DoodleDecoder, self).__init__()\n",
        "        self.opt = opt\n",
        "\n",
        "        self.fc1 = nn.Linear(16, 512)\n",
        "        self.fc2 = nn.Linear(512, 1024)\n",
        "        self.fc3 = nn.Linear(1024, opt.image_size*opt.image_size)\n",
        "\n",
        "    def forward(self, v, target):\n",
        "        '''\n",
        "        Args:\n",
        "            `v`: [batch_size, 10, 16]\n",
        "            `target`: [batch_size, 10]\n",
        "        Return:\n",
        "            `reconstruction`: [batch_size, 784]\n",
        "        We send the outputs of the `DigitCaps` layer, which is a\n",
        "        [batch_size, 10, 16] size tensor into the decoder network, and\n",
        "        reconstruct a [batch_size, 784] size tensor representing the image.\n",
        "        '''\n",
        "        batch_size = target.size(0)\n",
        "\n",
        "        target = target.type(torch.FloatTensor)\n",
        "        # mask: [batch_size, 10, 16]\n",
        "        mask = torch.stack([target for i in range(16)], dim=2)\n",
        "        assert mask.size() == torch.Size([batch_size, opt.n_classes, 16])\n",
        "        if self.opt.use_cuda & torch.cuda.is_available():\n",
        "            mask = mask.cuda()\n",
        "\n",
        "        # v: [bath_size, 10, 16]\n",
        "        v_masked = mask * v\n",
        "        v_masked = torch.sum(v_masked, dim=1)\n",
        "        assert v_masked.size() == torch.Size([batch_size, 16])\n",
        "\n",
        "        # Forward\n",
        "        v = F.relu(self.fc1(v_masked))\n",
        "        v = F.relu(self.fc2(v))\n",
        "        reconstruction = torch.sigmoid(self.fc3(v))\n",
        "\n",
        "        assert reconstruction.size() == torch.Size([batch_size, opt.image_size*opt.image_size])\n",
        "        return reconstruction"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CH6pouLtXQnx"
      },
      "source": [
        "class CapsuleNetwork(nn.Module):\n",
        "    '''Consists of a ReLU Convolution layer, a PrimaryCapsules layer, a DoodleCapsules\n",
        "    layer, and a Decoder layer. Section 4 of the paper.\n",
        "    '''\n",
        "\n",
        "    def __init__(self, opt):\n",
        "        super(CapsuleNetwork, self).__init__()\n",
        "        self.opt = opt\n",
        "\n",
        "        self.Conv1 = nn.Conv2d(in_channels=1, out_channels=256, kernel_size=21)\n",
        "        self.PrimaryCaps = PrimaryCapsules()\n",
        "        self.DigitCaps = DoodleCapsules(opt)\n",
        "\n",
        "        self.Decoder = DoodleDecoder(opt)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        '''\n",
        "        Args:\n",
        "            `x`: [batch_size, 1, 28, 28] MNIST samples\n",
        "        \n",
        "        Return:\n",
        "            `v`: [batch_size, 10, 16] CapsNet outputs, 16D prediction vectors of\n",
        "                10 digit capsules\n",
        "        The dimension transformation procedure of an input tensor in each layer:\n",
        "            0. Input: [batch_size, 1, 28, 28] -->\n",
        "            1. `Conv1` --> [batch_size, 256, 20, 20] --> \n",
        "            2. `PrimaryCaps` --> [batch_size, 8, 6, 6] x 32 capsules --> \n",
        "            3. Flatten, concatenate, squash --> [batch_size, 1152, 8] -->\n",
        "            4. `W_ij`s and `DigitCaps` --> [batch_size, 16, 10] -->\n",
        "            5. Length of 10 capsules --> [batch_size, 10] output probabilities\n",
        "        '''\n",
        "        # Input: [batch_size, 1, 28, 28]\n",
        "        x = x.view(-1, 1, opt.image_size, opt.image_size)\n",
        "        x = F.relu(self.Conv1(x))\n",
        "        # PrimaryCaps input: [batch_size, 256, 20, 20]\n",
        "        u = self.PrimaryCaps(x)\n",
        "        # PrimaryCaps output u: [batch_size, 1152, 8]\n",
        "        v = self.DigitCaps(u)\n",
        "        # DigitCaps output v: [batsh_size, 10, 16]\n",
        "        return v\n",
        "\n",
        "    def marginal_loss(self, v, target, l=0.5):\n",
        "        '''\n",
        "        Args:\n",
        "            `v`: [batch_size, 10, 16]\n",
        "            `target`: [batch_size, 10]\n",
        "            `l`: Scalar, lambda for down-weighing the loss for absent digit classes\n",
        "        Return:\n",
        "            `marginal_loss`: Scalar\n",
        "        \n",
        "        L_c = T_c * max(0, m_plus - norm(v_c)) ^ 2 + lambda * (1 - T_c) * max(0, norm(v_c) - m_minus) ^2\n",
        "        \n",
        "        Reference: Eq.4 in Section 3.\n",
        "        '''\n",
        "        batch_size = v.size(0)\n",
        "\n",
        "        square = v ** 2\n",
        "        square_sum = torch.sum(square, dim=2)\n",
        "        # norm: [batch_size, 10]\n",
        "        norm = torch.sqrt(square_sum)\n",
        "        assert norm.size() == torch.Size([batch_size, 2])\n",
        "\n",
        "        # The two T_c in Eq.4\n",
        "        T_c = target.type(torch.FloatTensor)\n",
        "        zeros = Variable(torch.zeros(norm.size()))\n",
        "        # Use GPU if available\n",
        "        if self.opt.use_cuda & torch.cuda.is_available():\n",
        "            zeros = zeros.cuda()\n",
        "            T_c = T_c.cuda()\n",
        "\n",
        "        # Eq.4\n",
        "        marginal_loss = T_c * (torch.max(zeros, 0.9 - norm) ** 2) + \\\n",
        "            (1 - T_c) * l * (torch.max(zeros, norm - 0.1) ** 2)\n",
        "        marginal_loss = torch.sum(marginal_loss)\n",
        "\n",
        "        return marginal_loss\n",
        "\n",
        "    def reconstruction_loss(self, reconstruction, image):\n",
        "        '''\n",
        "        Args:\n",
        "            `reconstruction`: [batch_size, 784] Decoder outputs of images\n",
        "            `image`: [batch_size, 1, 28, 28] MNIST samples\n",
        "        Return:\n",
        "            `reconstruction_loss`: Scalar Variable\n",
        "        The reconstruction loss is measured by a squared differences\n",
        "        between the reconstruction and the original image. \n",
        "        Reference: Section 4.1\n",
        "        '''\n",
        "        batch_size = image.size(0)\n",
        "        # image: [batch_size, 784]\n",
        "        image = image.view(batch_size, -1)\n",
        "        assert image.size() == (batch_size, opt.image_size*opt.image_size)\n",
        "        \n",
        "        # Scalar Variable\n",
        "        reconstruction_loss = torch.sum((reconstruction - image) ** 2)\n",
        "        return reconstruction_loss\n",
        "\n",
        "    def loss(self, v, image, target):\n",
        "        '''\n",
        "        Args:\n",
        "            `v`: [batch_size, 10, 16] CapsNet outputs\n",
        "            `target`: [batch_size, 10] One-hot MNIST labels\n",
        "            `image`: [batch_size, 1, 28, 28] MNIST samples\n",
        "        Return:\n",
        "            `L`: Scalar Variable, total loss\n",
        "            `marginal_loss`: Scalar Variable\n",
        "            `reconstruction_loss`: Scalar Variable\n",
        "        The reconstruction loss is scaled down by 5e-4, serving as a\n",
        "        regularization method.\n",
        "        Reference: Section 4.1\n",
        "        '''\n",
        "        batch_size = image.size(0)\n",
        "\n",
        "        marginal_loss = self.marginal_loss(v, target)\n",
        "\n",
        "        # Get reconstructions from the decoder network\n",
        "        reconstruction = self.Decoder(v, target)\n",
        "        reconstruction_loss = self.reconstruction_loss(reconstruction, image)\n",
        "\n",
        "        # Scalar Variable\n",
        "        loss = (marginal_loss + 0.0005 * reconstruction_loss) / batch_size\n",
        "\n",
        "        return loss, marginal_loss / batch_size, reconstruction_loss / batch_size"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zKPXAofhnYhK"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QQCM0T0_1d5W"
      },
      "source": [
        "def evaluate(opt, valid_loader, model, type_data, plot=False):\n",
        "    sum_loss = 0\n",
        "    sum_marginal_loss = 0\n",
        "    sum_reconstruction_loss = 0\n",
        "    correct = 0\n",
        "    num_sample = len(valid_loader.dataset)\n",
        "    num_batch = len(valid_loader)\n",
        "\n",
        "    model.eval()\n",
        "    for data, target in valid_loader:\n",
        "        data = data.to(torch.float32)\n",
        "        target = target.to(torch.int64)\n",
        "        batch_size = data.size(0)\n",
        "        assert target.size() == torch.Size([batch_size, opt.n_classes])\n",
        "\n",
        "        with torch.no_grad():\n",
        "            data, target = Variable(data), Variable(target)\n",
        "        if opt.use_cuda & torch.cuda.is_available():\n",
        "            data, target = data.cuda(), target.cuda()\n",
        "\n",
        "        output = model(data)  # (batch_size, n_classes, 16)\n",
        "        loss, marginal_loss, reconstruction_loss = model.loss(output, data, target)\n",
        "        sum_loss += loss.item()\n",
        "        sum_marginal_loss += marginal_loss.item()\n",
        "        sum_reconstruction_loss += reconstruction_loss.item()\n",
        "\n",
        "        norms = torch.sqrt(torch.sum(output**2, dim=2))  # (batch_size, n_classes)\n",
        "        pred = norms.data.max(1, keepdim=True)[1].type(torch.LongTensor)  # (batch_size, 1)\n",
        "        label = target.max(1, keepdim=True)[1].type(torch.LongTensor)  # (batch_size, 1)\n",
        "        correct += pred.eq(label.view_as(pred)).cpu().sum().item()\n",
        "\n",
        "    if plot:\n",
        "        recons = model.Decoder(output, target)\n",
        "        recons = recons.view(batch_size, opt.image_size, opt.image_size)\n",
        "        recons = recons[0].cpu()\n",
        "        plt.imshow(recons.detach(), cmap = \"gray\")\n",
        "\n",
        "    sum_loss /= num_batch\n",
        "    sum_marginal_loss /= num_batch\n",
        "    sum_reconstruction_loss /= num_batch\n",
        "    print('\\n{} loss: {:.4f}   Marginal loss: {:.4f}   Reconstruction loss: {:.4f}'.format(\n",
        "        type_data, sum_loss, sum_marginal_loss, sum_reconstruction_loss))\n",
        "    print('Accuracy: {}/{} {:.4f}\\n'.format(correct, num_sample,\n",
        "        correct / num_sample))"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jcoOvQfb1bVn"
      },
      "source": [
        "def train(opt, train_loader, valid_loader, model):\n",
        "    num_sample = len(train_loader.dataset)\n",
        "    num_batches = len(train_loader)\n",
        "    train_loss_list = []\n",
        "    loss_val = 0.\n",
        "\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=opt.lr)\n",
        "    scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, opt.gamma)\n",
        "    model.train()\n",
        "    for epoch in range(opt.epochs):\n",
        "    \n",
        "        for batch_idx, (data, target) in enumerate(train_loader):\n",
        "            optimizer.zero_grad()\n",
        "            data = data.to(torch.float32)\n",
        "            \n",
        "            batch_size = data.size(0)\n",
        "            target = target.to(torch.int64)\n",
        "            assert target.size() == torch.Size([batch_size, opt.n_classes])\n",
        "\n",
        "            # Use GPU if available\n",
        "            with torch.no_grad():\n",
        "                data, target = Variable(data), Variable(target)\n",
        "            if opt.use_cuda & torch.cuda.is_available():\n",
        "                data, target = data.cuda(), target.cuda()\n",
        "            \n",
        "            output = model(data)\n",
        "            loss, marginal_loss, reconstruction_loss = model.loss(output, data, target)\n",
        "            loss_val = loss.item()\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        if epoch % 5 == 0: \n",
        "            train_loss_list.append(loss_val)\n",
        "        if epoch % 1 == 0:\n",
        "            print('Epoch: {}'.format(epoch))\n",
        "            print('Learning rate: {:.2e}'.format(scheduler.get_last_lr()[0]))\n",
        "            evaluate(opt, train_loader, model, 'TRAIN', False) \n",
        "            evaluate(opt, valid_loader, model, 'VALID', False)\n",
        "\n",
        "        # torch.save({\n",
        "        #     'epoch': epoch,\n",
        "        #     'model_state_dict': model.state_dict(),\n",
        "        #     'optimizer_state_dict': optimizer.state_dict(),\n",
        "        #     'loss': loss,\n",
        "        #     }, 'guide_big_model.pt')\n",
        "        \n",
        "        scheduler.step()\n",
        "    fig = plt.figure()\n",
        "    plt.plot([i for i in range(len(train_loss_list))], train_loss_list, '-')"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Holn-KGHnvZH"
      },
      "source": [
        "def get_opts():\n",
        "    parser = argparse.ArgumentParser(description='CapsuleNetwork')\n",
        "    parser.add_argument('-image_size', type=int, default=40)\n",
        "    parser.add_argument('-batch_size', type=int, default=64)\n",
        "    parser.add_argument('-lr', type=float, default=1e-6)\n",
        "    parser.add_argument('-epochs', type=int, default=20)\n",
        "    parser.add_argument('-n_classes', type=int, default=2)\n",
        "    parser.add_argument('-use_cuda', default=True)\n",
        "    parser.add_argument('-gamma', type=float, default=0.8)\n",
        "    parser.add_argument('-r', type=int, default=4)\n",
        "    opt, _ = parser.parse_known_args()\n",
        "    return opt"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "4zXW1i3p1uq8",
        "outputId": "ea3fdf3c-dcef-494e-d5fa-1cbff6c7d24d"
      },
      "source": [
        "opt = get_opts()\n",
        "\n",
        "model = CapsuleNetwork(opt)\n",
        "if opt.use_cuda & torch.cuda.is_available():\n",
        "    model.cuda()\n",
        "\n",
        "train_loader = loader.train_loader\n",
        "valid_loader = loader.val_loader\n",
        "\n",
        "train(opt, train_loader, valid_loader, model)"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 0\n",
            "Learning rate: 1.00e-06\n",
            "\n",
            "TRAIN loss: 3533.2299   Marginal loss: 0.4050   Reconstruction loss: 7065649.4397\n",
            "Accuracy: 10152/18000 0.5640\n",
            "\n",
            "\n",
            "VALID loss: 3533.7343   Marginal loss: 0.4050   Reconstruction loss: 7066658.1543\n",
            "Accuracy: 3418/6000 0.5697\n",
            "\n",
            "Epoch: 1\n",
            "Learning rate: 8.00e-07\n",
            "\n",
            "TRAIN loss: 3533.2375   Marginal loss: 0.4050   Reconstruction loss: 7065664.6915\n",
            "Accuracy: 9750/18000 0.5417\n",
            "\n",
            "\n",
            "VALID loss: 3533.5747   Marginal loss: 0.4050   Reconstruction loss: 7066338.9787\n",
            "Accuracy: 3251/6000 0.5418\n",
            "\n",
            "Epoch: 2\n",
            "Learning rate: 6.40e-07\n",
            "\n",
            "TRAIN loss: 3532.4260   Marginal loss: 0.4050   Reconstruction loss: 7064041.5213\n",
            "Accuracy: 9740/18000 0.5411\n",
            "\n",
            "\n",
            "VALID loss: 3533.4255   Marginal loss: 0.4050   Reconstruction loss: 7066040.5532\n",
            "Accuracy: 3247/6000 0.5412\n",
            "\n",
            "Epoch: 3\n",
            "Learning rate: 5.12e-07\n",
            "\n",
            "TRAIN loss: 3532.1341   Marginal loss: 0.4050   Reconstruction loss: 7063457.8333\n",
            "Accuracy: 9681/18000 0.5378\n",
            "\n",
            "\n",
            "VALID loss: 3533.2914   Marginal loss: 0.4050   Reconstruction loss: 7065772.2926\n",
            "Accuracy: 3260/6000 0.5433\n",
            "\n",
            "Epoch: 4\n",
            "Learning rate: 4.10e-07\n",
            "\n",
            "TRAIN loss: 3532.0387   Marginal loss: 0.4050   Reconstruction loss: 7063267.0957\n",
            "Accuracy: 9736/18000 0.5409\n",
            "\n",
            "\n",
            "VALID loss: 3533.1749   Marginal loss: 0.4050   Reconstruction loss: 7065539.4309\n",
            "Accuracy: 3263/6000 0.5438\n",
            "\n",
            "Epoch: 5\n",
            "Learning rate: 3.28e-07\n",
            "\n",
            "TRAIN loss: 3531.9635   Marginal loss: 0.4050   Reconstruction loss: 7063116.6312\n",
            "Accuracy: 9811/18000 0.5451\n",
            "\n",
            "\n",
            "VALID loss: 3533.0768   Marginal loss: 0.4050   Reconstruction loss: 7065343.2660\n",
            "Accuracy: 3268/6000 0.5447\n",
            "\n",
            "Epoch: 6\n",
            "Learning rate: 2.62e-07\n",
            "\n",
            "TRAIN loss: 3531.2665   Marginal loss: 0.4050   Reconstruction loss: 7061722.5443\n",
            "Accuracy: 9820/18000 0.5456\n",
            "\n",
            "\n",
            "VALID loss: 3532.9958   Marginal loss: 0.4050   Reconstruction loss: 7065181.1596\n",
            "Accuracy: 3303/6000 0.5505\n",
            "\n",
            "Epoch: 7\n",
            "Learning rate: 2.10e-07\n",
            "\n",
            "TRAIN loss: 3531.0145   Marginal loss: 0.4050   Reconstruction loss: 7061218.5638\n",
            "Accuracy: 9791/18000 0.5439\n",
            "\n",
            "\n",
            "VALID loss: 3532.9297   Marginal loss: 0.4050   Reconstruction loss: 7065049.0319\n",
            "Accuracy: 3280/6000 0.5467\n",
            "\n",
            "Epoch: 8\n",
            "Learning rate: 1.68e-07\n",
            "\n",
            "TRAIN loss: 3531.7323   Marginal loss: 0.4050   Reconstruction loss: 7062654.2465\n",
            "Accuracy: 9824/18000 0.5458\n",
            "\n",
            "\n",
            "VALID loss: 3532.8762   Marginal loss: 0.4050   Reconstruction loss: 7064942.0691\n",
            "Accuracy: 3299/6000 0.5498\n",
            "\n",
            "Epoch: 9\n",
            "Learning rate: 1.34e-07\n",
            "\n",
            "TRAIN loss: 3531.9167   Marginal loss: 0.4050   Reconstruction loss: 7063023.0301\n",
            "Accuracy: 9797/18000 0.5443\n",
            "\n",
            "\n",
            "VALID loss: 3532.8332   Marginal loss: 0.4050   Reconstruction loss: 7064855.9415\n",
            "Accuracy: 3296/6000 0.5493\n",
            "\n",
            "Epoch: 10\n",
            "Learning rate: 1.07e-07\n",
            "\n",
            "TRAIN loss: 3531.6653   Marginal loss: 0.4050   Reconstruction loss: 7062520.2287\n",
            "Accuracy: 9826/18000 0.5459\n",
            "\n",
            "\n",
            "VALID loss: 3532.7986   Marginal loss: 0.4050   Reconstruction loss: 7064786.9043\n",
            "Accuracy: 3316/6000 0.5527\n",
            "\n",
            "Epoch: 11\n",
            "Learning rate: 8.59e-08\n",
            "\n",
            "TRAIN loss: 3530.7889   Marginal loss: 0.4050   Reconstruction loss: 7060767.3298\n",
            "Accuracy: 9775/18000 0.5431\n",
            "\n",
            "\n",
            "VALID loss: 3532.7710   Marginal loss: 0.4050   Reconstruction loss: 7064731.6064\n",
            "Accuracy: 3306/6000 0.5510\n",
            "\n",
            "Epoch: 12\n",
            "Learning rate: 6.87e-08\n",
            "\n",
            "TRAIN loss: 3531.0134   Marginal loss: 0.4050   Reconstruction loss: 7061216.3670\n",
            "Accuracy: 9754/18000 0.5419\n",
            "\n",
            "\n",
            "VALID loss: 3532.7489   Marginal loss: 0.4050   Reconstruction loss: 7064687.4255\n",
            "Accuracy: 3280/6000 0.5467\n",
            "\n",
            "Epoch: 13\n",
            "Learning rate: 5.50e-08\n",
            "\n",
            "TRAIN loss: 3530.6068   Marginal loss: 0.4050   Reconstruction loss: 7060403.2252\n",
            "Accuracy: 9740/18000 0.5411\n",
            "\n",
            "\n",
            "VALID loss: 3532.7312   Marginal loss: 0.4050   Reconstruction loss: 7064652.0798\n",
            "Accuracy: 3274/6000 0.5457\n",
            "\n",
            "Epoch: 14\n",
            "Learning rate: 4.40e-08\n",
            "\n",
            "TRAIN loss: 3531.0058   Marginal loss: 0.4050   Reconstruction loss: 7061201.2908\n",
            "Accuracy: 9768/18000 0.5427\n",
            "\n",
            "\n",
            "VALID loss: 3532.7172   Marginal loss: 0.4050   Reconstruction loss: 7064623.9202\n",
            "Accuracy: 3260/6000 0.5433\n",
            "\n",
            "Epoch: 15\n",
            "Learning rate: 3.52e-08\n",
            "\n",
            "TRAIN loss: 3530.9152   Marginal loss: 0.4050   Reconstruction loss: 7061019.9557\n",
            "Accuracy: 9726/18000 0.5403\n",
            "\n",
            "\n",
            "VALID loss: 3532.7059   Marginal loss: 0.4050   Reconstruction loss: 7064601.4043\n",
            "Accuracy: 3258/6000 0.5430\n",
            "\n",
            "Epoch: 16\n",
            "Learning rate: 2.81e-08\n",
            "\n",
            "TRAIN loss: 3531.2963   Marginal loss: 0.4050   Reconstruction loss: 7061782.1170\n",
            "Accuracy: 9787/18000 0.5437\n",
            "\n",
            "\n",
            "VALID loss: 3532.6969   Marginal loss: 0.4050   Reconstruction loss: 7064583.4255\n",
            "Accuracy: 3254/6000 0.5423\n",
            "\n",
            "Epoch: 17\n",
            "Learning rate: 2.25e-08\n",
            "\n",
            "TRAIN loss: 3531.0509   Marginal loss: 0.4050   Reconstruction loss: 7061291.4628\n",
            "Accuracy: 9703/18000 0.5391\n",
            "\n",
            "\n",
            "VALID loss: 3532.6897   Marginal loss: 0.4050   Reconstruction loss: 7064569.0479\n",
            "Accuracy: 3264/6000 0.5440\n",
            "\n",
            "Epoch: 18\n",
            "Learning rate: 1.80e-08\n",
            "\n",
            "TRAIN loss: 3532.1994   Marginal loss: 0.4050   Reconstruction loss: 7063588.4929\n",
            "Accuracy: 9732/18000 0.5407\n",
            "\n",
            "\n",
            "VALID loss: 3532.6840   Marginal loss: 0.4050   Reconstruction loss: 7064557.6011\n",
            "Accuracy: 3240/6000 0.5400\n",
            "\n",
            "Epoch: 19\n",
            "Learning rate: 1.44e-08\n",
            "\n",
            "TRAIN loss: 3531.2961   Marginal loss: 0.4050   Reconstruction loss: 7061781.7926\n",
            "Accuracy: 9743/18000 0.5413\n",
            "\n",
            "\n",
            "VALID loss: 3532.6794   Marginal loss: 0.4050   Reconstruction loss: 7064548.4255\n",
            "Accuracy: 3250/6000 0.5417\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAD4CAYAAAAAczaOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3deXxU9fX/8dfJHgJkI0ggLGEngGwx4I5YkEXQr6WK2Krd+FVFu9hFbRVQ6/Ktta2o9Wu/XawV96WAoGJxrQqETQiEEAlL2BISkhBC9vP7Yy58UwQzIZPcWc7z8ZjHY+beOzPvD5OcXM7c+7miqhhjjAkNYW4HMMYY036s6BtjTAixom+MMSHEir4xxoQQK/rGGBNCItwO8FW6dOmiffr0cTuGMcYElLVr1x5S1ZRTrfProt+nTx+ys7PdjmGMMQFFRHadbp21d4wxJoRY0TfGmBBiRd8YY0KIFX1jjAkhVvSNMSaEWNE3xpgQYkXfGGNCiF8fp2+MMaFEVfmiuJJVBaUAXDe2t8/fw4q+Mca4pKFRyT1QweqC0hO3kqO1AIzulWBF3xhjAlldQyM5+ypYtaOE1QWlrNlZSkV1PQBpibFcPCiFselJjE1PpndyhzbJYEXfGGPaSE19Axv3lLO6oIRVBaWs3XWYqtoGAPp2iWPa2alkpSeRlZ5Mj4TYdslkRd8YY3ykqrae9bvLWLXDU+TX7ymjtr4RgMHdOjFzTBpj05M5Jz2Rrp1iXMloRd8YY85QRXUda3ceZlVBKasKSthUWE59oxImMLR7PNeP601WehLn9EkiMS7K7biAFX1jjPFa6dHa//vSdWcJW/ZV0KgQGS6cnZbAnIv6kpWexJjeiXSKiXQ77ik1W/RFJAb4EIh2tn9FVeeJyKXAb/Ac618J3Kiq+SISDfwdGAOUANeo6k7nte4Evgs0ALep6tu+H5IxxvhGUUX1ib341QWl5B2sBCA6IozRvRK5dcIAxvZNYlTPRGKjwl1O6x1v9vRrgAmqWikikcDHIrIc+CNwhapuFZGbgV8BN+Ip6odVtb+IzAIeBq4RkQxgFjAU6A68KyIDVbXB98MyxpiWKzxcxaodx/fkSyk4dBSAuKhwxvRJ4oqRPRibnsTwtHiiIwKjyJ+s2aKvqopnTx4g0rmpc+vsLI8H9jn3rwDmO/dfAR4XEXGWv6CqNUCBiOQDWcCnrR+GMca0jKpScOgoqwtKWeW0bPaWHQMgPjaSc/okMTurF2P7JpGR2pmI8OCYwMCrnr6IhANrgf7AE6q6SkS+BywTkWNABTDO2bwHsAdAVetFpBxIdpZ/1uRlC51lJ7/XHGAOQK9evc5kTMYY8yWNjUpe0ZH/KPLFR2oA6NIxirHpySd68oPO6kRYmLicuG14VfSdFsxIEUkAXheRYcCPganOH4CfAY8C32ttIFV9GngaIDMzU1v7esaY0FTf0MjW/UdY5Rwjv2ZnKWVVdQCkxsdwfr9kxvZNJis9ib5d4vA0JIJfi47eUdUyEXkPmAKMUNVVzqoXgbec+3uBnkChiETgaf2UNFl+XJqzzBhjWq22vpFNe8s8X7zu8JwIVVnjOdu1d3IHJmWcRVZ6MmPTk0hLjA2ZIn8yb47eSQHqnIIfC0zE8+VsvPNFbJ6zbKvzlMXADXh69TOBlaqqIrIYWCQij+L5IncAsNrnIzLGhITqugbPiVDOkTXrdh+mus5zItSArh25YmR3z558nyS6xbtzIpQ/8mZPPxV4xunrhwEvqepSEfk+8KqINAKHge842/8ZeNb5orYUzxE7qGqOiLwEbAHqgVvsyB1jjLcqa+pZu+uwZ0qDHaVsLCyjrkERgYzUzlyb1YuxzolQyR2j3Y7rt8RzcI5/yszM1OzsbLdjGGNcUF5Vx+qdpax29uQ376ugoVEJDxOG94j3TEzWN4kxvZOIj/XPE6HcIiJrVTXzVOvsjFxjjF8oPlLDmp2lJ+at2XbwCKoQFRHGyJ4J3Dy+H1npSYzulUhctJWuM2X/csYYV+wrO3bi8MlVBSXsKPacCBUbGc6Y3olMG+6ZgXJEzwRiIgPzRCh/ZEXfGNPmVJXdpZ6zXVc589bsKfWcCNUpJoJz+iRxdWZPxqYnMaxHPJFBciKUP7Kib4zxOVUlv6jS2Yv39OUPVnhOhEqKiyKrTxLfPi+drPQkhqR2JjxIT4TyR1b0jTGtdvyyf03nrSl1LvvXtVM0Y/smO1eESqJ/144he4y8P7Cib4xpsbqGRjbvLT/Rk1+zs5QjTS77d8mgrieOrumV1MGKvB+xom+MaVZ1XQMb95Sd2Iv/j8v+pcRx+dmpzhWhktrtsn/mzFjRN8Z8SVVtPet2lZ2Yt2bDSZf9+8aYNLLSPfPWpHSyE6ECiRV9YwwV1XVk7/y/2SebXvZvWI94bji3N1npyZzTJ5GEDv5x2T9zZqzoGxPCDlXW8INn17J292HUuezfiAC57J85M1b0jQlhDy/PZWNhGbdOGMC4ALvsnzkzVvSNCVEb9pTx8tpC/t/FffnJxIFuxzHtxE57MyYENTYq8xbnkNIpmlsnDHA7jmlHVvSNCUGvritk454y7pwymI42eVlIsaJvTIipqK7j4be2MbpXAleO/NJlqk2Qsz/xxoSYhf/aTsnRGv564zlBe/Fvc3q2p29MCMkvOsJf/72TWef0ZHhavNtxjAus6BsTIlSVBUu2EBsVzk8nDXI7jnGJFX1jQsSKLQf5aPshfjJxoF1DNoRZ0TcmBFTXNXDfm1sYeFZHvjmut9txjIvsi1xjQsD/frSDPaXHeO57Y+2qVCHOPn1jgty+smM88d4XTBnWjfP7d3E7jnGZFX1jgtyDy3NpVOWuqUPcjmL8QLNFX0RiRGS1iGwUkRwRWeAs/0hENji3fSLyhrN8vIiUN1l3T5PXmiwi20QkX0TuaLthGWMAPttRwpKN+7hpfD96JnVwO47xA9709GuACapaKSKRwMcislxVLzy+gYi8CvyzyXM+UtXLm76IiIQDTwATgUJgjYgsVtUtrR6FMeZL6hsamb84hx4Jsfzg4n5uxzF+otk9ffWodB5GOjc9vl5EOgMTgDeaeaksIF9Vd6hqLfACcMUZpTbGNOv51bvJPXCEX00bQkykTZdsPLzq6YtIuIhsAIqAFaq6qsnqK4F/qWpFk2XnOu2g5SIy1FnWA9jTZJtCZ9nJ7zVHRLJFJLu4uLhFgzHGeBw+Wssj7+RxXr9kJg/r5nYc40e8Kvqq2qCqI4E0IEtEhjVZfS3wfJPH64DeqjoCWEjz/wM4+b2eVtVMVc1MSUlpyVONMY7frthGZU0986YPRcTm1zH/p0VH76hqGfAeMBlARLrgadu82WSbiuPtIFVdBkQ62+0FejZ5uTRnmTHGh3L2lbNo1W6+Na43g7p1cjuO8TPeHL2TIiIJzv1YPF/E5jqrZwJLVbW6yfbdxNm1EJEs5z1KgDXAABFJF5EoYBaw2JeDMSbUqSoLFm8hoUMUP7arYZlT8ObonVTgGefomzDgJVVd6qybBTx00vYzgZtEpB44BsxSVQXqRWQu8DYQDvxFVXN8MQhjjMfijftYvbOUh64aTnysXdDcfJl46rF/yszM1OzsbLdjGBMQjtbUc+lvPyClUzRv3HI+4TZXfsgSkbWqmnmqdTb3jjFB4sn38zlQUc0T142ygm9Oy6ZhMCYI7Co5yp8+LOCqUT0Y0zvJ7TjGj1nRNyYI3Ld0K5Hhwh1TBrsdxfg5K/rGBLj3txXx7taD3HbpALp2jnE7jvFzVvSNCWC19Y3cu2QLfbvE8e3z092OYwKAFX1jAtjfPilgx6Gj3D09g6gI+3U2zbOfEmMCVFFFNX94dzuXDu7KJYO6uh3HBAgr+sYEqIff2kZdg3L35RluRzEBxIq+MQFo3e7DvLqukO9emE6fLnFuxzEBxIq+MQGmsVGZvziHszpHM/eS/m7HMQHGir4xAebltXv4vLCcu6YOIS7aTqo3LWNF35gAUn6sjv9+axuZvROZMaK723FMALLdBGMCyB/e3U5pVS3PzMiyi6OYM2J7+sYEiO0Hj/DMpzu5NqsXw3rEux3HBCgr+sYEAFVl/pIc4qLC+emkQW7HMQEsKIv+4aO1/ODZteTsK3c7ijE+8XbOQf6dX8LtkwaRFBfldhwTwIKy6Deqsn7PYeYuWs+R6jq34xjTKtV1Ddz/5hYGd+vEdWN7uR3HBLigLPrJHaNZeO1odpdWcedrm/Dnq4MZ05z/+WAHhYePMW/6UCLCg/JX1rSjoP0JykpP4vZJA1n6+X7+sWq323GMOSOFh6t48v18pp2dyrn9kt2OY4JA0BZ9gB9c1I/xg1K4b8kWNu+1/r4JPA8uy0UE7po6xO0oJkgEddEPCxMevXokyR2juGXROiqsv28CyCdfHOLNTfu5eXx/eiTEuh3HBImgLvoASXFRLLx2FIWHj3HHq59bf98EhPqGRhYs3kJaYixzLurrdhwTRIK+6ANk9kniZ5cNYtmmAzz72S634xjTrH98tottB49w9+UZxESGux3HBJGQKPoAcy7sy4TBXbl/6VY2FVp/3/ivksoaHl2Rx4UDujAp4yy345gg02zRF5EYEVktIhtFJEdEFjjLPxKRDc5tn4i84SwXEXlMRPJF5HMRGd3ktW4Qke3O7Ya2G9aXhYUJv/3GCLp0jOLmRWspP2b9feOfHnknj6raBuZNz7D5dYzPebOnXwNMUNURwEhgsoiMU9ULVXWkqo4EPgVec7afAgxwbnOAPwKISBIwDxgLZAHzRCTRp6NpRmJcFAtnj2Z/WTW/eMX6+8b/bN5bzgtrdnPDeX3o37WT23FMEGq26KtHpfMw0rmdqJYi0hmYALzhLLoC+LvzvM+ABBFJBS4DVqhqqaoeBlYAk303FO+M6Z3ILyYP5q2cA/ztk53t/fbGnJaqMm9xDslxUfzwawPcjmOClFc9fREJF5ENQBGewr2qyeorgX+paoXzuAewp8n6QmfZ6Zaf/F5zRCRbRLKLi4u9H0kLfO/CdL42pCsPLNvKhj1lbfIexrTUPzfsY+2uw/z8ssF0jol0O44JUl4VfVVtcNo4aUCWiAxrsvpa4HlfBVLVp1U1U1UzU1JSfPWy/0FEeOQbI+jaKYa5i9ZRXmX9feOuypp6Hli2lRFp8cwck+Z2HBPEWnT0jqqWAe/htGVEpAue/vybTTbbC/Rs8jjNWXa65a5I6BDF47NHcbCimp++stH6+8ZVj6/Mp+hIDfNnDCUszL68NW3Hm6N3UkQkwbkfC0wEcp3VM4Glqlrd5CmLgeudo3jGAeWquh94G5gkIonOF7iTnGWuGdUrkTumDGHFloP8+eMCN6OYEFZw6Ch//ngHM8ekMapXux7bYEKQN5dLTAWeEZFwPH8kXlLVpc66WcBDJ22/DJgK5ANVwLcBVLVURO4D1jjb3auqpa3M32rfOb8Pq3aU8NDyXMb0TrRfOtPu7lu6heiIcH4+2S6OYtqe+HNbIzMzU7Ozs9v8fcqr6pi28CNU4c3bLiChg12kwrSPlbkH+c7fsvnl1CF836ZbMD4iImtVNfNU60LmjNyvEt8hkidmj6boSDU/fdn6+6Z91NQ3cN/SrfRNieOG8/q4HceECCv6jhE9E7hr6hDe3VrE/35k/X3T9v76750UHDrKvOlDiYqwX0XTPuwnrYkbz+vDlGHdePitXNbuOux2HBPEDlZUs/Bf25mYcRYXD2ybQ5ONORUr+k2ICA/PPJvuCbHcumgdh4/Wuh3JBKmHludS16jcPS3D7SgmxFjRP0nnGE9//1BlLbe/vJHGRuvvG9/K3lnK6+v3MufCvvRK7uB2HBNirOifwvC0eH51+RBW5hbx9Ec73I5jgkhDozJ/SQ7dOsdw8yX93I5jQpAV/dP41rjeTBueym/e3kb2TtdPJzBB4qXsPWzeW8Fd04bQIcqb02SM8S0r+qchIjz49eGkJcYyd9F6Sq2/b1qpvKqO37y9jaz0JKafnep2HBOirOh/heP9/dKqWn784gbr75tW+d27eZRV1TJ/+lC7OIpxjRX9ZgzrEc89l2fwQV4xT334hdtxTIDKPVDBs5/t4rqxvcno3tntOCaEWdH3wnVje3H52an89p08VhdYf9+0jKqyYPEWOsVE8JOJA92OY0KcFX0viAgPXjWcXkkduPX5dRyqrHE7kgkgyzcf4NMdJdw+aRCJcTavk3GXFX0vdYqJ5PHZozhcVWf9feO1Y7UN/PrNrQxJ7czsrF5uxzHGin5LDO0ez/zpQ/lo+yGefD/f7TgmADz1wRfsLTvG/OkZhNvFUYwfsKLfQtdm9eSKkd15dEUen35R4nYc48f2lFbx1AdfMH1Ed8b2TXY7jjGAFf0WExF+/V/D6ZMcx20vrKf4iPX3zan9+s2thIlw19TBbkcx5gQr+megY3QET1w3mopjnv5+g/X3zUk+3n6It3IOMHdCf1LjY92OY8wJVvTP0JDUztx7xVA+zj/E4yutv2/+T11DIwuW5NArqQPfvSDd7TjG/Acr+q1wdWZP/mtUD37/rzw+yT/kdhzjJ579dBfbiyq5+/IMYiLD3Y5jzH+wot8KIsL9Vw6jb5c4bnthA0VHqt2OZFx2qLKG372bx0UDU/jakK5uxzHmS6zot1JcdARPXjeGypo6fvi89fdD3SNvb+NYbQP3XJ5h8+sYv2RF3wcGdevEvVcM49MdJTz2r+1uxzEu+bywjBez9/CdC9Lp37Wj23GMOSUr+j5ydWZPvj46jcdWbufj7dbfDzWNjcq8xTkkx0Vz64T+bscx5rSaLfoiEiMiq0Vko4jkiMgCZ7mIyK9FJE9EtorIbc7y8SJSLiIbnNs9TV5rsohsE5F8Ebmj7YbljvuuHEr/lI786MX1FFVYfz+UvL5+L+t3l3HHlMF0iol0O44xp+XNnn4NMEFVRwAjgckiMg64EegJDFbVIcALTZ7zkaqOdG73AohIOPAEMAXIAK4VkaC6KnSHqAievG40R2sauPX59dQ3NLodybSDI9V1PPRWLiN7JnDVqB5uxzHmKzVb9NWj0nkY6dwUuAm4V1Ubne2KmnmpLCBfVXeoai2ePxJXnHFyPzXgrE7cf+UwVhWU8gfr74eEx1fmU3ykhgUzhhJm8+sYP+dVT19EwkVkA1AErFDVVUA/4BoRyRaR5SIyoMlTznXaQctFZKizrAewp8k2hc6yoPP1MWlcnZnG4+/l82FesdtxTBv6oriSv/y7gKsz0xjRM8HtOMY0y6uir6oNqjoSSAOyRGQYEA1Uq2om8CfgL87m64DeTjtoIfBGSwKJyBznD0l2cXHgFswFM4YxsGsnfvTiBg6UW38/GKkq9y7ZQkxEOD+fbPPrmMDQoqN3VLUMeA+YjGdP/TVn1evA2c42FcfbQaq6DIgUkS7AXjzfARyX5iw7+T2eVtVMVc1MSUlp4XD8R2xUOE9cN5rqugZus/5+UPrX1iI+yCvmRxMH0qVjtNtxjPGKN0fvpIhIgnM/FpgI5OLZg7/E2exiIM/Zpps4Z6WISJbzHiXAGmCAiKSLSBQwC1js2+H4l/5dO/LAfw1n9c5SHl2R53Yc40PVdQ3cu3QL/bt25Ppze7sdxxivRXixTSrwjHP0TRjwkqouFZGPgedE5MdAJfA9Z/uZwE0iUg8cA2apqgL1IjIXeBsIB/6iqjk+Ho/fuXJUDz7bUcKT739BVnoS4wfZqfnB4M8fF7C7tIpnv5tFZLid7mICh3jqsX/KzMzU7Oxst2O0WnVdA1c+8W8OVlSz7IcX2lS7AW5/+TEmPPIBFw3swv98K9PtOMZ8iYisdb5v/RLbRWkHMZGe/n5tfSO3LlpPnfX3A9pDy3NpUOVX04LqNBMTIqzot5N+KR154KrhZO86zG/fsf5+oFqzs5R/btjHDy7qS8+kDm7HMabFrOi3oytG9mD22F489cEXrMw96HYc00INjcq8f+bQPT6Gm8bb/DomMFnRb2f3XJ7BkNTO/OSljewtO+Z2HNMCz6/ezZb9FfxyWgaxUXZxFBOYrOi3s5jIcJ68bjT1Dcqti9ZZfz9AlFXV8sg72xjXN4mpw7u5HceYM2ZF3wXpXeJ48KrhrNtdxm/e3uZ2HOOFR1fkUXGsjvkzhtrFUUxAs6LvkukjuvPNcb14+sMdvLvF+vv+bOv+Cv7x2S6+Na43g7t1djuOMa1iRd9Fv5qWwdDunbn95Y0UHq5yO445BVVl/uIc4mMj+fHEgW7HMabVrOi76Hh/v7FRmbtoPbX11t/3N0s/38+qglJ+dtlgEjpEuR3HmFazou+y3slxPDzzbDbsKePht3LdjmOaqKqt54FlWxnavTPXnNOz+ScYEwCs6PuBqcNTueHc3vz54wLeyTngdhzj+OP7X7C/vJoFM4YSbhdHMUHCir6fuGvaEIb3iOenL29kT6n19922u6SK//lwB1eO7E5mnyS34xjjM1b0/UR0RDhPzB6NAnMXrbP+vsvuf3MLEWHCHVOGuB3FGJ+you9HeiV34DczR7CxsJwHl291O07I+jCvmHe2HGTuhP50i49xO44xPmVF389MHtaNb5/fh7/+eydvbd7vdpyQU1vfyIIlOfRJ7sB3L0h3O44xPmdF3w/dOWUII9Li+dkrn7O7xPr77envn+7ki+Kj3DM9g+gIm1/HBB8r+n4oKiKMx2ePRoBbFq2jpr7B7UghoehINb9/dzuXDEphwuCz3I5jTJuwou+neiZ14JFvjGDT3nIeeNP6++3hN29to6a+gbsvt4ujmOBlRd+PTRraje9ekM4zn+5i2Sbr77elDXvKeHltId+5IJ2+KR3djmNMm7Gi7+d+MXkwI3sm8ItXPmdXyVG34wSlxkZl3uIcUjpFc+uEAW7HMaZNWdH3c57+/ijCwoSbn1tHdZ31933t1XWFbNxTxp1TBtMxOsLtOMa0KSv6ASAtsQO//cYIcvZV8Gvr7/tURXUdD7+Vy+heCfzXqB5uxzGmzVnRDxBfyziLORf15dnPdrFk4z634wSNx97dTsnRWhbMGGYXRzEhwYp+APnZZYMY3SuBO1/bRMEh6++3Vn7REf72yU5mndOT4Wnxbscxpl00W/RFJEZEVovIRhHJEZEFznIRkV+LSJ6IbBWR25osf0xE8kXkcxEZ3eS1bhCR7c7thrYbVnCKDPccvx8Rbv391lJVFizZQmxUOD+dNMjtOMa0G2/29GuACao6AhgJTBaRccCNQE9gsKoOAV5wtp8CDHBuc4A/AohIEjAPGAtkAfNEJNF3QwkN3RNi+d3VI9m6v4J7l25xO07AWrHlIB9tP8RPJg4kuWO023GMaTfNFn31qHQeRjo3BW4C7lXVRme7ImebK4C/O8/7DEgQkVTgMmCFqpaq6mFgBTDZt8MJDZcM7soPLu7HolW7+eeGvW7HCTjVdQ3c9+YWBp7VkW+O6+12HGPalVc9fREJF5ENQBGewr0K6AdcIyLZIrJcRI4f4NwD2NPk6YXOstMtP/m95jivmV1cXNzyEYWI2ycNJLN3Ine9tokviiubf4I54U8f7mBP6THmTx9KZLh9rWVCi1c/8araoKojgTQgS0SGAdFAtapmAn8C/uKLQKr6tKpmqmpmSkqKL14yKEWGh7Fw9iiiI8O5xfr7XttXdown3s9n6vBunNe/i9txjGl3LdrNUdUy4D08bZlC4DVn1evA2c79vXh6/celOctOt9ycodT4WB69egS5B44wf3GO23ECwgPLtqIKd021i6OY0OTN0TspIpLg3I8FJgK5wBvAJc5mFwN5zv3FwPXOUTzjgHJV3Q+8DUwSkUTnC9xJzjLTCuMHdeXm8f14Yc0eXl9f6HYcv/bZjhKWfr6fm8b3Iy2xg9txjHGFN+ecpwLPiEg4nj8SL6nqUhH5GHhORH4MVALfc7ZfBkwF8oEq4NsAqloqIvcBa5zt7lXVUt8NJXT9ZOJAsnce5pevb2Z4jwT6d7UJw05W39DI/MU59EiI5QcX93M7jjGuEVV1O8NpZWZmanZ2ttsxAsKB8mqmPfYRXTpG88Yt5xMbZRcAaerZT3dy9z9z+ON1o5kyPNXtOMa0KRFZ63zf+iV26EKQ6BYfw++uGUle0RHmLd7sdhy/Unq0lkfeyeP8/slMHtbN7TjGuMqKfhC5aGAKcy/pz0vZhby61vr7x/32nW1U1tQzb/pQm1/HhDwr+kHmh5cOYGx6Er96YzPbDx5xO47rNu8tZ9Hq3Vx/bm8GntXJ7TjGuM6KfpCJCA9j4bWjiIsO5+bn1lFVW+92JNd45tfJIbFDFD/62kC34xjjF6zoB6GunWP4/TWjyC+u5O43Qvf4/cUb97Fm52F+ftkg4mMj3Y5jjF+woh+kLhjQhVsnDODVdYW8lL2n+ScEmaM19Ty4LJfhPeL5RmbP5p9gTIiwoh/EfnjpAM7rl8w9/9zMtgOh1d9/8v18DlRUM3/GUMLD7MtbY46zoh/EwsOE388aScfoSG5+bi1Ha0Kjv7/z0FH+9GEBV43uwZjeNnu3MU1Z0Q9yXTvF8NiskRQcOsqv3tiMP5+M5yv3v7mFyHDhjsmD3Y5ijN+xoh8CzuvfhR9eOpDX1+8N+v7+e9uKeHdrEbddOoCunWPcjmOM37GiHyLmTujPBf27cM8/c9i6v8LtOG2itr6R+5ZsoW+XOL59frrbcYzxS1b0Q0R4mPC7a0bSOTaSW55bR2UQ9vf/9kkBOw4d5e7pGURF2I+2MadivxkhJKVTNAuvHcXOkqPc9dqmoOrvF1VU84d3t3Pp4K5cMqir23GM8VtW9EPMuL7J/GTiQBZv3Mfzq4Onv//QW7nUNSh3X57hdhRj/JoV/RB08/j+XDigC/OX5JCzr9ztOK22dtdhXlu3l+9dmE6fLnFuxzHGr1nRD0FhYcLvrxlJYodI5i5az5HqOrcjnbHGRmX+4hzO6hzNLZf0dzuOMX7Pin6ISu4YzcJrR7Or5Ch3BnB//+W1e9i0t5y7pg4hLtqbC8EZE9qs6IewrPQkbp80iKWf7+cfq3a7HafFyo/V8d9vbSOzdyIzRnR3O44xAcGKfoi76eJ+jB+Uwn1LtrB5b2D19//w7nZKq2qZP8MujmKMtzToOEQAAAwTSURBVKzoh7iwMOHRq0eSFBfFLYvWUREg/f28g0d45tOdzM7qxbAe8W7HMSZgWNE3JMVF8fjsURQePsYdr37u9/394xdH6Rgdwe2TBrkdx5iAYkXfAJDZJ4mfXTaIZZsO8Oxnu9yO85XezjnAv/NLuH3SQJLiotyOY0xAsaJvTphzYV8uGZTC/Uu3sqnQP/v71XUN3Ld0K4O7dWJ2Vi+34xgTcKzomxOO9/e7dIzi5kVrKT/mf/39//lgB3vLjjFv+lAiwu3H15iWava3RkRiRGS1iGwUkRwRWeAs/5uIFIjIBuc20lk+XkTKmyy/p8lrTRaRbSKSLyJ3tN2wzJlKjIti4ezR7C+r5hev+Fd/v/BwFU++n8+0s1M5t1+y23GMCUje7CrVABNUdQQwEpgsIuOcdT9T1ZHObUOT53zUZPm9ACISDjwBTAEygGtFxCZK8UNjeify88mDeCvnAH/7ZKfbcU54cFkuInDX1CFuRzEmYDVb9NWj0nkY6dzOZPcvC8hX1R2qWgu8AFxxBq9j2sH3L+zL14Z05YFlW9mwp8ztOHySf4g3N+3nlvH96ZEQ63YcYwKWV01REQkXkQ1AEbBCVVc5q34tIp+LyO9EJLrJU8512kHLRWSos6wH0HRax0Jn2cnvNUdEskUku7i4uOUjMj4hIjzyjRF07RTD3EXrKK9yr79f39DI/CU59EyK5fsX9XUthzHBwKuir6oNqjoSSAOyRGQYcCcwGDgHSAJ+4Wy+DujttIMWAm+0JJCqPq2qmaqamZKS0pKnGh9L6BDFwtmjOFBezU9f2ehaf/8fn+0i72Alv5qWQUxkuCsZjAkWLTr8QVXLgPeAyaq632n91AB/xdO+QVUrjreDVHUZECkiXYC9QM8mL5fmLDN+bHSvRO6YMpgVWw7y548L2v39SypreHRFHhcO6MKkjLPa/f2NCTbeHL2TIiIJzv1YYCKQKyKpzjIBrgQ2O4+7OcsQkSznPUqANcAAEUkXkShgFrDY90MyvvbdC9KZmHEWDy3PZf3uw+363o+8k0dVbQPzpmfY/DrG+IA3e/qpwHsi8jmewr1CVZcCz4nIJmAT0AW439l+JrBZRDYCjwGznP8R1ANzgbeBrcBLqprj2+GYtiAiPDJzBN3iY5i7aD1lVbXt8r6b95bzwprd3HBeH/p37dQu72lMsBN/Og77ZJmZmZqdne12DOPYuKeMmU99wsUDU/jT9Zltuuetqsx86lN2lRxl5U/H0zkmss3ey5hgIyJrVTXzVOvslEbjtRE9E7hr6hDe3VrE/37Utv39NzbsZe2uw/x88mAr+Mb4kBV90yI3nteHyUO78fBbuazd1Tb9/cqaeh5clsuItHhmjk5rk/cwJlRZ0TctIiI8PPNsUhNiuHXROg4f9X1///GV+RQdqWH+jKGEhdmXt8b4khV902LxsZE8OXsMhypruf3ljTQ2+u57oYJDR/nzxzuYOSaNUb0Sffa6xhgPK/rmjAxPi+eX04awMreIpz/a4bPXvW/pFqIjwvn5ZLs4ijFtwYq+OWPXn9ubacNT+c3b28jeWdrq11uZe5CVuUX86GsD6NopxgcJjTEns6JvzpiI8ODXh5OWGMvcRespbUV/v6a+gXuXbKFfShzXn9vHdyGNMf/Bir5plc4xkTwxezSlR2v58Ysbzri//5ePd7KzpIp504cSFWE/lsa0FfvtMq02rEc8d0/P4IO8Yp768IsWP/9gRTULV25nYsZZXDTQJtkzpi1Z0Tc+8c2xvbj87FR++04eqwta1t9/aHku9Y3K3dPsmjrGtDUr+sYnRIQHrxpOr6QO3Pr8Og5V1nj1vOydpby+fi9zLuxLr+QObZzSGGNF3/hMp5hIHp89isNVdV719xsalXmLc0iNj+HmS/q1U0pjQpsVfeNTQ7vHM3/6UD7afogn38//ym1fXLOHnH0V3DV1CB2iItopoTGhzYq+8blrs3oyY0R3Hl2Rx6dflJxym/KqOn7zdi5Z6UlcfnZqOyc0JnRZ0Tc+JyI8cNVw+iTHcdsL6yk+8uX+/u/ezaP8WB3zpw+1i6MY046s6Js20TE6gieuG03FMU9/v6FJfz/3QAXPfraL68b2JqN7ZxdTGhN6rOibNjMktTMLZgzl4/xDPL7S099XVRYs3kKnmAh+MnGgywmNCT327ZlpU9ec05NVBaX8/l95nNMnkbJjdXy6o4T7rhxGYlyU2/GMCTlW9E2bEhHuv3IYnxeWcdsLG4gKF4akdmZ2Vi+3oxkTkqy9Y9pcXHQET143hsqaOvaVV7NgxlDC7eIoxrjC9vRNuxjUrRNPfXMMu0qqyEpPcjuOMSHLir5pN+MHdXU7gjEhz9o7xhgTQpot+iISIyKrRWSjiOSIyAJn+d9EpEBENji3kc5yEZHHRCRfRD4XkdFNXusGEdnu3G5ou2EZY4w5FW/aOzXABFWtFJFI4GMRWe6s+5mqvnLS9lOAAc5tLPBHYKyIJAHzgExAgbUislhVD/tiIMYYY5rX7J6+elQ6DyOd21dNn3gF8HfneZ8BCSKSClwGrFDVUqfQrwAmty6+McaYlvCqpy8i4SKyASjCU7hXOat+7bRwfici0c6yHsCeJk8vdJadbvnJ7zVHRLJFJLu4uLiFwzHGGPNVvCr6qtqgqiOBNCBLRIYBdwKDgXOAJOAXvgikqk+raqaqZqak2KXzjDHGl1p09I6qlgHvAZNVdb/TwqkB/gpkOZvtBXo2eVqas+x0y40xxrQTb47eSRGRBOd+LDARyHX69IhnXtwrgc3OUxYD1ztH8YwDylV1P/A2MElEEkUkEZjkLDPGGNNOvDl6JxV4RkTC8fyReElVl4rIShFJAQTYAPzA2X4ZMBXIB6qAbwOoaqmI3Aescba7V1W/8graa9euPSQiu1o6qCa6AIda8Xx/ESzjABuLvwqWsQTLOKB1Y+l9uhWi+tXXMQ1kIpKtqplu52itYBkH2Fj8VbCMJVjGAW03Fjsj1xhjQogVfWOMCSHBXvSfdjuAjwTLOMDG4q+CZSzBMg5oo7EEdU/fGGPMfwr2PX1jjDFNWNE3xpgQEvBFX0Qmi8g2ZyrnO06xPlpEXnTWrxKRPu2f0jtejOVGESluMp3199zI2RwR+YuIFInI5tOsP+302/7Gi7GMF5HyJp/JPe2d0Rsi0lNE3hORLc4U6T88xTYB8bl4OZZA+VxOOXX9Sdv4toapasDegHDgC6AvEAVsBDJO2uZm4Cnn/izgRbdzt2IsNwKPu53Vi7FcBIwGNp9m/VRgOZ4T+8YBq9zO3IqxjAeWup3Ti3GkAqOd+52AvFP8fAXE5+LlWALlcxGgo3M/ElgFjDtpG5/WsEDf088C8lV1h6rWAi/gmdq5qSuAZ5z7rwCXOlNH+BtvxhIQVPVD4KvOtj7d9Nt+x4uxBAT1zJW1zrl/BNjKl2e5DYjPxcuxBATn37q5qet9WsMCveh7M13ziW1UtR4oB5LbJV3LeDX1NPB157/er4hIz1OsDwTejjVQnOv893y5iAx1O0xznPbAKDx7lU0F3OfyFWOBAPlcvmLq+uN8WsMCveiHmiVAH1U9G89FaJ5pZnvT9tYBvVV1BLAQeMPlPF9JRDoCrwI/UtUKt/O0RjNjCZjPRU89dX2bCfSi7810zSe2EZEIIB4oaZd0LdPsWFS1RD1TWQP8LzCmnbL5WtBMs62qFcf/e66qy4BIEenicqxTEs/lTl8FnlPV106xScB8Ls2NJZA+l+O0ydT1J63yaQ0L9KK/BhggIukiEoXnS47FJ22zGDh+EfaZwEp1vhHxM82O5aT+6gw8vcxAdLrptwOOiHQ73l8VkSw8v1N+t1PhZPwzsFVVHz3NZgHxuXgzlgD6XE45df1Jm/m0hnkztbLfUtV6EZmLZ17+cOAvqpojIvcC2aq6GM8Px7Miko/nC7lZ7iU+PS/HcpuIzADq8YzlRtcCfwUReR7P0RNdRKQQmIfnCypU9SlOM/22P/JiLDOBm0SkHjgGzPLTnYrzgW8Bm5z+McBdQC8IuM/Fm7EEyudyuqnr26yG2TQMxhgTQgK9vWOMMaYFrOgbY0wIsaJvjDEhxIq+McaEECv6xhgTQqzoG2NMCLGib4wxIeT/AxLTTPB4/3srAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 335
        },
        "id": "qM8J2BxNrJ1G",
        "outputId": "30c24ad1-23cb-42d9-dab9-3db56ac4bbb2"
      },
      "source": [
        "# Inference\n",
        "evaluate(opt, valid_loader, model, 'VALID', True)"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "VALID loss: 3532.6794   Marginal loss: 0.4050   Reconstruction loss: 7064548.4255\n",
            "Accuracy: 3250/6000 0.5417\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}