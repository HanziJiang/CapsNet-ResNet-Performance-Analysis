{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CapsNet big.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PiQpTflFkuqi"
      },
      "source": [
        "# Capsule Neural Network (CapsNet) Implementation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QDxJ_A_0Gnt9"
      },
      "source": [
        "Implementation of the paper [Dynamic Routing Between Capsules](https://arxiv.org/pdf/1710.09829.pdf) by Sara Sabour, Nicholas Frosst, and Geoffrey E. Hinton. Used [jindongwang/Pytorch-CapsuleNet](https://github.com/jindongwang/Pytorch-CapsuleNet) and [laubonghaudoi/CapsNet_guide_PyTorch](https://github.com/laubonghaudoi/CapsNet_guide_PyTorch) to clarify some confusions, and borrowed some code."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_pkx4McDDItG"
      },
      "source": [
        "## Setup PyTorch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8aohcW3YECPM",
        "outputId": "96a6b8bf-3009-4d85-967e-03373d635393"
      },
      "source": [
        "!git clone https://github.com/HanziJiang/Hahaha-Project.git\n",
        "%cd \"CSC413-Porject\"\n",
        "!git pull"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'Hahaha-Project'...\n",
            "remote: Enumerating objects: 45, done.\u001b[K\n",
            "remote: Counting objects: 100% (45/45), done.\u001b[K\n",
            "remote: Compressing objects: 100% (39/39), done.\u001b[K\n",
            "remote: Total 45 (delta 20), reused 8 (delta 3), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (45/45), done.\n",
            "[Errno 2] No such file or directory: 'CSC413-Porject'\n",
            "/content\n",
            "fatal: not a git repository (or any of the parent directories): .git\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "peSdN_P8CElk",
        "outputId": "112b9118-bcca-45b3-f8e5-27e999f72a05"
      },
      "source": [
        "!pip install torch torchvision\n",
        "!pip install matplotlib\n",
        "!pip install import-ipynb\n",
        "!pip install tqdm\n",
        "!pip install pytorch_extras"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (1.8.1+cu101)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (0.9.1+cu101)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch) (3.7.4.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch) (1.19.5)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.7/dist-packages (from torchvision) (7.1.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (3.2.2)\n",
            "Requirement already satisfied: numpy>=1.11 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (1.19.5)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (0.10.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (1.3.1)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (2.8.1)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (2.4.7)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from cycler>=0.10->matplotlib) (1.15.0)\n",
            "Collecting import-ipynb\n",
            "  Downloading https://files.pythonhosted.org/packages/63/35/495e0021bfdcc924c7cdec4e9fbb87c88dd03b9b9b22419444dc370c8a45/import-ipynb-0.1.3.tar.gz\n",
            "Building wheels for collected packages: import-ipynb\n",
            "  Building wheel for import-ipynb (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for import-ipynb: filename=import_ipynb-0.1.3-cp37-none-any.whl size=2976 sha256=a20cdfae982ca45d0ed15a9ec9c56519fb16689d5aae6af2e50496b0ca6340f9\n",
            "  Stored in directory: /root/.cache/pip/wheels/b4/7b/e9/a3a6e496115dffdb4e3085d0ae39ffe8a814eacc44bbf494b5\n",
            "Successfully built import-ipynb\n",
            "Installing collected packages: import-ipynb\n",
            "Successfully installed import-ipynb-0.1.3\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (4.41.1)\n",
            "Collecting pytorch_extras\n",
            "  Downloading https://files.pythonhosted.org/packages/66/79/42d7d9a78c27eb897b14790c9759dd9a991f67bc987e9e137527a68db9dc/pytorch-extras-0.1.3.tar.gz\n",
            "Building wheels for collected packages: pytorch-extras\n",
            "  Building wheel for pytorch-extras (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pytorch-extras: filename=pytorch_extras-0.1.3-cp37-none-any.whl size=2833 sha256=6a2247f92b4db0fa75361c263361f41f57c016f8dd79a4b1f2693b6bb4fd44bf\n",
            "  Stored in directory: /root/.cache/pip/wheels/5b/7c/5a/f27d4088adfe722cb280d523a1ed9eeb33be11b8d3a653292a\n",
            "Successfully built pytorch-extras\n",
            "Installing collected packages: pytorch-extras\n",
            "Successfully installed pytorch-extras-0.1.3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YMdBumXa75tw"
      },
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount(\"mnt\")\n",
        "# %cd \"mnt/My Drive\""
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rsZEHMrIEVz6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 369
        },
        "outputId": "5229490f-2e8e-4de0-99e1-1522d62f5fbf"
      },
      "source": [
        "import torch\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "import torch_extras\n",
        "import torch.nn as nn\n",
        "import torchvision.utils as tv_utils\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "import import_ipynb\n",
        "import load_DrawData_with_transform as loader\n",
        "import numpy as np\n",
        "import argparse\n",
        "from tqdm import tqdm"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-fefa33c28edd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mimport_ipynb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mload_DrawData_with_transform\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mloader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0margparse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'load_DrawData_with_transform'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XcYm7y8rECqg"
      },
      "source": [
        "## CapsNet Modules"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fC4zxyWWeF-k"
      },
      "source": [
        "def squash(s):\n",
        "    '''Non-linear \"squashing\" function to ensure that short vectors get shrunk \n",
        "    to almost zero length and long vectors get shrunk to a length slightly \n",
        "    below 1. Equation (1) in the paper.\n",
        "    \n",
        "    Input:\n",
        "      s: \ttotal input vector\n",
        "    \n",
        "    Output:\n",
        "      squashed output vector\n",
        "    '''\n",
        "    norm_sqrd = torch.sum(s**2, dim=2, keepdim=True)\n",
        "    return (norm_sqrd / (1 + norm_sqrd)) * (s / (torch.sqrt(norm_sqrd) + 1e-8))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VdGH3V4jJPmQ"
      },
      "source": [
        "class PrimaryCapsules(nn.Module):\n",
        "    '''The layer after Conv1. Section 4 of the paper.\n",
        "    '''\n",
        "\n",
        "    def __init__(self):\n",
        "        super(PrimaryCapsules, self).__init__()\n",
        "        self.capsules = nn.ModuleList(\n",
        "            [nn.Conv2d(in_channels=256,\n",
        "                    out_channels=8,\n",
        "                    kernel_size=9,\n",
        "                    stride=2)\n",
        "            for i in range(32)]\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        '''Section 4 of the paper.\n",
        "\n",
        "        Input:\n",
        "        x: outut of the ReLUConv1 layer, of shape (batch_size x 256 x 32 x 32)\n",
        "\n",
        "        Output:\n",
        "        squashed PrimaryCapsules output tensor, of shape (batch_size x 4608 x 8)\n",
        "        '''\n",
        "        batch_size = x.size(0)\n",
        "        \n",
        "        # Get the output of each primary capsule; combine and prepare them to \n",
        "        # serve as the input to the next layer (DoodleCapsules)\n",
        "        all_u = [] \n",
        "        assert x.shape==torch.Size([batch_size, 256, 32, 32]), x.shape\n",
        "        for cap in self.capsules:\n",
        "            u = cap(x)  # (batch_size x 8 x 12 x 12)\n",
        "            assert u.shape == torch.Size([batch_size, 8, 12, 12]), u.shape\n",
        "            u = u.view(batch_size, 8, 144, 1)  # (batch_size x 8 x 144 x 1)\n",
        "            all_u.append(u)\n",
        "        all_u = torch.cat(all_u, dim=3)  # (batch_size x 8 x 144 x 32)\n",
        "        assert all_u.shape == torch.Size([batch_size, 8, 144, 32]), all_u.shape\n",
        "        all_u = all_u.view(batch_size, 8, -1)  # (batch_size x 8 x 4)\n",
        "        all_u = torch.transpose(all_u, 1, 2)  # (batch_size x 11460852 x 8)\n",
        "        all_u = squash(all_u)  # (batch_size x 4608 x 8)\n",
        "        assert all_u.shape == torch.Size([batch_size, 4608, 8]), all_u.shape\n",
        "        \n",
        "        return all_u"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xz1tFdzkaHyP"
      },
      "source": [
        "class DoodleCapsules(nn.Module):\n",
        "    '''The layer after PrimaryCapsules. Section 4 of the paper.\n",
        "    '''\n",
        "\n",
        "    def __init__(self, opt):\n",
        "        super(DoodleCapsules, self).__init__()\n",
        "        self.opt = opt\n",
        "        self.W = nn.Parameter(torch.randn(1, 4608, opt.n_classes, 8, 24))\n",
        "    \n",
        "    def forward(self, u):\n",
        "        '''Equation (2) and Procedure 1 in the paper.\n",
        "\n",
        "        Input:\n",
        "        u: output of the PrimaryCapsules layer, of shape (batch_size x 8)\n",
        "\n",
        "        Output:\n",
        "        output tensor of the DoodleCapsules layer, of shape (batch_size x 4608 x n_classes x 1)\n",
        "        '''\n",
        "        batch_size = u.size(0)\n",
        "        \n",
        "        u = torch.unsqueeze(u, dim=2)  # (batch_size x 1 x 8)\n",
        "        u = torch.unsqueeze(u, dim=2)  # (batch_size x 1 x 1 x 8)\n",
        "        u_hat = torch.matmul(u, self.W).squeeze()  # (batch_size x 4608 x n_classes x 24)\n",
        "\n",
        "        b = Variable(torch.zeros(batch_size, 4608, self.opt.n_classes, 1))  # (batch_size x 4608 x n_classes x 1)\n",
        "        b = b.cuda()\n",
        "\n",
        "        for i in range(self.opt.iterations):\n",
        "            c = F.softmax(b, dim=2)  # (batch_size x 4608 x n_classes x 1)\n",
        "            s = torch.sum(u_hat * c, dim=1)  # (batch_size x n_classes x 24)\n",
        "            v = squash(s)  # (batch_size x n_classes x 24)\n",
        "            b = b + torch.sum(u_hat * v.unsqueeze(1), dim=3, keepdim=True)  # (batch_size x 4608 x n_classes x 1)\n",
        "        \n",
        "        return v  # (batch_size x n_classes x 24)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q-k05aXUmDs8"
      },
      "source": [
        "class DoodleDecoder(nn.Module):\n",
        "    '''Decoder structure to reconstruct the doodle from the output of the DoodleCapsules layer.\n",
        "    Section 4 of the paper. For an illustrative explaination, see Figure 2 of the paper.\n",
        "    '''\n",
        "\n",
        "    def __init__(self, opt):\n",
        "        super(DoodleDecoder, self).__init__()\n",
        "        self.opt = opt\n",
        "        self.layers = nn.Sequential(\n",
        "            nn.Linear(opt.n_classes * 24, 512),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(512, 1024),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(1024, opt.image_size * opt.image_size),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, v, target):\n",
        "        '''Takes a 24-dimensional vector v from the *correct* DoodleCapsules, and \n",
        "        learns to decode it into an image of a doodle. Mask out the other (n_classes - 1) classes.\n",
        "        Section 4 of the paper. For an illustrative explaination, see Figure 2 of the paper.\n",
        "\n",
        "        Input:\n",
        "        v: output of DoodleCapsules, of shape (batch_size x n_classes x 24)\n",
        "        target: one-hot targets, of shape (batch_size, n_classes)\n",
        "\n",
        "        Output:\n",
        "        decoder constructed images, of shape (batch_size x image_size^2)\n",
        "        '''\n",
        "        # TODO: the true target or the most probable?\n",
        "        # correct = torch.sqrt((v ** 2).sum(2))  # (batch_size x n_classes)\n",
        "        # correct = F.softmax(correct, dim=0)  # (batch_size x n_classes)\n",
        "        # correct = correct.max(dim=1)[1]  # (batch_size)\n",
        "        batch_size = v.size(0)\n",
        "\n",
        "        # Create the mask, which is 1 only for the correct class and 0 otherwise\n",
        "        mask = target.type(torch.FloatTensor).unsqueeze(-1)  # (batch_size x n_classes x 1)\n",
        "        mask = torch.repeat_interleave(mask, 24, dim=2)  # (batch_size x n_classes x 24)\n",
        "        mask = mask.cuda()\n",
        "        assert mask.size() == torch.Size([batch_size, 2, 24]), mask.size()\n",
        "        \n",
        "        masked = (v * mask).view(batch_size, -1)  # (batch_size x n_classes x 24)\n",
        "        result = self.layers(masked)  # (batch_size x f)\n",
        "        assert result.shape == torch.Size([batch_size, opt.image_size*opt.image_size]), result.shape\n",
        "        \n",
        "        return result"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CH6pouLtXQnx"
      },
      "source": [
        "class CapsuleNetwork(nn.Module):\n",
        "    '''Consists of a ReLU Convolution layer, a PrimaryCapsules layer, a DoodleCapsules\n",
        "    layer, and a Decoder layer. Section 4 of the paper.\n",
        "    '''\n",
        "\n",
        "    def __init__(self, opt):\n",
        "        super(CapsuleNetwork, self).__init__()\n",
        "        self.opt = opt\n",
        "        self.ReLUConv1 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=1, out_channels=256, kernel_size=9),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        self.PrimaryCapsules = PrimaryCapsules()\n",
        "        self.DoodleCapsules = DoodleCapsules(opt)\n",
        "        self.DoodleDecoder = DoodleDecoder(opt)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        '''Section 4 of the paper.\n",
        "\n",
        "        Input:\n",
        "        the input to the network, of shape (batch_size x image_size x image_size)\n",
        "\n",
        "        Output:\n",
        "        the output of the network, of shape (batch_size x n_classes x 24)\n",
        "        '''\n",
        "        v = torch.unsqueeze(x, 1)\n",
        "        v = self.ReLUConv1(v)  # (batch_size x 256 x 32 x 32)\n",
        "        v = self.PrimaryCapsules(v)  # (batch_size x 4608 x 8)\n",
        "        v = self.DoodleCapsules(v)  # (batch_size x n_classes x 24)\n",
        "        return v  # (batch_size x n_classes x 24)\n",
        "\n",
        "    def marginal_loss(self, v, target):\n",
        "        '''Section 3, Equation (4) of the paper.\n",
        "\n",
        "        Input:\n",
        "        v: the output of the network, of shape (batch_size x n_classes x 24)\n",
        "        target: the one-hot target, of shape (batch_size x n_classes)\n",
        "        lambd: a scalor for down-weighting of the loss for absent doodle classes\n",
        "        \n",
        "        Output:\n",
        "        marginal loss (a scalor summed over all batches and classes)\n",
        "        '''\n",
        "        v_norm = torch.sqrt((v ** 2).sum(dim=2))  # (batch_size x n_classes)\n",
        "        \n",
        "        zeros = torch.zeros(v_norm.size())  # (batch_size x n_classes)\n",
        "        zeros = zeros.cuda()\n",
        "        \n",
        "        max1 = torch.max(zeros, 0.9 - v_norm) ** 2  # (batch_size x n_classes)\n",
        "        max2 = torch.max(zeros, v_norm - 0.1) ** 2  # (batch_size x n_classes) \n",
        "        loss = target * max1 + (1 - target) * 0.5 * max2  # (batch_size x n_classes)\n",
        "        \n",
        "        return torch.sum(loss)  # scalor\n",
        "\n",
        "    def reconstruction_loss(self, data, reconstruction):\n",
        "        '''Reconstruction for regularization. Ecourages the doodle capsules to \n",
        "        encode the instantiation parameters of the input doodle. Section 4.1 of the paper.\n",
        "\n",
        "        Input:\n",
        "        data: the real image, of shape (batch_size, image_size, image_size)\n",
        "        reconstruction: the reconstructed image, of shape (batch_size, image_size * image_size)\n",
        "\n",
        "        Output:\n",
        "        reconstruction loss (a scalor summed over all batches and classes)\n",
        "        '''\n",
        "        batch_size = data.size(0)\n",
        "        return torch.sum((reconstruction - data.view(batch_size, -1)) ** 2)\n",
        "\n",
        "    def loss(self, v, data, target):\n",
        "        '''Loss is marginal loss + 0.0005 * reconstruction loss. 0.0005 to ensure\n",
        "        the reconstruction loss does not dominate the training. Section 4.1 of the paper.\n",
        "\n",
        "        Input:\n",
        "        v: output of the network, of shape (batch_size x n_classes x 24)\n",
        "        target: one-hot target, of shape (batch_size x n_classes)\n",
        "        data: the input to the network (the image), of shape (batch_size, image_size, image_size)\n",
        "        \n",
        "        Output:\n",
        "        averaged loss (a scalor) over batches \n",
        "        averaged marginal loss (a scalor) over batches\n",
        "        averaged reconstruction loss (a scalor) over batches\n",
        "        '''\n",
        "        batch_size = data.size(0)\n",
        "        \n",
        "        marginal_loss = self.marginal_loss(v, target)  # scalor\n",
        "        \n",
        "        reconstruction = self.DoodleDecoder(v, target)  # (batch_size, image_size^2)\n",
        "        assert reconstruction.shape == torch.Size([batch_size, 40 * 40]), (reconstruction.shape, opt.image_size)\n",
        "        reconstruction_loss = self.reconstruction_loss(data, reconstruction)  # scalor\n",
        "        \n",
        "        loss = marginal_loss + 0.0005 * reconstruction_loss  # scalor\n",
        "        \n",
        "        return loss/batch_size, marginal_loss/batch_size, reconstruction_loss/batch_size"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zKPXAofhnYhK"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Holn-KGHnvZH"
      },
      "source": [
        "def get_opts():\n",
        "    parser = argparse.ArgumentParser(description='CapsuleNetwork')\n",
        "    parser.add_argument('-batch_size', type=int, default=32)\n",
        "    parser.add_argument('-lr', type=float, default=1e-6)\n",
        "    parser.add_argument('-epochs', type=int, default=200)\n",
        "    parser.add_argument('-image_size', type=int, default=40)\n",
        "    parser.add_argument('-n_classes', type=int, default=2)\n",
        "    parser.add_argument('-iterations', type=int, default=3)\n",
        "    parser.add_argument('-print_every', type=int, default=10)\n",
        "    parser.add_argument('-gamma', type=float, default=0.8)\n",
        "    opt, _ = parser.parse_known_args()\n",
        "    return opt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QQCM0T0_1d5W"
      },
      "source": [
        "def evaluate(opt, valid_loader, model, epoch, num_batches, dataset_type):\n",
        "    sum_loss = 0\n",
        "    sum_marginal_loss = 0\n",
        "    sum_reconstruction_loss = 0\n",
        "    correct = 0\n",
        "    num_sample = len(valid_loader.dataset)\n",
        "    num_batch = len(valid_loader)\n",
        "\n",
        "    model.eval()\n",
        "    for data, target in valid_loader:\n",
        "        data = data.to(torch.float32)\n",
        "        target = target.to(torch.int64)\n",
        "        batch_size = data.size(0)\n",
        "        assert target.size() == torch.Size([batch_size, opt.n_classes])\n",
        "\n",
        "        # Use GPU if available\n",
        "        with torch.no_grad():\n",
        "            data, target = Variable(data), Variable(target)\n",
        "        data, target = data.cuda(), target.cuda()\n",
        "\n",
        "        output = model(data)  # (batch_size, n_classes, 24)\n",
        "        loss, marginal_loss, reconstruction_loss = model.loss(output, data, target)\n",
        "        sum_loss += loss.item()\n",
        "        sum_marginal_loss += marginal_loss.item()\n",
        "        sum_reconstruction_loss += reconstruction_loss.item()\n",
        "\n",
        "        norms = torch.sqrt(torch.sum(output**2, dim=2))  # (batch_size, n_classes)\n",
        "        pred = norms.data.max(1, keepdim=True)[1].type(torch.LongTensor)  # (batch_size, 1)\n",
        "        label = target.max(1, keepdim=True)[1].type(torch.LongTensor)  # (batch_size, 1)\n",
        "        correct += pred.eq(label.view_as(pred)).sum().item()\n",
        "\n",
        "    recons = model.DoodleDecoder(output, target)\n",
        "    recons = recons.view(batch_size, 1, opt.image_size, opt.image_size)\n",
        "    recons = tv_utils.make_grid(recons.data, normalize=True, scale_each=True)\n",
        "\n",
        "    sum_loss /= num_batch\n",
        "    sum_marginal_loss /= num_batch\n",
        "    sum_reconstruction_loss /= num_batch\n",
        "    \n",
        "    print('{}'.format(dataset_type))\n",
        "    print('\\tLoss: {:.4f}   Marginal loss: {:.4f}   Reconstruction loss: {:.4f}'.format(\n",
        "        sum_loss, sum_marginal_loss, sum_reconstruction_loss))\n",
        "    print('\\tAccuracy: {}/{} {:.4f}'.format(correct, num_sample,\n",
        "        correct / num_sample))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jcoOvQfb1bVn"
      },
      "source": [
        "def train(opt, train_loader, valid_loader, model):\n",
        "    num_sample = len(train_loader.dataset)\n",
        "    num_batches = len(train_loader)\n",
        "    train_loss_list = []\n",
        "    correct = 0\n",
        "\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=opt.lr)\n",
        "    scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, opt.gamma)\n",
        "    model.train()\n",
        "    for epoch in range(opt.epochs):\n",
        "        for batch_idx, (data, target) in enumerate(train_loader):\n",
        "            optimizer.zero_grad()\n",
        "            data = data.to(torch.float32)\n",
        "            batch_size = data.size(0)\n",
        "            target = target.to(torch.int64)\n",
        "            assert target.size() == torch.Size([batch_size, opt.n_classes])\n",
        "\n",
        "            # Use GPU if available\n",
        "            with torch.no_grad():\n",
        "                data, target = Variable(data), Variable(target)\n",
        "            data, target = data.cuda(), target.cuda()\n",
        "            \n",
        "            output = model(data)\n",
        "            loss, marginal_loss, reconstruction_loss = model.loss(output, data, target)\n",
        "\n",
        "            norms = torch.sqrt(torch.sum(output**2, dim=2))  # (batch_size, n_classes)\n",
        "            pred = norms.data.max(1, keepdim=True)[1].type(torch.LongTensor)  # (batch_size, 1)\n",
        "            label = target.max(1, keepdim=True)[1].type(torch.LongTensor)  # (batch_size, 1)\n",
        "            correct += pred.eq(label.view_as(pred)).sum().item()\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        if epoch % 1 == 0: \n",
        "            train_loss_list.append(loss.item())\n",
        "        if epoch % 1 == 0:\n",
        "            print('\\nEpoch: {}'.format(epoch))\n",
        "            evaluate(opt, train_loader, model, epoch, num_batches, 'TRAIN') \n",
        "            evaluate(opt, valid_loader, model, epoch, num_batches, 'VALID') \n",
        "        scheduler.step()\n",
        "    fig = plt.figure()\n",
        "    plt.plot([i for i in range(len(train_loss_list))], train_loss_list, '-')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4zXW1i3p1uq8"
      },
      "source": [
        "opt = get_opts()\n",
        "\n",
        "model = CapsuleNetwork(opt)\n",
        "model = model.cuda()\n",
        "\n",
        "train_loader = loader.train_loader\n",
        "valid_loader = loader.val_loader\n",
        "train(opt, train_loader, valid_loader, model)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}