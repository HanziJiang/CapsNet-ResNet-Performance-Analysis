{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CapsNet big.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PiQpTflFkuqi"
      },
      "source": [
        "# Capsule Neural Network (CapsNet) Implementation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QDxJ_A_0Gnt9"
      },
      "source": [
        "Implementation of the paper [Dynamic Routing Between Capsules](https://arxiv.org/pdf/1710.09829.pdf) by Sara Sabour, Nicholas Frosst, and Geoffrey E. Hinton. Used [jindongwang/Pytorch-CapsuleNet](https://github.com/jindongwang/Pytorch-CapsuleNet) and [laubonghaudoi/CapsNet_guide_PyTorch](https://github.com/laubonghaudoi/CapsNet_guide_PyTorch) to clarify some confusions, and borrowed some code."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_pkx4McDDItG"
      },
      "source": [
        "## Setup PyTorch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "peSdN_P8CElk",
        "outputId": "544d9e20-1ff1-4382-ef57-df00a2d328bd"
      },
      "source": [
        "!pip install torch torchvision\n",
        "!pip install matplotlib\n",
        "!pip install import-ipynb\n",
        "!pip install tqdm\n",
        "!pip install pytorch_extras"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (1.8.1+cu101)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (0.9.1+cu101)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch) (3.7.4.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch) (1.19.5)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.7/dist-packages (from torchvision) (7.1.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (3.2.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (0.10.0)\n",
            "Requirement already satisfied: numpy>=1.11 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (1.19.5)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (2.8.1)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (2.4.7)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from cycler>=0.10->matplotlib) (1.15.0)\n",
            "Collecting import-ipynb\n",
            "  Downloading https://files.pythonhosted.org/packages/63/35/495e0021bfdcc924c7cdec4e9fbb87c88dd03b9b9b22419444dc370c8a45/import-ipynb-0.1.3.tar.gz\n",
            "Building wheels for collected packages: import-ipynb\n",
            "  Building wheel for import-ipynb (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for import-ipynb: filename=import_ipynb-0.1.3-cp37-none-any.whl size=2976 sha256=747d0dbbe3f8feb12a696859201a0181659be9def08e73e56c0b05c21d8369a1\n",
            "  Stored in directory: /root/.cache/pip/wheels/b4/7b/e9/a3a6e496115dffdb4e3085d0ae39ffe8a814eacc44bbf494b5\n",
            "Successfully built import-ipynb\n",
            "Installing collected packages: import-ipynb\n",
            "Successfully installed import-ipynb-0.1.3\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (4.41.1)\n",
            "Collecting pytorch_extras\n",
            "  Downloading https://files.pythonhosted.org/packages/66/79/42d7d9a78c27eb897b14790c9759dd9a991f67bc987e9e137527a68db9dc/pytorch-extras-0.1.3.tar.gz\n",
            "Building wheels for collected packages: pytorch-extras\n",
            "  Building wheel for pytorch-extras (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pytorch-extras: filename=pytorch_extras-0.1.3-cp37-none-any.whl size=2833 sha256=24381728cf5b976e44395f6deec131c8a5488dde417138869e5cb6a5dc20075a\n",
            "  Stored in directory: /root/.cache/pip/wheels/5b/7c/5a/f27d4088adfe722cb280d523a1ed9eeb33be11b8d3a653292a\n",
            "Successfully built pytorch-extras\n",
            "Installing collected packages: pytorch-extras\n",
            "Successfully installed pytorch-extras-0.1.3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YMdBumXa75tw",
        "outputId": "02be8789-864c-45f0-f94f-477af106a3bc"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"mnt\")"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at mnt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wlYHtO737_dA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5ed5c794-5fd6-43f9-cb31-3b3f3b75dabc"
      },
      "source": [
        "%cd \"mnt/My Drive\""
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/mnt/My Drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rsZEHMrIEVz6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "05e2277f-c632-451e-86eb-b70b013cb8f7"
      },
      "source": [
        "import torch\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "import torch_extras\n",
        "import torch.nn as nn\n",
        "import torchvision.utils as tv_utils\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "import import_ipynb\n",
        "import load_DrawData_with_transform\n",
        "import numpy as np\n",
        "import argparse\n",
        "from tqdm import tqdm\n",
        "# %mkdir -p /content/project/\n",
        "# %cd /content/project/"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "importing Jupyter notebook from load_DrawData_with_transform.ipynb\n",
            "Collecting ndjson\n",
            "  Downloading https://files.pythonhosted.org/packages/70/c9/04ba0056011ba96a58163ebfd666d8385300bd12da1afe661a5a147758d7/ndjson-0.3.1-py2.py3-none-any.whl\n",
            "Installing collected packages: ndjson\n",
            "Successfully installed ndjson-0.3.1\n",
            "Collecting cairocffi\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/84/ca/0bffed5116d21251469df200448667e90acaa5131edea869b44a3fbc73d0/cairocffi-1.2.0.tar.gz (70kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 3.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: cffi>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from cairocffi) (1.14.5)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.1.0->cairocffi) (2.20)\n",
            "Building wheels for collected packages: cairocffi\n",
            "  Building wheel for cairocffi (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for cairocffi: filename=cairocffi-1.2.0-cp37-none-any.whl size=89548 sha256=3b7d66e5814ce46344c8ef9cdc89e1872864acc5136a4f190a384f2596ac3bb2\n",
            "  Stored in directory: /root/.cache/pip/wheels/40/76/48/f1effadceea83b32e7d957dd0f92db4db8b537d7b72b4ef374\n",
            "Successfully built cairocffi\n",
            "Installing collected packages: cairocffi\n",
            "Successfully installed cairocffi-1.2.0\n",
            "Requirement already satisfied: imutils in /usr/local/lib/python3.7/dist-packages (0.5.4)\n",
            "500\n",
            "mkdir: cannot create directory ‘quickDrawData’: File exists\n",
            "gs://quickdraw_dataset/full/simplified/apple.ndjson\n",
            "Copying gs://quickdraw_dataset/full/simplified/apple.ndjson...\n",
            "| [1/1 files][ 56.1 MiB/ 56.1 MiB] 100% Done                                    \n",
            "Operation completed over 1 objects/56.1 MiB.                                     \n",
            "gs://quickdraw_dataset/full/simplified/face.ndjson\n",
            "Copying gs://quickdraw_dataset/full/simplified/face.ndjson...\n",
            "/ [1/1 files][ 89.4 MiB/ 89.4 MiB] 100% Done                                    \n",
            "Operation completed over 1 objects/89.4 MiB.                                     \n",
            "apple\n",
            "face\n",
            "apple\n",
            "face\n",
            "apple\n",
            "face\n",
            "(40, 40)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XcYm7y8rECqg"
      },
      "source": [
        "## CapsNet Modules"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fC4zxyWWeF-k"
      },
      "source": [
        "def squash(s):\n",
        "    '''Non-linear \"squashing\" function to ensure that short vectors get shrunk \n",
        "    to almost zero length and long vectors get shrunk to a length slightly \n",
        "    below 1. Equation (1) in the paper.\n",
        "    \n",
        "    Input:\n",
        "      s: \ttotal input vector\n",
        "    \n",
        "    Output:\n",
        "      squashed output vector\n",
        "    '''\n",
        "    norm_sqrd = torch.sum(s**2, dim=2, keepdim=True)\n",
        "    return (norm_sqrd / (1 + norm_sqrd)) * (s / (torch.sqrt(norm_sqrd) + 1e-8))"
      ],
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VdGH3V4jJPmQ"
      },
      "source": [
        "class PrimaryCapsules(nn.Module):\n",
        "    '''The layer after Conv1. Section 4 of the paper.\n",
        "    '''\n",
        "\n",
        "    def __init__(self):\n",
        "        super(PrimaryCapsules, self).__init__()\n",
        "        self.capsules = nn.ModuleList(\n",
        "            [nn.Conv2d(in_channels=256,\n",
        "                    out_channels=8,\n",
        "                    kernel_size=9,\n",
        "                    stride=2)\n",
        "            for i in range(32)]\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        '''Section 4 of the paper.\n",
        "\n",
        "        Input:\n",
        "        x: outut of the ReLUConv1 layer, of shape (batch_size x 256 x 32 x 32)\n",
        "\n",
        "        Output:\n",
        "        squashed PrimaryCapsules output tensor, of shape (batch_size x 4608 x 8)\n",
        "        '''\n",
        "        batch_size = x.size(0)\n",
        "        \n",
        "        # Get the output of each primary capsule; combine and prepare them to \n",
        "        # serve as the input to the next layer (DoodleCapsules)\n",
        "        all_u = [] \n",
        "        assert x.shape==torch.Size([batch_size, 256, 32, 32]), x.shape\n",
        "        for cap in self.capsules:\n",
        "            u = cap(x)  # (batch_size x 8 x 12 x 12)\n",
        "            assert u.shape == torch.Size([batch_size, 8, 12, 12]), u.shape\n",
        "            u = u.view(batch_size, 8, 144, 1)  # (batch_size x 8 x 144 x 1)\n",
        "            all_u.append(u)\n",
        "        all_u = torch.cat(all_u, dim=3)  # (batch_size x 8 x 144 x 32)\n",
        "        assert all_u.shape == torch.Size([batch_size, 8, 144, 32]), all_u.shape\n",
        "        all_u = all_u.view(batch_size, 8, -1)  # (batch_size x 8 x 4)\n",
        "        all_u = torch.transpose(all_u, 1, 2)  # (batch_size x 11460852 x 8)\n",
        "        all_u = squash(all_u)  # (batch_size x 4608 x 8)\n",
        "        assert all_u.shape == torch.Size([batch_size, 4608, 8]), all_u.shape\n",
        "        \n",
        "        return all_u"
      ],
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xz1tFdzkaHyP"
      },
      "source": [
        "class DoodleCapsules(nn.Module):\n",
        "    '''The layer after PrimaryCapsules. Section 4 of the paper.\n",
        "    '''\n",
        "\n",
        "    def __init__(self, opt):\n",
        "        super(DoodleCapsules, self).__init__()\n",
        "        self.opt = opt\n",
        "        self.W = nn.Parameter(torch.randn(1, 4608, opt.n_classes, 8, 24))\n",
        "    \n",
        "    def forward(self, u):\n",
        "        '''Equation (2) and Procedure 1 in the paper.\n",
        "\n",
        "        Input:\n",
        "        u: output of the PrimaryCapsules layer, of shape (batch_size x 8)\n",
        "\n",
        "        Output:\n",
        "        output tensor of the DoodleCapsules layer, of shape (batch_size x 4608 x n_classes x 1)\n",
        "        '''\n",
        "        batch_size = u.size(0)\n",
        "        \n",
        "        u = torch.unsqueeze(u, dim=2)  # (batch_size x 1 x 8)\n",
        "        u = torch.unsqueeze(u, dim=2)  # (batch_size x 1 x 1 x 8)\n",
        "        u_hat = torch.matmul(u, self.W).squeeze()  # (batch_size x 4608 x n_classes x 24)\n",
        "\n",
        "        b = Variable(torch.zeros(batch_size, 4608, self.opt.n_classes, 1))  # (batch_size x 4608 x n_classes x 1)\n",
        "        b = b.cuda()\n",
        "\n",
        "        for i in range(self.opt.iterations):\n",
        "            c = F.softmax(b, dim=2)  # (batch_size x 4608 x n_classes x 1)\n",
        "            s = torch.sum(u_hat * c, dim=1)  # (batch_size x n_classes x 24)\n",
        "            v = squash(s)  # (batch_size x n_classes x 24)\n",
        "            b = b + torch.sum(u_hat * v.unsqueeze(1), dim=3, keepdim=True)  # (batch_size x 4608 x n_classes x 1)\n",
        "        \n",
        "        return v  # (batch_size x n_classes x 24)"
      ],
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q-k05aXUmDs8"
      },
      "source": [
        "class DoodleDecoder(nn.Module):\n",
        "    '''Decoder structure to reconstruct the doodle from the output of the DoodleCapsules layer.\n",
        "    Section 4 of the paper. For an illustrative explaination, see Figure 2 of the paper.\n",
        "    '''\n",
        "\n",
        "    def __init__(self, opt):\n",
        "        super(DoodleDecoder, self).__init__()\n",
        "        self.opt = opt\n",
        "        self.layers = nn.Sequential(\n",
        "            nn.Linear(opt.n_classes * 24, 512),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(512, 1024),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(1024, opt.image_size * opt.image_size),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, v, target):\n",
        "        '''Takes a 24-dimensional vector v from the *correct* DoodleCapsules, and \n",
        "        learns to decode it into an image of a doodle. Mask out the other (n_classes - 1) classes.\n",
        "        Section 4 of the paper. For an illustrative explaination, see Figure 2 of the paper.\n",
        "\n",
        "        Input:\n",
        "        v: output of DoodleCapsules, of shape (batch_size x n_classes x 24)\n",
        "        target: one-hot targets, of shape (batch_size, n_classes)\n",
        "\n",
        "        Output:\n",
        "        decoder constructed images, of shape (batch_size x image_size^2)\n",
        "        '''\n",
        "        # TODO: the true target or the most probable?\n",
        "        # correct = torch.sqrt((v ** 2).sum(2))  # (batch_size x n_classes)\n",
        "        # correct = F.softmax(correct, dim=0)  # (batch_size x n_classes)\n",
        "        # correct = correct.max(dim=1)[1]  # (batch_size)\n",
        "        batch_size = v.size(0)\n",
        "\n",
        "        # Create the mask, which is 1 only for the correct class and 0 otherwise\n",
        "        mask = target.type(torch.FloatTensor).unsqueeze(-1)  # (batch_size x n_classes x 1)\n",
        "        mask = torch.repeat_interleave(mask, 24, dim=2)  # (batch_size x n_classes x 24)\n",
        "        mask = mask.cuda()\n",
        "        assert mask.size() == torch.Size([batch_size, 2, 24]), mask.size()\n",
        "        \n",
        "        masked = (v * mask).view(batch_size, -1)  # (batch_size x n_classes x 24)\n",
        "        result = self.layers(masked)  # (batch_size x f)\n",
        "        assert result.shape == torch.Size([batch_size, opt.image_size*opt.image_size]), result.shape\n",
        "        \n",
        "        return result"
      ],
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CH6pouLtXQnx"
      },
      "source": [
        "class CapsuleNetwork(nn.Module):\n",
        "    '''Consists of a ReLU Convolution layer, a PrimaryCapsules layer, a DoodleCapsules\n",
        "    layer, and a Decoder layer. Section 4 of the paper.\n",
        "    '''\n",
        "\n",
        "    def __init__(self, opt):\n",
        "        super(CapsuleNetwork, self).__init__()\n",
        "        self.opt = opt\n",
        "        self.ReLUConv1 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=1, out_channels=256, kernel_size=9),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        self.PrimaryCapsules = PrimaryCapsules()\n",
        "        self.DoodleCapsules = DoodleCapsules(opt)\n",
        "        self.DoodleDecoder = DoodleDecoder(opt)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        '''Section 4 of the paper.\n",
        "\n",
        "        Input:\n",
        "        the input to the network, of shape (batch_size x image_size x image_size)\n",
        "\n",
        "        Output:\n",
        "        the output of the network, of shape (batch_size x n_classes x 24)\n",
        "        '''\n",
        "        v = torch.unsqueeze(x, 1)\n",
        "        v = self.ReLUConv1(v)  # (batch_size x 256 x 32 x 32)\n",
        "        v = self.PrimaryCapsules(v)  # (batch_size x 4608 x 8)\n",
        "        v = self.DoodleCapsules(v)  # (batch_size x n_classes x 24)\n",
        "        return v  # (batch_size x n_classes x 24)\n",
        "\n",
        "    def marginal_loss(self, v, target):\n",
        "        '''Section 3, Equation (4) of the paper.\n",
        "\n",
        "        Input:\n",
        "        v: the output of the network, of shape (batch_size x n_classes x 24)\n",
        "        target: the one-hot target, of shape (batch_size x n_classes)\n",
        "        lambd: a scalor for down-weighting of the loss for absent doodle classes\n",
        "        \n",
        "        Output:\n",
        "        marginal loss (a scalor summed over all batches and classes)\n",
        "        '''\n",
        "        v_norm = torch.sqrt((v ** 2).sum(dim=2))  # (batch_size x n_classes)\n",
        "        \n",
        "        zeros = torch.zeros(v_norm.size())  # (batch_size x n_classes)\n",
        "        zeros = zeros.cuda()\n",
        "        \n",
        "        max1 = torch.max(zeros, 0.9 - v_norm) ** 2  # (batch_size x n_classes)\n",
        "        max2 = torch.max(zeros, v_norm - 0.1) ** 2  # (batch_size x n_classes) \n",
        "        loss = target * max1 + (1 - target) * 0.5 * max2  # (batch_size x n_classes)\n",
        "        \n",
        "        return torch.sum(loss)  # scalor\n",
        "\n",
        "    def reconstruction_loss(self, data, reconstruction):\n",
        "        '''Reconstruction for regularization. Ecourages the doodle capsules to \n",
        "        encode the instantiation parameters of the input doodle. Section 4.1 of the paper.\n",
        "\n",
        "        Input:\n",
        "        data: the real image, of shape (batch_size, image_size, image_size)\n",
        "        reconstruction: the reconstructed image, of shape (batch_size, image_size * image_size)\n",
        "\n",
        "        Output:\n",
        "        reconstruction loss (a scalor summed over all batches and classes)\n",
        "        '''\n",
        "        batch_size = data.size(0)\n",
        "        return torch.sum((reconstruction - data.view(batch_size, -1)) ** 2)\n",
        "\n",
        "    def loss(self, v, data, target):\n",
        "        '''Loss is marginal loss + 0.0005 * reconstruction loss. 0.0005 to ensure\n",
        "        the reconstruction loss does not dominate the training. Section 4.1 of the paper.\n",
        "\n",
        "        Input:\n",
        "        v: output of the network, of shape (batch_size x n_classes x 24)\n",
        "        target: one-hot target, of shape (batch_size x n_classes)\n",
        "        data: the input to the network (the image), of shape (batch_size, image_size, image_size)\n",
        "        \n",
        "        Output:\n",
        "        averaged loss (a scalor) over batches \n",
        "        averaged marginal loss (a scalor) over batches\n",
        "        averaged reconstruction loss (a scalor) over batches\n",
        "        '''\n",
        "        batch_size = data.size(0)\n",
        "        \n",
        "        marginal_loss = self.marginal_loss(v, target)  # scalor\n",
        "        \n",
        "        reconstruction = self.DoodleDecoder(v, target)  # (batch_size, image_size^2)\n",
        "        assert reconstruction.shape == torch.Size([batch_size, 40 * 40]), (reconstruction.shape, opt.image_size)\n",
        "        reconstruction_loss = self.reconstruction_loss(data, reconstruction)  # scalor\n",
        "        \n",
        "        loss = marginal_loss + 0.0005 * reconstruction_loss  # scalor\n",
        "        \n",
        "        return loss/batch_size, marginal_loss/batch_size, reconstruction_loss/batch_size"
      ],
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zKPXAofhnYhK"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Holn-KGHnvZH"
      },
      "source": [
        "def get_opts():\n",
        "    parser = argparse.ArgumentParser(description='CapsuleNetwork')\n",
        "    parser.add_argument('-batch_size', type=int, default=32)\n",
        "    parser.add_argument('-lr', type=float, default=1e-6)\n",
        "    parser.add_argument('-epochs', type=int, default=200)\n",
        "    parser.add_argument('-image_size', type=int, default=40)\n",
        "    parser.add_argument('-n_classes', type=int, default=2)\n",
        "    parser.add_argument('-iterations', type=int, default=3)\n",
        "    parser.add_argument('-print_every', type=int, default=10)\n",
        "    parser.add_argument('-gamma', type=float, default=0.8)\n",
        "    opt, _ = parser.parse_known_args()\n",
        "    return opt"
      ],
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QQCM0T0_1d5W"
      },
      "source": [
        "def evaluate(opt, test_loader, model, epoch, num_batches, dataset_type):\n",
        "    sum_loss = 0\n",
        "    sum_marginal_loss = 0\n",
        "    sum_reconstruction_loss = 0\n",
        "    correct = 0\n",
        "    num_sample = len(test_loader.dataset)\n",
        "    num_batch = len(test_loader)\n",
        "\n",
        "    model.eval()\n",
        "    for data, target in test_loader:\n",
        "        data = data.to(torch.float32)\n",
        "        target = target.to(torch.int64)\n",
        "        batch_size = data.size(0)\n",
        "        assert target.size() == torch.Size([batch_size, opt.n_classes])\n",
        "\n",
        "        # Use GPU if available\n",
        "        with torch.no_grad():\n",
        "            data, target = Variable(data), Variable(target)\n",
        "        data, target = data.cuda(), target.cuda()\n",
        "\n",
        "        output = model(data)  # (batch_size, n_classes, 24)\n",
        "        loss, marginal_loss, reconstruction_loss = model.loss(output, data, target)\n",
        "        sum_loss += loss.item()\n",
        "        sum_marginal_loss += marginal_loss.item()\n",
        "        sum_reconstruction_loss += reconstruction_loss.item()\n",
        "\n",
        "        norms = torch.sqrt(torch.sum(output**2, dim=2))  # (batch_size, n_classes)\n",
        "        pred = norms.data.max(1, keepdim=True)[1].type(torch.LongTensor)  # (batch_size, 1)\n",
        "        label = target.max(1, keepdim=True)[1].type(torch.LongTensor)  # (batch_size, 1)\n",
        "        correct += pred.eq(label.view_as(pred)).sum().item()\n",
        "\n",
        "    recons = model.DoodleDecoder(output, target)\n",
        "    recons = recons.view(batch_size, 1, opt.image_size, opt.image_size)\n",
        "    recons = tv_utils.make_grid(recons.data, normalize=True, scale_each=True)\n",
        "\n",
        "    sum_loss /= num_batch\n",
        "    sum_marginal_loss /= num_batch\n",
        "    sum_reconstruction_loss /= num_batch\n",
        "    \n",
        "    print('{}'.format(dataset_type))\n",
        "    print('\\tLoss: {:.4f}   Marginal loss: {:.4f}   Reconstruction loss: {:.4f}'.format(\n",
        "        sum_loss, sum_marginal_loss, sum_reconstruction_loss))\n",
        "    print('\\tAccuracy: {}/{} {:.4f}'.format(correct, num_sample,\n",
        "        correct / num_sample))"
      ],
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jcoOvQfb1bVn"
      },
      "source": [
        "def train(opt, train_loader, test_loader, model):\n",
        "    num_sample = len(train_loader.dataset)\n",
        "    num_batches = len(train_loader)\n",
        "    train_loss_list = []\n",
        "    correct = 0\n",
        "\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=opt.lr)\n",
        "    scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, opt.gamma)\n",
        "    model.train()\n",
        "    for epoch in range(opt.epochs):\n",
        "        for batch_idx, (data, target) in enumerate(train_loader):\n",
        "            optimizer.zero_grad()\n",
        "            data = data.to(torch.float32)\n",
        "            batch_size = data.size(0)\n",
        "            target = target.to(torch.int64)\n",
        "            assert target.size() == torch.Size([batch_size, opt.n_classes])\n",
        "\n",
        "            # Use GPU if available\n",
        "            with torch.no_grad():\n",
        "                data, target = Variable(data), Variable(target)\n",
        "            data, target = data.cuda(), target.cuda()\n",
        "            \n",
        "            output = model(data)\n",
        "            loss, marginal_loss, reconstruction_loss = model.loss(output, data, target)\n",
        "\n",
        "            norms = torch.sqrt(torch.sum(output**2, dim=2))  # (batch_size, n_classes)\n",
        "            pred = norms.data.max(1, keepdim=True)[1].type(torch.LongTensor)  # (batch_size, 1)\n",
        "            label = target.max(1, keepdim=True)[1].type(torch.LongTensor)  # (batch_size, 1)\n",
        "            correct += pred.eq(label.view_as(pred)).sum().item()\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        if epoch % 1 == 0: \n",
        "            train_loss_list.append(loss.item())\n",
        "        if epoch % 4 == 0:\n",
        "            print('\\nEpoch: {}'.format(epoch))\n",
        "            evaluate(opt, train_loader, model, epoch, num_batches, 'TRAIN') \n",
        "            evaluate(opt, test_loader, model, epoch, num_batches, 'TEST') \n",
        "        scheduler.step()\n",
        "    fig = plt.figure()\n",
        "    plt.plot([i for i in range(len(train_loss_list))], train_loss_list, '-')"
      ],
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4zXW1i3p1uq8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 731
        },
        "outputId": "b0f2d633-6e36-49c6-b609-53943ed5a7bb"
      },
      "source": [
        "opt = get_opts()\n",
        "\n",
        "model = CapsuleNetwork(opt)\n",
        "model = model.cuda()\n",
        "\n",
        "train_loader = load_DrawData_with_transform.train_loader\n",
        "\n",
        "train(opt, train_loader, test_loader, model)"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch: 0\n",
            "TRAIN\n",
            "\tLoss: 3563.7421   Marginal loss: 0.4050   Reconstruction loss: 7126673.8864\n",
            "\tAccuracy: 351/700 0.5014\n",
            "TEST\n",
            "\tLoss: 3532.8020   Marginal loss: 0.4050   Reconstruction loss: 7064793.7059\n",
            "\tAccuracy: 296/518 0.5714\n",
            "\n",
            "Epoch: 4\n",
            "TRAIN\n",
            "\tLoss: 3565.1707   Marginal loss: 0.4050   Reconstruction loss: 7129531.0000\n",
            "\tAccuracy: 436/700 0.6229\n",
            "TEST\n",
            "\tLoss: 3532.7789   Marginal loss: 0.4050   Reconstruction loss: 7064747.3824\n",
            "\tAccuracy: 314/518 0.6062\n",
            "\n",
            "Epoch: 8\n",
            "TRAIN\n",
            "\tLoss: 3564.5872   Marginal loss: 0.4050   Reconstruction loss: 7128363.9318\n",
            "\tAccuracy: 432/700 0.6171\n",
            "TEST\n",
            "\tLoss: 3532.7707   Marginal loss: 0.4050   Reconstruction loss: 7064731.2353\n",
            "\tAccuracy: 327/518 0.6313\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-69-ba85abb9acd3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mtrain_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_DrawData_with_transform\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-68-8dd5a12f3e16>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(opt, train_loader, test_loader, model)\u001b[0m\n\u001b[1;32m     21\u001b[0m             \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmarginal_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreconstruction_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-65-ec24397b4c30>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mReLUConv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# (batch_size x 256 x 32 x 32)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPrimaryCapsules\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# (batch_size x 4608 x 8)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m         \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDoodleCapsules\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# (batch_size x n_classes x 24)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mv\u001b[0m  \u001b[0;31m# (batch_size x n_classes x 24)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-63-42e28f2b56eb>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, u)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4608\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_classes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# (batch_size x 4608 x n_classes x 1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}