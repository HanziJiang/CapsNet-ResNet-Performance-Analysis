{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "guide.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PiQpTflFkuqi"
      },
      "source": [
        "# Capsule Neural Network (CapsNet) Implementation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QDxJ_A_0Gnt9"
      },
      "source": [
        "Implementation of the paper [Dynamic Routing Between Capsules](https://arxiv.org/pdf/1710.09829.pdf) by Sara Sabour, Nicholas Frosst, and Geoffrey E. Hinton. Used [jindongwang/Pytorch-CapsuleNet](https://github.com/jindongwang/Pytorch-CapsuleNet) and [laubonghaudoi/CapsNet_guide_PyTorch](https://github.com/laubonghaudoi/CapsNet_guide_PyTorch) to clarify some confusions, and borrowed some code."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_pkx4McDDItG"
      },
      "source": [
        "## Setup PyTorch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "peSdN_P8CElk",
        "outputId": "e118bca2-d62e-41c3-81bd-37ea41157a0a"
      },
      "source": [
        "!pip install torch torchvision\n",
        "!pip install matplotlib\n",
        "!pip install import-ipynb\n",
        "!pip install tqdm\n",
        "!pip install pytorch_extras"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (1.8.1+cu101)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (0.9.1+cu101)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch) (1.19.5)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch) (3.7.4.3)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.7/dist-packages (from torchvision) (7.1.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (3.2.2)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (2.4.7)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (0.10.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (2.8.1)\n",
            "Requirement already satisfied: numpy>=1.11 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (1.19.5)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (1.3.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from cycler>=0.10->matplotlib) (1.15.0)\n",
            "Collecting import-ipynb\n",
            "  Downloading https://files.pythonhosted.org/packages/63/35/495e0021bfdcc924c7cdec4e9fbb87c88dd03b9b9b22419444dc370c8a45/import-ipynb-0.1.3.tar.gz\n",
            "Building wheels for collected packages: import-ipynb\n",
            "  Building wheel for import-ipynb (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for import-ipynb: filename=import_ipynb-0.1.3-cp37-none-any.whl size=2976 sha256=31a13bcb8276a3d693c1c30896b147345659bc7f8ea3f2038c79c31c1333681b\n",
            "  Stored in directory: /root/.cache/pip/wheels/b4/7b/e9/a3a6e496115dffdb4e3085d0ae39ffe8a814eacc44bbf494b5\n",
            "Successfully built import-ipynb\n",
            "Installing collected packages: import-ipynb\n",
            "Successfully installed import-ipynb-0.1.3\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (4.41.1)\n",
            "Collecting pytorch_extras\n",
            "  Downloading https://files.pythonhosted.org/packages/66/79/42d7d9a78c27eb897b14790c9759dd9a991f67bc987e9e137527a68db9dc/pytorch-extras-0.1.3.tar.gz\n",
            "Building wheels for collected packages: pytorch-extras\n",
            "  Building wheel for pytorch-extras (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pytorch-extras: filename=pytorch_extras-0.1.3-cp37-none-any.whl size=2833 sha256=e46f5a8c8488a0ac52883af252048fca2dd5957170172b5d540ff4351a84103b\n",
            "  Stored in directory: /root/.cache/pip/wheels/5b/7c/5a/f27d4088adfe722cb280d523a1ed9eeb33be11b8d3a653292a\n",
            "Successfully built pytorch-extras\n",
            "Installing collected packages: pytorch-extras\n",
            "Successfully installed pytorch-extras-0.1.3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YMdBumXa75tw",
        "outputId": "017efc75-79ac-4a2c-877e-d16dc37b9d83"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"mnt\")"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at mnt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wlYHtO737_dA",
        "outputId": "43095e55-f3ec-4847-99e2-3ce8ad8bebce"
      },
      "source": [
        "%cd \"mnt/My Drive\""
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/mnt/My Drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rsZEHMrIEVz6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "540ca6bf-1350-4944-b865-d35cd1871a74"
      },
      "source": [
        "import torch\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "import torch_extras\n",
        "import torch.nn as nn\n",
        "import torchvision.utils as tv_utils\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "import import_ipynb\n",
        "import load_DrawData_with_transform_28 as loader\n",
        "import numpy as np\n",
        "import argparse\n",
        "from tqdm import tqdm\n",
        "%matplotlib inline\n",
        "# %mkdir -p /content/project/\n",
        "# %cd /content/project/"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "importing Jupyter notebook from load_DrawData.ipynb\n",
            "Collecting ndjson\n",
            "  Downloading https://files.pythonhosted.org/packages/70/c9/04ba0056011ba96a58163ebfd666d8385300bd12da1afe661a5a147758d7/ndjson-0.3.1-py2.py3-none-any.whl\n",
            "Installing collected packages: ndjson\n",
            "Successfully installed ndjson-0.3.1\n",
            "Collecting cairocffi\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/84/ca/0bffed5116d21251469df200448667e90acaa5131edea869b44a3fbc73d0/cairocffi-1.2.0.tar.gz (70kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 6.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: cffi>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from cairocffi) (1.14.5)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.1.0->cairocffi) (2.20)\n",
            "Building wheels for collected packages: cairocffi\n",
            "  Building wheel for cairocffi (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for cairocffi: filename=cairocffi-1.2.0-cp37-none-any.whl size=89548 sha256=fd3808a684be701cc5cafe7dbc94dd73557946fa1d2908def663ac100e586224\n",
            "  Stored in directory: /root/.cache/pip/wheels/40/76/48/f1effadceea83b32e7d957dd0f92db4db8b537d7b72b4ef374\n",
            "Successfully built cairocffi\n",
            "Installing collected packages: cairocffi\n",
            "Successfully installed cairocffi-1.2.0\n",
            "3600\n",
            "2400\n",
            "total training samples: 3600\n",
            "total validatoin samples: 1200\n",
            "total test samples: 1200\n",
            "mkdir: cannot create directory ‘quickDrawData’: File exists\n",
            "gs://quickdraw_dataset/full/simplified/triangle.ndjson\n",
            "Copying gs://quickdraw_dataset/full/simplified/triangle.ndjson...\n",
            "- [1/1 files][ 30.5 MiB/ 30.5 MiB] 100% Done                                    \n",
            "Operation completed over 1 objects/30.5 MiB.                                     \n",
            "gs://quickdraw_dataset/full/simplified/circle.ndjson\n",
            "Copying gs://quickdraw_dataset/full/simplified/circle.ndjson...\n",
            "\\ [1/1 files][ 38.5 MiB/ 38.5 MiB] 100% Done                                    \n",
            "Operation completed over 1 objects/38.5 MiB.                                     \n",
            "adding from:  triangle\n",
            "adding from:  circle\n",
            "building  train  dataset\n",
            "adding from:  triangle\n",
            "adding from:  circle\n",
            "building  val  dataset\n",
            "adding from:  triangle\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-48d1a38c7f4e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mimport_ipynb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mload_DrawData\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mloader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0margparse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_load_unlocked\u001b[0;34m(spec)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_load_backward_compatible\u001b[0;34m(spec)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/import_ipynb.py\u001b[0m in \u001b[0;36mload_module\u001b[0;34m(self, fullname)\u001b[0m\n\u001b[1;32m     59\u001b[0m                 \u001b[0mcode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshell\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_transformer_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform_cell\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcell\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m                 \u001b[0;31m# run the code in themodule\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m                 \u001b[0mexec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmod\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__dict__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshell\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muser_ns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msave_user_ns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/mnt/My Drive/load_DrawData.ipynb\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/content/mnt/My Drive/load_DrawData.ipynb\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, dataset_type, image_size, categories, number_per_catogory, transformation)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ndjson/api.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetdefault\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cls'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDecoder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/json/__init__.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(fp, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    294\u001b[0m         \u001b[0mcls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobject_hook\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobject_hook\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m         \u001b[0mparse_float\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparse_float\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparse_int\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparse_int\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 296\u001b[0;31m         parse_constant=parse_constant, object_pairs_hook=object_pairs_hook, **kw)\n\u001b[0m\u001b[1;32m    297\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/json/__init__.py\u001b[0m in \u001b[0;36mloads\u001b[0;34m(s, encoding, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    359\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mparse_constant\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    360\u001b[0m         \u001b[0mkw\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'parse_constant'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparse_constant\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 361\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ndjson/codecs.py\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, s, *args, **kwargs)\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mlines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m','\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplitlines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'[{}]'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlines\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDecoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/json/decoder.py\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, s, _w)\u001b[0m\n\u001b[1;32m    335\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m         \"\"\"\n\u001b[0;32m--> 337\u001b[0;31m         \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_w\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    338\u001b[0m         \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_w\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    339\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/json/decoder.py\u001b[0m in \u001b[0;36mraw_decode\u001b[0;34m(self, s, idx)\u001b[0m\n\u001b[1;32m    351\u001b[0m         \"\"\"\n\u001b[1;32m    352\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 353\u001b[0;31m             \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscan_once\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    354\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    355\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mJSONDecodeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Expecting value\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XcYm7y8rECqg"
      },
      "source": [
        "## CapsNet Modules"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VdGH3V4jJPmQ"
      },
      "source": [
        "class PrimaryCapsules(nn.Module):\n",
        "    '''\n",
        "    The `PrimaryCaps` layer consists of 32 capsule units. Each unit takes\n",
        "    the output of the `Conv1` layer, which is a `[256, 20, 20]` feature\n",
        "    tensor (omitting `batch_size`), and performs a 2D convolution with 8\n",
        "    output channels, kernel size 9 and stride 2, thus outputing a [8, 6, 6]\n",
        "    tensor. In other words, you can see these 32 capsules as 32 paralleled 2D\n",
        "    convolutional layers. Then we concatenate these 32 capsules' outputs and\n",
        "    flatten them into a tensor of size `[1152, 8]`, representing 1152 8D\n",
        "    vectors, and send it to the next layer `DigitCaps`.\n",
        "    As indicated in Section 4, Page 4 in the paper, *One can see PrimaryCaps\n",
        "    as a Convolution layer with Eq.1 as its block non-linearity.*, outputs of\n",
        "    the `PrimaryCaps` layer are squashed before being passed to the next layer.\n",
        "    Reference: Section 4, Fig. 1\n",
        "    '''\n",
        "\n",
        "    def __init__(self):\n",
        "        '''\n",
        "        We build 8 capsule units in the `PrimaryCaps` layer, each can be\n",
        "        seen as a 2D convolution layer.\n",
        "        '''\n",
        "        super(PrimaryCapsules, self).__init__()\n",
        "\n",
        "        self.capsules = nn.ModuleList([\n",
        "            nn.Conv2d(in_channels=256,\n",
        "                      out_channels=8,\n",
        "                      kernel_size=9,\n",
        "                      stride=2)\n",
        "            for i in range(32)\n",
        "        ])\n",
        "\n",
        "    def forward(self, x):\n",
        "        '''\n",
        "        Each capsule outputs a [batch_size, 8, 6, 6] tensor, we need to\n",
        "        flatten and concatenate them into a [batch_size, 8, 6*6, 32] size\n",
        "        tensor and flatten and transpose into `u` [batch_size, 1152, 8], \n",
        "        where each [batch_size, 1152, 1] size tensor is the `u_i` in Eq.2. \n",
        "        #### Dimension transformation in this layer(ignoring `batch_size`):\n",
        "        [256, 20, 20] --> [8, 6, 6] x 32 capsules --> [1152, 8]\n",
        "        Note: `u_i` is one [1, 8] in the final [1152, 8] output, thus there are\n",
        "        1152 `u_i`s.\n",
        "        '''\n",
        "        batch_size = x.size(0)\n",
        "\n",
        "        u = []\n",
        "        for i in range(32):\n",
        "            # Input: [batch_size, 256, 20, 20]\n",
        "            assert x.data.size() == torch.Size([batch_size, 256, 20, 20])\n",
        "\n",
        "            u_i = self.capsules[i](x)\n",
        "            assert u_i.size() == torch.Size([batch_size, 8, 6, 6])\n",
        "            # u_i: [batch_size, 8, 6, 6]\n",
        "            u_i = u_i.view(batch_size, 8, -1, 1)\n",
        "            # u_i: [batch_size, 8, 36]\n",
        "            u.append(u_i)\n",
        "\n",
        "        # u: [batch_size, 8, 36, 1] x 32\n",
        "        u = torch.cat(u, dim=3)\n",
        "        # u: [batch_size, 8, 36, 32]\n",
        "        u = u.view(batch_size, 8, -1)\n",
        "        # u: [batch_size, 8, 1152]\n",
        "        u = torch.transpose(u, 1, 2)\n",
        "        # u: [batch_size, 1152, 8]\n",
        "        assert u.data.size() == torch.Size([batch_size, 1152, 8])\n",
        "\n",
        "        # Squash before output\n",
        "        u_squashed = self.squash(u)\n",
        "\n",
        "        return u_squashed\n",
        "\n",
        "    def squash(self, u):\n",
        "        '''\n",
        "        Args:\n",
        "            `u`: [batch_size, 1152, 8]\n",
        "        Return:\n",
        "            `u_squashed`: [batch_size, 1152, 8]\n",
        "        In CapsNet, we use the squash function after the output of both \n",
        "        capsule layers. Squash functions can be seen as activating functions\n",
        "        like sigmoid, but for capsule layers rather than traditional fully\n",
        "        connected layers, as they squash vectors instead of scalars.\n",
        "        v_j = (norm(s_j) ^ 2 / (1 + norm(s_j) ^ 2)) * (s_j / norm(s_j))\n",
        "        Reference: Eq.1 in Section 2.\n",
        "        '''\n",
        "        batch_size = u.size(0)\n",
        "\n",
        "        # u: [batch_size, 1152, 8]\n",
        "        square = u ** 2\n",
        "\n",
        "        # square_sum for u: [batch_size, 1152]\n",
        "        square_sum = torch.sum(square, dim=2)\n",
        "\n",
        "        # norm for u: [batch_size, 1152]\n",
        "        norm = torch.sqrt(square_sum)\n",
        "\n",
        "        # factor for u: [batch_size, 1152]\n",
        "        factor = norm ** 2 / (norm * (1 + norm ** 2))\n",
        "\n",
        "        # u_squashed: [batch_size, 1152, 8]\n",
        "        u_squashed = factor.unsqueeze(2) * u\n",
        "        assert u_squashed.size() == torch.Size([batch_size, 1152, 8])\n",
        "\n",
        "        return u_squashed"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xz1tFdzkaHyP"
      },
      "source": [
        "class DoodleCapsules(nn.Module):\n",
        "    '''\n",
        "    The `DigitCaps` layer consists of 10 16D capsules. Compared to the traditional\n",
        "    scalar output neurons in fully connected networks(FCN), the `DigitCaps` layer\n",
        "    can be seen as an FCN with ten 16-dimensional output neurons, which we call\n",
        "    these neurons \"capsules\".\n",
        "    In this layer, we take the input `[1152, 8]` tensor `u` as 1152 [8,] vectors\n",
        "    `u_i`, each `u_i` is a 8D output of the capsules from `PrimaryCaps` (see Eq.2\n",
        "    in Section 2, Page 2) and sent to the 10 capsules. For each capsule, the tensor\n",
        "    is first transformed by `W_ij`s into [1152, 16] size. Then we perform the Dynamic\n",
        "    Routing algorithm to get the output `v_j` of size [16,]. As there are 10 capsules,\n",
        "    the final output is [16, 10] size.\n",
        "    #### Dimension transformation in this layer(ignoring `batch_size`):\n",
        "    [1152, 8] --> [1152, 16] --> [1, 16] x 10 capsules --> [10, 16] output\n",
        "    Note that in our codes we have vectorized these computations, so the dimensions\n",
        "    above are just for understanding, actual dimensions of tensors are different.\n",
        "    '''\n",
        "\n",
        "    def __init__(self, opt):\n",
        "        '''\n",
        "        There is only one parameter in this layer, `W` [1, 1152, 10, 16, 8], where\n",
        "        every [8, 16] is a weight matrix W_ij in Eq.2, that is, there are 11520\n",
        "        `W_ij`s in total.\n",
        "        The the coupling coefficients `b` [64, 1152, 10, 1] is a temporary variable which\n",
        "        does NOT belong to the layer's parameters. In other words, `b` is not updated\n",
        "        by gradient back-propagations. Instead, we update `b` by Dynamic Routing\n",
        "        in every forward propagation. See the docstring of `self.forward` for details.\n",
        "        '''\n",
        "        super(DoodleCapsules, self).__init__()\n",
        "        self.opt = opt\n",
        "\n",
        "        self.W = nn.Parameter(torch.randn(1, 1152, opt.n_classes, 8, 16))\n",
        "\n",
        "    def forward(self, u):\n",
        "        '''\n",
        "        Args:\n",
        "            `u`: [batch_size, 1152, 8]\n",
        "        Return:\n",
        "            `v`: [batch_size, 10, 16]\n",
        "        In this layer, we vectorize our computations by calling `W` and using\n",
        "        `torch.matmul()`. Thus the full computaion steps are as follows.\n",
        "            1. Expand `W` into batches and compute `u_hat` (Eq.2)\n",
        "            2. Line 2: Initialize `b` into zeros\n",
        "            3. Line 3: Start Routing for `r` iterations:\n",
        "                1. Line 4: c = softmax(b)\n",
        "                2. Line 5: s = sum(c * u_hat)\n",
        "                3. Line 6: v = squash(s)\n",
        "                4. Line 7: b += u_hat * v\n",
        "        The coupling coefficients `b` can be seen as a kind of attention matrix\n",
        "        in the attentional sequence-to-sequence networks, which is widely used in\n",
        "        Neural Machine Translation systems. For tutorials on  attentional seq2seq\n",
        "        models, see https://arxiv.org/abs/1703.01619 or\n",
        "        http://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html\n",
        "        Reference: Section 2, Procedure 1\n",
        "        '''\n",
        "        batch_size = u.size(0)\n",
        "\n",
        "        # First, we need to expand the dimensions of `W` and `u` to compute `u_hat`\n",
        "        assert u.size() == torch.Size([batch_size, 1152, 8])\n",
        "        # u: [batch_size, 1152, 1, 1, 8]\n",
        "        u = torch.unsqueeze(u, dim=2)\n",
        "        u = torch.unsqueeze(u, dim=2)\n",
        "        # Now we compute u_hat in Eq.2\n",
        "        # u_hat: [batch_size, 1152, 10, 16]\n",
        "        u_hat = torch.matmul(u, self.W).squeeze()\n",
        "\n",
        "        # Line 2: Initialize b into zeros\n",
        "        # b: [batch_size, 1152, 10, 1]\n",
        "        b = Variable(torch.zeros(batch_size, 1152, opt.n_classes, 1))\n",
        "        if self.opt.use_cuda & torch.cuda.is_available():\n",
        "            b = b.cuda()\n",
        "\n",
        "        # Start Routing\n",
        "        for r in range(self.opt.r):\n",
        "            # Line 4: c_i = softmax(b_i)\n",
        "            # c: [b, 1152, 10, 1]\n",
        "            c = F.softmax(b, dim=2)\n",
        "            assert c.size() == torch.Size([batch_size, 1152, opt.n_classes, 1])\n",
        "\n",
        "            # Line 5: s_j = sum_i(c_ij * u_hat_j|i)\n",
        "            # u_hat: [batch_size, 1152, 10, 16]\n",
        "            # s: [batch_size, 10, 16]\n",
        "            s = torch.sum(u_hat * c, dim=1)\n",
        "\n",
        "            # Line 6: v_j = squash(s_j)\n",
        "            # v: [batch_size, 10, 16]\n",
        "            v = self.squash(s)\n",
        "            assert v.size() == torch.Size([batch_size, opt.n_classes, 16])\n",
        "\n",
        "            # Line 7: b_ij += u_hat * v_j\n",
        "            # u_hat: [batch_size, 1152, 10, 16]\n",
        "            # v: [batch_size, 10, 16]\n",
        "            # a: [batch_size, 10, 1152, 16]\n",
        "            a = u_hat * v.unsqueeze(1)\n",
        "            # b: [batch_size, 1152, 10, 1]\n",
        "            b = b + torch.sum(a, dim=3, keepdim=True)\n",
        "\n",
        "        return v\n",
        "\n",
        "    def squash(self, s):\n",
        "        '''\n",
        "        Args:\n",
        "            `s`: [batch_size, 10, 16]\n",
        "        v_j = (norm(s_j) ^ 2 / (1 + norm(s_j) ^ 2)) * (s_j / norm(s_j))\n",
        "        Reference: Eq.1 in Section 2.\n",
        "        '''\n",
        "        batch_size = s.size(0)\n",
        "\n",
        "        # s: [batch_size, 10, 16]\n",
        "        square = s ** 2\n",
        "\n",
        "        # square_sum for v: [batch_size, 10]\n",
        "        square_sum = torch.sum(square, dim=2)\n",
        "\n",
        "        # norm for v: [batch_size, 10]\n",
        "        norm = torch.sqrt(square_sum)\n",
        "\n",
        "        # factor for v: [batch_size, 10]\n",
        "        factor = norm ** 2 / (norm * (1 + norm ** 2))\n",
        "\n",
        "        # v: [batch_size, 10, 16]\n",
        "        v = factor.unsqueeze(2) * s\n",
        "        assert v.size() == torch.Size([batch_size, opt.n_classes, 16])\n",
        "\n",
        "        return v"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q-k05aXUmDs8"
      },
      "source": [
        "class DoodleDecoder(nn.Module):\n",
        "    '''\n",
        "    The decoder network consists of 3 fully connected layers. For each\n",
        "    [10, 16] output, we mask out the incorrect predictions, and send\n",
        "    the [16,] vector to the decoder network to reconstruct a [784,] size\n",
        "    image.\n",
        "    Reference: Section 4.1, Fig. 2\n",
        "    '''\n",
        "\n",
        "    def __init__(self, opt):\n",
        "        '''\n",
        "        The decoder network consists of 3 fully connected layers, with\n",
        "        512, 1024, 784 neurons each.\n",
        "        '''\n",
        "        super(DoodleDecoder, self).__init__()\n",
        "        self.opt = opt\n",
        "\n",
        "        self.fc1 = nn.Linear(16, 512)\n",
        "        self.fc2 = nn.Linear(512, 1024)\n",
        "        self.fc3 = nn.Linear(1024, opt.image_size*opt.image_size)\n",
        "\n",
        "    def forward(self, v, target):\n",
        "        '''\n",
        "        Args:\n",
        "            `v`: [batch_size, 10, 16]\n",
        "            `target`: [batch_size, 10]\n",
        "        Return:\n",
        "            `reconstruction`: [batch_size, 784]\n",
        "        We send the outputs of the `DigitCaps` layer, which is a\n",
        "        [batch_size, 10, 16] size tensor into the decoder network, and\n",
        "        reconstruct a [batch_size, 784] size tensor representing the image.\n",
        "        '''\n",
        "        batch_size = target.size(0)\n",
        "\n",
        "        target = target.type(torch.FloatTensor)\n",
        "        # mask: [batch_size, 10, 16]\n",
        "        mask = torch.stack([target for i in range(16)], dim=2)\n",
        "        assert mask.size() == torch.Size([batch_size, opt.n_classes, 16])\n",
        "        if self.opt.use_cuda & torch.cuda.is_available():\n",
        "            mask = mask.cuda()\n",
        "\n",
        "        # v: [bath_size, 10, 16]\n",
        "        v_masked = mask * v\n",
        "        v_masked = torch.sum(v_masked, dim=1)\n",
        "        assert v_masked.size() == torch.Size([batch_size, 16])\n",
        "\n",
        "        # Forward\n",
        "        v = F.relu(self.fc1(v_masked))\n",
        "        v = F.relu(self.fc2(v))\n",
        "        reconstruction = torch.sigmoid(self.fc3(v))\n",
        "\n",
        "        assert reconstruction.size() == torch.Size([batch_size, opt.image_size * opt.image_size])\n",
        "        return reconstruction"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CH6pouLtXQnx"
      },
      "source": [
        "class CapsuleNetwork(nn.Module):\n",
        "    '''Consists of a ReLU Convolution layer, a PrimaryCapsules layer, a DoodleCapsules\n",
        "    layer, and a Decoder layer. Section 4 of the paper.\n",
        "    '''\n",
        "\n",
        "    def __init__(self, opt):\n",
        "        super(CapsuleNetwork, self).__init__()\n",
        "        self.opt = opt\n",
        "\n",
        "        self.Conv1 = nn.Conv2d(in_channels=1, out_channels=256, kernel_size=9)\n",
        "        self.PrimaryCaps = PrimaryCapsules()\n",
        "        self.DigitCaps = DoodleCapsules(opt)\n",
        "\n",
        "        self.Decoder = DoodleDecoder(opt)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        '''\n",
        "        Args:\n",
        "            `x`: [batch_size, 1, 28, 28] MNIST samples\n",
        "        \n",
        "        Return:\n",
        "            `v`: [batch_size, 10, 16] CapsNet outputs, 16D prediction vectors of\n",
        "                10 digit capsules\n",
        "        The dimension transformation procedure of an input tensor in each layer:\n",
        "            0. Input: [batch_size, 1, 28, 28] -->\n",
        "            1. `Conv1` --> [batch_size, 256, 20, 20] --> \n",
        "            2. `PrimaryCaps` --> [batch_size, 8, 6, 6] x 32 capsules --> \n",
        "            3. Flatten, concatenate, squash --> [batch_size, 1152, 8] -->\n",
        "            4. `W_ij`s and `DigitCaps` --> [batch_size, 16, 10] -->\n",
        "            5. Length of 10 capsules --> [batch_size, 10] output probabilities\n",
        "        '''\n",
        "        # Input: [batch_size, 1, 28, 28]\n",
        "        x = x.view(-1, 1, opt.image_size, opt.image_size)\n",
        "        x = F.relu(self.Conv1(x))\n",
        "        # PrimaryCaps input: [batch_size, 256, 20, 20]\n",
        "        u = self.PrimaryCaps(x)\n",
        "        # PrimaryCaps output u: [batch_size, 1152, 8]\n",
        "        v = self.DigitCaps(u)\n",
        "        # DigitCaps output v: [batsh_size, 10, 16]\n",
        "        return v\n",
        "\n",
        "    def marginal_loss(self, v, target, l=0.5):\n",
        "        '''\n",
        "        Args:\n",
        "            `v`: [batch_size, 10, 16]\n",
        "            `target`: [batch_size, 10]\n",
        "            `l`: Scalar, lambda for down-weighing the loss for absent digit classes\n",
        "        Return:\n",
        "            `marginal_loss`: Scalar\n",
        "        \n",
        "        L_c = T_c * max(0, m_plus - norm(v_c)) ^ 2 + lambda * (1 - T_c) * max(0, norm(v_c) - m_minus) ^2\n",
        "        \n",
        "        Reference: Eq.4 in Section 3.\n",
        "        '''\n",
        "        batch_size = v.size(0)\n",
        "\n",
        "        square = v ** 2\n",
        "        square_sum = torch.sum(square, dim=2)\n",
        "        # norm: [batch_size, 10]\n",
        "        norm = torch.sqrt(square_sum)\n",
        "        assert norm.size() == torch.Size([batch_size, opt.n_classes])\n",
        "\n",
        "        # The two T_c in Eq.4\n",
        "        T_c = target.type(torch.FloatTensor)\n",
        "        zeros = Variable(torch.zeros(norm.size()))\n",
        "        # Use GPU if available\n",
        "        if self.opt.use_cuda & torch.cuda.is_available():\n",
        "            zeros = zeros.cuda()\n",
        "            T_c = T_c.cuda()\n",
        "\n",
        "        # Eq.4\n",
        "        marginal_loss = T_c * (torch.max(zeros, 0.9 - norm) ** 2) + \\\n",
        "            (1 - T_c) * l * (torch.max(zeros, norm - 0.1) ** 2)\n",
        "        marginal_loss = torch.sum(marginal_loss)\n",
        "\n",
        "        return marginal_loss\n",
        "\n",
        "    def reconstruction_loss(self, reconstruction, image):\n",
        "        '''\n",
        "        Args:\n",
        "            `reconstruction`: [batch_size, 784] Decoder outputs of images\n",
        "            `image`: [batch_size, 1, 28, 28] MNIST samples\n",
        "        Return:\n",
        "            `reconstruction_loss`: Scalar Variable\n",
        "        The reconstruction loss is measured by a squared differences\n",
        "        between the reconstruction and the original image. \n",
        "        Reference: Section 4.1\n",
        "        '''\n",
        "        batch_size = image.size(0)\n",
        "        # image: [batch_size, 784]\n",
        "        image = image.view(batch_size, -1)\n",
        "        assert image.size() == (batch_size, opt.image_size * opt.image_size)\n",
        "        \n",
        "        # Scalar Variable\n",
        "        reconstruction_loss = torch.sum((reconstruction - image) ** 2)\n",
        "        return reconstruction_loss\n",
        "\n",
        "    def loss(self, v, image, target):\n",
        "        '''\n",
        "        Args:\n",
        "            `v`: [batch_size, 10, 16] CapsNet outputs\n",
        "            `target`: [batch_size, 10] One-hot MNIST labels\n",
        "            `image`: [batch_size, 1, 28, 28] MNIST samples\n",
        "        Return:\n",
        "            `L`: Scalar Variable, total loss\n",
        "            `marginal_loss`: Scalar Variable\n",
        "            `reconstruction_loss`: Scalar Variable\n",
        "        The reconstruction loss is scaled down by 5e-4, serving as a\n",
        "        regularization method.\n",
        "        Reference: Section 4.1\n",
        "        '''\n",
        "        batch_size = image.size(0)\n",
        "\n",
        "        marginal_loss = self.marginal_loss(v, target)\n",
        "\n",
        "        # Get reconstructions from the decoder network\n",
        "        reconstruction = self.Decoder(v, target)\n",
        "        reconstruction_loss = self.reconstruction_loss(reconstruction, image)\n",
        "\n",
        "        # Scalar Variable\n",
        "        loss = (marginal_loss + 0.0005 * reconstruction_loss) / batch_size\n",
        "\n",
        "        return loss, marginal_loss / batch_size, reconstruction_loss / batch_size"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zKPXAofhnYhK"
      },
      "source": [
        "## Training QuickDraw"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QQCM0T0_1d5W"
      },
      "source": [
        "def evaluate(opt, valid_loader, model, type_data, plot=False):\n",
        "    sum_loss = 0\n",
        "    sum_marginal_loss = 0\n",
        "    sum_reconstruction_loss = 0\n",
        "    correct = 0\n",
        "    num_sample = len(valid_loader.dataset)\n",
        "    num_batch = len(valid_loader)\n",
        "\n",
        "    model.eval()\n",
        "    for data, target in valid_loader:\n",
        "        data = data.to(torch.float32)\n",
        "        target = target.to(torch.int64)\n",
        "        batch_size = data.size(0)\n",
        "        assert target.size() == torch.Size([batch_size, opt.n_classes])\n",
        "\n",
        "        with torch.no_grad():\n",
        "            data, target = Variable(data), Variable(target)\n",
        "        if opt.use_cuda & torch.cuda.is_available():\n",
        "            data, target = data.cuda(), target.cuda()\n",
        "\n",
        "        output = model(data)  # (batch_size, n_classes, 16)\n",
        "        loss, marginal_loss, reconstruction_loss = model.loss(output, data, target)\n",
        "        sum_loss += loss.item()\n",
        "        sum_marginal_loss += marginal_loss.item()\n",
        "        sum_reconstruction_loss += reconstruction_loss.item()\n",
        "\n",
        "        norms = torch.sqrt(torch.sum(output**2, dim=2))  # (batch_size, n_classes)\n",
        "        pred = norms.data.max(1, keepdim=True)[1].type(torch.LongTensor)  # (batch_size, 1)\n",
        "        label = target.max(1, keepdim=True)[1].type(torch.LongTensor)  # (batch_size, 1)\n",
        "        correct += pred.eq(label.view_as(pred)).cpu().sum().item()\n",
        "        print(correct)\n",
        "\n",
        "    if plot:\n",
        "        recons = model.Decoder(output, target)\n",
        "        recons = recons.view(batch_size, opt.image_size, opt.image_size)\n",
        "        recons = recons[0].cpu()\n",
        "        if (correct / float(num_sample)) > 0.703: plt.imshow(recons.detach(), cmap = \"gray\")\n",
        "\n",
        "    sum_loss /= num_batch\n",
        "    sum_marginal_loss /= num_batch\n",
        "    sum_reconstruction_loss /= num_batch\n",
        "    print('\\n{} loss: {:.4f}   Marginal loss: {:.4f}   Reconstruction loss: {:.4f}'.format(\n",
        "        type_data, sum_loss, sum_marginal_loss, sum_reconstruction_loss))\n",
        "    print('Accuracy: {}/{} {:.4f}\\n'.format(correct, num_sample,\n",
        "        correct / num_sample))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jcoOvQfb1bVn"
      },
      "source": [
        "def train(opt, train_loader, valid_loader, model):\n",
        "    num_sample = len(train_loader.dataset)\n",
        "    num_batches = len(train_loader)\n",
        "    train_loss_list = []\n",
        "    loss_val = 0.\n",
        "\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=opt.lr)\n",
        "    scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, opt.gamma)\n",
        "    model.train()\n",
        "    evaluate(opt, train_loader, model, 'initial TRAIN', False) \n",
        "    evaluate(opt, valid_loader, model, 'initial VALID', False)\n",
        "\n",
        "    for epoch in range(opt.epochs):\n",
        "        for batch_idx, (data, target) in enumerate(train_loader):\n",
        "            optimizer.zero_grad()\n",
        "            data = data.to(torch.float32)\n",
        "            \n",
        "            batch_size = data.size(0)\n",
        "            target = target.to(torch.int64)\n",
        "            assert target.size() == torch.Size([batch_size, opt.n_classes])\n",
        "\n",
        "            # Use GPU if available\n",
        "            with torch.no_grad():\n",
        "                data, target = Variable(data), Variable(target)\n",
        "            if opt.use_cuda & torch.cuda.is_available():\n",
        "                data, target = data.cuda(), target.cuda()\n",
        "            \n",
        "            output = model(data)\n",
        "            loss, marginal_loss, reconstruction_loss = model.loss(output, data, target)\n",
        "            loss_val = loss.item()\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        if epoch % 5 == 0: \n",
        "            train_loss_list.append(loss_val)\n",
        "        if epoch % 2 == 0:\n",
        "            print('Epoch: {}'.format(epoch))\n",
        "            print('Learning rate: {:.2e}'.format(scheduler.get_last_lr()[0]))\n",
        "            evaluate(opt, train_loader, model, 'TRAIN', False) \n",
        "            evaluate(opt, valid_loader, model, 'VALID', False) \n",
        "        torch.save({\n",
        "            'epoch': epoch,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'loss': loss,\n",
        "            }, 'guide_model.pt')\n",
        "        scheduler.step()\n",
        "    fig = plt.figure()\n",
        "    plt.plot([i for i in range(len(train_loss_list))], train_loss_list, '-')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Holn-KGHnvZH"
      },
      "source": [
        "def get_opts():\n",
        "    parser = argparse.ArgumentParser(description='CapsuleNetwork')\n",
        "    parser.add_argument('-batch_size', type=int, default=32)\n",
        "    parser.add_argument('-lr', type=float, default=1e-6)\n",
        "    parser.add_argument('-epochs', type=int, default=40)\n",
        "    parser.add_argument('-image_size', type=int, default=28)\n",
        "    parser.add_argument('-n_classes', type=int, default=2)\n",
        "    parser.add_argument('-use_cuda', default=True)\n",
        "    parser.add_argument('-gamma', type=float, default=0.95)\n",
        "    parser.add_argument('-r', type=int, default=3)\n",
        "    opt, _ = parser.parse_known_args()\n",
        "    return opt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jp16Ag_f0Q7G"
      },
      "source": [
        "opt = get_opts()\n",
        "model = CapsuleNetwork(opt)\n",
        "if opt.use_cuda & torch.cuda.is_available():\n",
        "    model.cuda()\n",
        "#model.load_state_dict(torch.load('guide_model.pt'))\n",
        "train_loader = loader.train_loader\n",
        "valid_loader = loader.val_loader\n",
        "test_loader = loader.test_loader\n",
        "train(opt, train_loader, valid_loader, model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JNeQe9UX0Z0H"
      },
      "source": [
        "# evaluate(opt, valid_loader, model, 'VALID', True)\n",
        "# test_loader = loader.test_loader\n",
        "# evaluate(opt, test_loader, model, 'TEST', True)\n",
        "\n",
        "for i, (data, target) in enumerate(test_loader):\n",
        "    #print(data[0])\n",
        "    for j in range(12):\n",
        "        plt.figure()\n",
        "        plt.imshow(data[j], cmap = \"gray\")\n",
        "    break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a9QasPoHDSO9"
      },
      "source": [
        "## Training MNIST\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BKAj4LCUAvm-"
      },
      "source": [
        "def get_MNIST_dataloader(opt):\n",
        "    # MNIST Dataset\n",
        "    train_dataset = datasets.MNIST(root='./data/',\n",
        "                                   train=True,\n",
        "                                   transform=transforms.ToTensor(),\n",
        "                                   download=True)\n",
        "\n",
        "    test_dataset = datasets.MNIST(root='./data/',\n",
        "                                  train=False,\n",
        "                                  transform=transforms.ToTensor())\n",
        "\n",
        "    # Data Loader (Input Pipeline)\n",
        "    train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
        "                                               batch_size=opt.batch_size,\n",
        "                                               shuffle=True)\n",
        "\n",
        "    test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
        "                                              batch_size=opt.batch_size,\n",
        "                                              shuffle=True)\n",
        "\n",
        "    return train_loader, test_loader"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7oGMCHfVBH-o"
      },
      "source": [
        "def train_MNIST(opt, train_loader, valid_loader, model):\n",
        "    num_sample = len(train_loader.dataset)\n",
        "    num_batches = len(train_loader)\n",
        "    train_loss_list = []\n",
        "    loss_val = 0.\n",
        "\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=opt.lr)\n",
        "    scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, opt.gamma)\n",
        "    model.train()\n",
        "    for epoch in range(opt.epochs):\n",
        "    \n",
        "        for batch_idx, (data, target) in enumerate(train_loader):\n",
        "            optimizer.zero_grad()\n",
        "            batch_size = data.size(0)\n",
        "            target = torch.one_hot((batch_size, 10), target.view(-1, 1))\n",
        "            assert target.size() == torch.Size([batch_size, opt.n_classes]), target.size()\n",
        "\n",
        "            # Use GPU if available\n",
        "            with torch.no_grad():\n",
        "                data, target = Variable(data), Variable(target)\n",
        "            if opt.use_cuda & torch.cuda.is_available():\n",
        "                data, target = data.cuda(), target.cuda()\n",
        "            \n",
        "            output = model(data)\n",
        "            loss, marginal_loss, reconstruction_loss = model.loss(output, data, target)\n",
        "            loss_val = loss.item()\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        if epoch % 5 == 0: \n",
        "            train_loss_list.append(loss_val)\n",
        "        if epoch % 2 == 0:\n",
        "            print('Epoch: {}'.format(epoch))\n",
        "            print('Learning rate: {:.2e}'.format(scheduler.get_last_lr()[0]))\n",
        "            evaluate_MNIST(opt, train_loader, model, 'TRAIN', False) \n",
        "            evaluate_MNIST(opt, valid_loader, model, 'VALID', False) \n",
        "        torch.save({\n",
        "            'epoch': epoch,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'loss': loss,\n",
        "            }, 'guide_model_mnist.pt')\n",
        "        scheduler.step()\n",
        "    fig = plt.figure()\n",
        "    plt.plot([i for i in range(len(train_loss_list))], train_loss_list, '-')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cjrezNwwBVz-"
      },
      "source": [
        "def evaluate_MNIST(opt, valid_loader, model, type_data, plot=False):\n",
        "    sum_loss = 0\n",
        "    sum_marginal_loss = 0\n",
        "    sum_reconstruction_loss = 0\n",
        "    correct = 0\n",
        "    num_sample = len(valid_loader.dataset)\n",
        "    num_batch = len(valid_loader)\n",
        "\n",
        "    model.eval()\n",
        "    for data, target in valid_loader:\n",
        "        batch_size = data.size(0)\n",
        "        target = torch.one_hot((batch_size, 10), target.view(-1, 1))\n",
        "        assert target.size() == torch.Size([batch_size, opt.n_classes])\n",
        "\n",
        "        with torch.no_grad():\n",
        "            data, target = Variable(data), Variable(target)\n",
        "        if opt.use_cuda & torch.cuda.is_available():\n",
        "            data, target = data.cuda(), target.cuda()\n",
        "\n",
        "        output = model(data)  # (batch_size, n_classes, 16)\n",
        "        loss, marginal_loss, reconstruction_loss = model.loss(output, data, target)\n",
        "        sum_loss += loss.item()\n",
        "        sum_marginal_loss += marginal_loss.item()\n",
        "        sum_reconstruction_loss += reconstruction_loss.item()\n",
        "\n",
        "        norms = torch.sqrt(torch.sum(output**2, dim=2))  # (batch_size, n_classes)\n",
        "        pred = norms.data.max(1, keepdim=True)[1].type(torch.LongTensor)  # (batch_size, 1)\n",
        "        label = target.max(1, keepdim=True)[1].type(torch.LongTensor)  # (batch_size, 1)\n",
        "        correct += pred.eq(label.view_as(pred)).cpu().sum().item()\n",
        "\n",
        "    if plot:\n",
        "        recons = model.Decoder(output, target)\n",
        "        recons = recons.view(batch_size, opt.image_size, opt.image_size)\n",
        "        recons = recons[0].cpu()\n",
        "        if (correct / float(num_sample)) > 0.703: plt.imshow(recons.detach(), cmap = \"gray\")\n",
        "\n",
        "    sum_loss /= num_batch\n",
        "    sum_marginal_loss /= num_batch\n",
        "    sum_reconstruction_loss /= num_batch\n",
        "    print('\\n{} loss: {:.4f}   Marginal loss: {:.4f}   Reconstruction loss: {:.4f}'.format(\n",
        "        type_data, sum_loss, sum_marginal_loss, sum_reconstruction_loss))\n",
        "    print('Accuracy: {}/{} {:.4f}\\n'.format(correct, num_sample,\n",
        "        correct / num_sample))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w6V6HNPADUh4"
      },
      "source": [
        "def get_opts_MNIST():\n",
        "    parser = argparse.ArgumentParser(description='CapsuleNetwork')\n",
        "    parser.add_argument('-batch_size', type=int, default=32)\n",
        "    parser.add_argument('-lr', type=float, default=1e-6)\n",
        "    parser.add_argument('-epochs', type=int, default=20)\n",
        "    parser.add_argument('-image_size', type=int, default=28)\n",
        "    parser.add_argument('-n_classes', type=int, default=10)\n",
        "    parser.add_argument('-use_cuda', default=True)\n",
        "    parser.add_argument('-gamma', type=float, default=0.8)\n",
        "    parser.add_argument('-r', type=int, default=3)\n",
        "    opt, _ = parser.parse_known_args()\n",
        "    return opt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4zXW1i3p1uq8"
      },
      "source": [
        "# opt = get_opts_MNIST()\n",
        "# from torchvision import datasets, transforms\n",
        "\n",
        "# model = CapsuleNetwork(opt)\n",
        "# if opt.use_cuda & torch.cuda.is_available():\n",
        "#     model.cuda()\n",
        "\n",
        "# setattr(torch, 'one_hot', torch_extras.one_hot)\n",
        "# train_loader, valid_loader = get_MNIST_dataloader(opt)\n",
        "# train_MNIST(opt, train_loader, valid_loader, model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jXTaG31KxcXr"
      },
      "source": [
        "evaluate_MNIST(opt, valid_loader, model, 'VALID', True)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}