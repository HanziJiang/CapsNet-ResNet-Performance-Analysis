{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "guide.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PiQpTflFkuqi"
      },
      "source": [
        "# Capsule Neural Network (CapsNet) Implementation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QDxJ_A_0Gnt9"
      },
      "source": [
        "Implementation of the paper [Dynamic Routing Between Capsules](https://arxiv.org/pdf/1710.09829.pdf) by Sara Sabour, Nicholas Frosst, and Geoffrey E. Hinton. Used [jindongwang/Pytorch-CapsuleNet](https://github.com/jindongwang/Pytorch-CapsuleNet) and [laubonghaudoi/CapsNet_guide_PyTorch](https://github.com/laubonghaudoi/CapsNet_guide_PyTorch) to clarify some confusions, and borrowed some code."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_pkx4McDDItG"
      },
      "source": [
        "## Setup PyTorch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "peSdN_P8CElk",
        "outputId": "c2dd9102-19cc-4f0b-ecaa-11d79981bf4e"
      },
      "source": [
        "!pip install torch torchvision\n",
        "!pip install matplotlib\n",
        "!pip install import-ipynb\n",
        "!pip install tqdm\n",
        "!pip install pytorch_extras"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (1.8.1+cu101)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (0.9.1+cu101)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch) (1.19.5)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch) (3.7.4.3)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.7/dist-packages (from torchvision) (7.1.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (3.2.2)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (2.8.1)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (2.4.7)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (1.3.1)\n",
            "Requirement already satisfied: numpy>=1.11 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (1.19.5)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (0.10.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib) (1.15.0)\n",
            "Collecting import-ipynb\n",
            "  Downloading https://files.pythonhosted.org/packages/63/35/495e0021bfdcc924c7cdec4e9fbb87c88dd03b9b9b22419444dc370c8a45/import-ipynb-0.1.3.tar.gz\n",
            "Building wheels for collected packages: import-ipynb\n",
            "  Building wheel for import-ipynb (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for import-ipynb: filename=import_ipynb-0.1.3-cp37-none-any.whl size=2976 sha256=9c53663bf9e21b7226f45d02822697fc05235700fa1338e4dfa564f18ff20bac\n",
            "  Stored in directory: /root/.cache/pip/wheels/b4/7b/e9/a3a6e496115dffdb4e3085d0ae39ffe8a814eacc44bbf494b5\n",
            "Successfully built import-ipynb\n",
            "Installing collected packages: import-ipynb\n",
            "Successfully installed import-ipynb-0.1.3\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (4.41.1)\n",
            "Collecting pytorch_extras\n",
            "  Downloading https://files.pythonhosted.org/packages/66/79/42d7d9a78c27eb897b14790c9759dd9a991f67bc987e9e137527a68db9dc/pytorch-extras-0.1.3.tar.gz\n",
            "Building wheels for collected packages: pytorch-extras\n",
            "  Building wheel for pytorch-extras (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pytorch-extras: filename=pytorch_extras-0.1.3-cp37-none-any.whl size=2833 sha256=eeca9c30935d029abb158c1fed9ec49c06c40eb85ff33da4ef0d86880c9ddb1c\n",
            "  Stored in directory: /root/.cache/pip/wheels/5b/7c/5a/f27d4088adfe722cb280d523a1ed9eeb33be11b8d3a653292a\n",
            "Successfully built pytorch-extras\n",
            "Installing collected packages: pytorch-extras\n",
            "Successfully installed pytorch-extras-0.1.3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YMdBumXa75tw",
        "outputId": "e3701eb9-5242-49cf-ad41-c371d279d6f5"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"mnt\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at mnt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wlYHtO737_dA",
        "outputId": "247e80e8-af51-4ca3-c1b8-351f89de42d4"
      },
      "source": [
        "%cd \"mnt/My Drive\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/mnt/My Drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rsZEHMrIEVz6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "38444bfa-7c82-4c1b-ff9e-d14f4754f94b"
      },
      "source": [
        "import torch\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "import torch_extras\n",
        "import torch.nn as nn\n",
        "import torchvision.utils as tv_utils\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "import import_ipynb\n",
        "import load_DrawData as loader\n",
        "import numpy as np\n",
        "import argparse\n",
        "from tqdm import tqdm\n",
        "%matplotlib inline\n",
        "# %mkdir -p /content/project/\n",
        "# %cd /content/project/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "importing Jupyter notebook from load_DrawData.ipynb\n",
            "Collecting ndjson\n",
            "  Downloading https://files.pythonhosted.org/packages/70/c9/04ba0056011ba96a58163ebfd666d8385300bd12da1afe661a5a147758d7/ndjson-0.3.1-py2.py3-none-any.whl\n",
            "Installing collected packages: ndjson\n",
            "Successfully installed ndjson-0.3.1\n",
            "Collecting cairocffi\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/84/ca/0bffed5116d21251469df200448667e90acaa5131edea869b44a3fbc73d0/cairocffi-1.2.0.tar.gz (70kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 6.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: cffi>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from cairocffi) (1.14.5)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.1.0->cairocffi) (2.20)\n",
            "Building wheels for collected packages: cairocffi\n",
            "  Building wheel for cairocffi (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for cairocffi: filename=cairocffi-1.2.0-cp37-none-any.whl size=89548 sha256=e7e53d7c1a347852b562640a14204e0da6c772869bce5d1d6a2262eed4535b25\n",
            "  Stored in directory: /root/.cache/pip/wheels/40/76/48/f1effadceea83b32e7d957dd0f92db4db8b537d7b72b4ef374\n",
            "Successfully built cairocffi\n",
            "Installing collected packages: cairocffi\n",
            "Successfully installed cairocffi-1.2.0\n",
            "4200\n",
            "2800\n",
            "total training samples: 4200\n",
            "total validatoin samples: 1400\n",
            "total test samples: 1400\n",
            "mkdir: cannot create directory ‘quickDrawData’: File exists\n",
            "gs://quickdraw_dataset/full/simplified/t-shirt.ndjson\n",
            "Copying gs://quickdraw_dataset/full/simplified/t-shirt.ndjson...\n",
            "\\ [1/1 files][ 41.3 MiB/ 41.3 MiB] 100% Done                                    \n",
            "Operation completed over 1 objects/41.3 MiB.                                     \n",
            "gs://quickdraw_dataset/full/simplified/rabbit.ndjson\n",
            "Copying gs://quickdraw_dataset/full/simplified/rabbit.ndjson...\n",
            "| [1/1 files][ 95.0 MiB/ 95.0 MiB] 100% Done                                    \n",
            "Operation completed over 1 objects/95.0 MiB.                                     \n",
            "adding from:  t-shirt\n",
            "adding from:  rabbit\n",
            "building  train  dataset\n",
            "adding from:  t-shirt\n",
            "adding from:  rabbit\n",
            "building  val  dataset\n",
            "adding from:  t-shirt\n",
            "adding from:  rabbit\n",
            "building test dataset\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XcYm7y8rECqg"
      },
      "source": [
        "## CapsNet Modules"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VdGH3V4jJPmQ"
      },
      "source": [
        "class PrimaryCapsules(nn.Module):\n",
        "    '''\n",
        "    The `PrimaryCaps` layer consists of 32 capsule units. Each unit takes\n",
        "    the output of the `Conv1` layer, which is a `[256, 20, 20]` feature\n",
        "    tensor (omitting `batch_size`), and performs a 2D convolution with 8\n",
        "    output channels, kernel size 9 and stride 2, thus outputing a [8, 6, 6]\n",
        "    tensor. In other words, you can see these 32 capsules as 32 paralleled 2D\n",
        "    convolutional layers. Then we concatenate these 32 capsules' outputs and\n",
        "    flatten them into a tensor of size `[1152, 8]`, representing 1152 8D\n",
        "    vectors, and send it to the next layer `DigitCaps`.\n",
        "    As indicated in Section 4, Page 4 in the paper, *One can see PrimaryCaps\n",
        "    as a Convolution layer with Eq.1 as its block non-linearity.*, outputs of\n",
        "    the `PrimaryCaps` layer are squashed before being passed to the next layer.\n",
        "    Reference: Section 4, Fig. 1\n",
        "    '''\n",
        "\n",
        "    def __init__(self):\n",
        "        '''\n",
        "        We build 8 capsule units in the `PrimaryCaps` layer, each can be\n",
        "        seen as a 2D convolution layer.\n",
        "        '''\n",
        "        super(PrimaryCapsules, self).__init__()\n",
        "\n",
        "        self.capsules = nn.ModuleList([\n",
        "            nn.Conv2d(in_channels=256,\n",
        "                      out_channels=8,\n",
        "                      kernel_size=9,\n",
        "                      stride=2)\n",
        "            for i in range(32)\n",
        "        ])\n",
        "\n",
        "    def forward(self, x):\n",
        "        '''\n",
        "        Each capsule outputs a [batch_size, 8, 6, 6] tensor, we need to\n",
        "        flatten and concatenate them into a [batch_size, 8, 6*6, 32] size\n",
        "        tensor and flatten and transpose into `u` [batch_size, 1152, 8], \n",
        "        where each [batch_size, 1152, 1] size tensor is the `u_i` in Eq.2. \n",
        "        #### Dimension transformation in this layer(ignoring `batch_size`):\n",
        "        [256, 20, 20] --> [8, 6, 6] x 32 capsules --> [1152, 8]\n",
        "        Note: `u_i` is one [1, 8] in the final [1152, 8] output, thus there are\n",
        "        1152 `u_i`s.\n",
        "        '''\n",
        "        batch_size = x.size(0)\n",
        "\n",
        "        u = []\n",
        "        for i in range(32):\n",
        "            # Input: [batch_size, 256, 20, 20]\n",
        "            assert x.data.size() == torch.Size([batch_size, 256, 20, 20])\n",
        "\n",
        "            u_i = self.capsules[i](x)\n",
        "            assert u_i.size() == torch.Size([batch_size, 8, 6, 6])\n",
        "            # u_i: [batch_size, 8, 6, 6]\n",
        "            u_i = u_i.view(batch_size, 8, -1, 1)\n",
        "            # u_i: [batch_size, 8, 36]\n",
        "            u.append(u_i)\n",
        "\n",
        "        # u: [batch_size, 8, 36, 1] x 32\n",
        "        u = torch.cat(u, dim=3)\n",
        "        # u: [batch_size, 8, 36, 32]\n",
        "        u = u.view(batch_size, 8, -1)\n",
        "        # u: [batch_size, 8, 1152]\n",
        "        u = torch.transpose(u, 1, 2)\n",
        "        # u: [batch_size, 1152, 8]\n",
        "        assert u.data.size() == torch.Size([batch_size, 1152, 8])\n",
        "\n",
        "        # Squash before output\n",
        "        u_squashed = self.squash(u)\n",
        "\n",
        "        return u_squashed\n",
        "\n",
        "    def squash(self, u):\n",
        "        '''\n",
        "        Args:\n",
        "            `u`: [batch_size, 1152, 8]\n",
        "        Return:\n",
        "            `u_squashed`: [batch_size, 1152, 8]\n",
        "        In CapsNet, we use the squash function after the output of both \n",
        "        capsule layers. Squash functions can be seen as activating functions\n",
        "        like sigmoid, but for capsule layers rather than traditional fully\n",
        "        connected layers, as they squash vectors instead of scalars.\n",
        "        v_j = (norm(s_j) ^ 2 / (1 + norm(s_j) ^ 2)) * (s_j / norm(s_j))\n",
        "        Reference: Eq.1 in Section 2.\n",
        "        '''\n",
        "        batch_size = u.size(0)\n",
        "\n",
        "        # u: [batch_size, 1152, 8]\n",
        "        square = u ** 2\n",
        "\n",
        "        # square_sum for u: [batch_size, 1152]\n",
        "        square_sum = torch.sum(square, dim=2)\n",
        "\n",
        "        # norm for u: [batch_size, 1152]\n",
        "        norm = torch.sqrt(square_sum)\n",
        "\n",
        "        # factor for u: [batch_size, 1152]\n",
        "        factor = norm ** 2 / (norm * (1 + norm ** 2))\n",
        "\n",
        "        # u_squashed: [batch_size, 1152, 8]\n",
        "        u_squashed = factor.unsqueeze(2) * u\n",
        "        assert u_squashed.size() == torch.Size([batch_size, 1152, 8])\n",
        "\n",
        "        return u_squashed"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xz1tFdzkaHyP"
      },
      "source": [
        "class DoodleCapsules(nn.Module):\n",
        "    '''\n",
        "    The `DigitCaps` layer consists of 10 16D capsules. Compared to the traditional\n",
        "    scalar output neurons in fully connected networks(FCN), the `DigitCaps` layer\n",
        "    can be seen as an FCN with ten 16-dimensional output neurons, which we call\n",
        "    these neurons \"capsules\".\n",
        "    In this layer, we take the input `[1152, 8]` tensor `u` as 1152 [8,] vectors\n",
        "    `u_i`, each `u_i` is a 8D output of the capsules from `PrimaryCaps` (see Eq.2\n",
        "    in Section 2, Page 2) and sent to the 10 capsules. For each capsule, the tensor\n",
        "    is first transformed by `W_ij`s into [1152, 16] size. Then we perform the Dynamic\n",
        "    Routing algorithm to get the output `v_j` of size [16,]. As there are 10 capsules,\n",
        "    the final output is [16, 10] size.\n",
        "    #### Dimension transformation in this layer(ignoring `batch_size`):\n",
        "    [1152, 8] --> [1152, 16] --> [1, 16] x 10 capsules --> [10, 16] output\n",
        "    Note that in our codes we have vectorized these computations, so the dimensions\n",
        "    above are just for understanding, actual dimensions of tensors are different.\n",
        "    '''\n",
        "\n",
        "    def __init__(self, opt):\n",
        "        '''\n",
        "        There is only one parameter in this layer, `W` [1, 1152, 10, 16, 8], where\n",
        "        every [8, 16] is a weight matrix W_ij in Eq.2, that is, there are 11520\n",
        "        `W_ij`s in total.\n",
        "        The the coupling coefficients `b` [64, 1152, 10, 1] is a temporary variable which\n",
        "        does NOT belong to the layer's parameters. In other words, `b` is not updated\n",
        "        by gradient back-propagations. Instead, we update `b` by Dynamic Routing\n",
        "        in every forward propagation. See the docstring of `self.forward` for details.\n",
        "        '''\n",
        "        super(DoodleCapsules, self).__init__()\n",
        "        self.opt = opt\n",
        "\n",
        "        self.W = nn.Parameter(torch.randn(1, 1152, opt.n_classes, 8, 16))\n",
        "\n",
        "    def forward(self, u):\n",
        "        '''\n",
        "        Args:\n",
        "            `u`: [batch_size, 1152, 8]\n",
        "        Return:\n",
        "            `v`: [batch_size, 10, 16]\n",
        "        In this layer, we vectorize our computations by calling `W` and using\n",
        "        `torch.matmul()`. Thus the full computaion steps are as follows.\n",
        "            1. Expand `W` into batches and compute `u_hat` (Eq.2)\n",
        "            2. Line 2: Initialize `b` into zeros\n",
        "            3. Line 3: Start Routing for `r` iterations:\n",
        "                1. Line 4: c = softmax(b)\n",
        "                2. Line 5: s = sum(c * u_hat)\n",
        "                3. Line 6: v = squash(s)\n",
        "                4. Line 7: b += u_hat * v\n",
        "        The coupling coefficients `b` can be seen as a kind of attention matrix\n",
        "        in the attentional sequence-to-sequence networks, which is widely used in\n",
        "        Neural Machine Translation systems. For tutorials on  attentional seq2seq\n",
        "        models, see https://arxiv.org/abs/1703.01619 or\n",
        "        http://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html\n",
        "        Reference: Section 2, Procedure 1\n",
        "        '''\n",
        "        batch_size = u.size(0)\n",
        "\n",
        "        # First, we need to expand the dimensions of `W` and `u` to compute `u_hat`\n",
        "        assert u.size() == torch.Size([batch_size, 1152, 8])\n",
        "        # u: [batch_size, 1152, 1, 1, 8]\n",
        "        u = torch.unsqueeze(u, dim=2)\n",
        "        u = torch.unsqueeze(u, dim=2)\n",
        "        # Now we compute u_hat in Eq.2\n",
        "        # u_hat: [batch_size, 1152, 10, 16]\n",
        "        u_hat = torch.matmul(u, self.W).squeeze()\n",
        "\n",
        "        # Line 2: Initialize b into zeros\n",
        "        # b: [batch_size, 1152, 10, 1]\n",
        "        b = Variable(torch.zeros(batch_size, 1152, opt.n_classes, 1))\n",
        "        if self.opt.use_cuda & torch.cuda.is_available():\n",
        "            b = b.cuda()\n",
        "\n",
        "        # Start Routing\n",
        "        for r in range(self.opt.r):\n",
        "            # Line 4: c_i = softmax(b_i)\n",
        "            # c: [b, 1152, 10, 1]\n",
        "            c = F.softmax(b, dim=2)\n",
        "            assert c.size() == torch.Size([batch_size, 1152, opt.n_classes, 1])\n",
        "\n",
        "            # Line 5: s_j = sum_i(c_ij * u_hat_j|i)\n",
        "            # u_hat: [batch_size, 1152, 10, 16]\n",
        "            # s: [batch_size, 10, 16]\n",
        "            s = torch.sum(u_hat * c, dim=1)\n",
        "\n",
        "            # Line 6: v_j = squash(s_j)\n",
        "            # v: [batch_size, 10, 16]\n",
        "            v = self.squash(s)\n",
        "            assert v.size() == torch.Size([batch_size, opt.n_classes, 16])\n",
        "\n",
        "            # Line 7: b_ij += u_hat * v_j\n",
        "            # u_hat: [batch_size, 1152, 10, 16]\n",
        "            # v: [batch_size, 10, 16]\n",
        "            # a: [batch_size, 10, 1152, 16]\n",
        "            a = u_hat * v.unsqueeze(1)\n",
        "            # b: [batch_size, 1152, 10, 1]\n",
        "            b = b + torch.sum(a, dim=3, keepdim=True)\n",
        "\n",
        "        return v\n",
        "\n",
        "    def squash(self, s):\n",
        "        '''\n",
        "        Args:\n",
        "            `s`: [batch_size, 10, 16]\n",
        "        v_j = (norm(s_j) ^ 2 / (1 + norm(s_j) ^ 2)) * (s_j / norm(s_j))\n",
        "        Reference: Eq.1 in Section 2.\n",
        "        '''\n",
        "        batch_size = s.size(0)\n",
        "\n",
        "        # s: [batch_size, 10, 16]\n",
        "        square = s ** 2\n",
        "\n",
        "        # square_sum for v: [batch_size, 10]\n",
        "        square_sum = torch.sum(square, dim=2)\n",
        "\n",
        "        # norm for v: [batch_size, 10]\n",
        "        norm = torch.sqrt(square_sum)\n",
        "\n",
        "        # factor for v: [batch_size, 10]\n",
        "        factor = norm ** 2 / (norm * (1 + norm ** 2))\n",
        "\n",
        "        # v: [batch_size, 10, 16]\n",
        "        v = factor.unsqueeze(2) * s\n",
        "        assert v.size() == torch.Size([batch_size, opt.n_classes, 16])\n",
        "\n",
        "        return v"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q-k05aXUmDs8"
      },
      "source": [
        "class DoodleDecoder(nn.Module):\n",
        "    '''\n",
        "    The decoder network consists of 3 fully connected layers. For each\n",
        "    [10, 16] output, we mask out the incorrect predictions, and send\n",
        "    the [16,] vector to the decoder network to reconstruct a [784,] size\n",
        "    image.\n",
        "    Reference: Section 4.1, Fig. 2\n",
        "    '''\n",
        "\n",
        "    def __init__(self, opt):\n",
        "        '''\n",
        "        The decoder network consists of 3 fully connected layers, with\n",
        "        512, 1024, 784 neurons each.\n",
        "        '''\n",
        "        super(DoodleDecoder, self).__init__()\n",
        "        self.opt = opt\n",
        "\n",
        "        self.fc1 = nn.Linear(16, 512)\n",
        "        self.fc2 = nn.Linear(512, 1024)\n",
        "        self.fc3 = nn.Linear(1024, opt.image_size*opt.image_size)\n",
        "\n",
        "    def forward(self, v, target):\n",
        "        '''\n",
        "        Args:\n",
        "            `v`: [batch_size, 10, 16]\n",
        "            `target`: [batch_size, 10]\n",
        "        Return:\n",
        "            `reconstruction`: [batch_size, 784]\n",
        "        We send the outputs of the `DigitCaps` layer, which is a\n",
        "        [batch_size, 10, 16] size tensor into the decoder network, and\n",
        "        reconstruct a [batch_size, 784] size tensor representing the image.\n",
        "        '''\n",
        "        batch_size = target.size(0)\n",
        "\n",
        "        target = target.type(torch.FloatTensor)\n",
        "        # mask: [batch_size, 10, 16]\n",
        "        mask = torch.stack([target for i in range(16)], dim=2)\n",
        "        assert mask.size() == torch.Size([batch_size, opt.n_classes, 16])\n",
        "        if self.opt.use_cuda & torch.cuda.is_available():\n",
        "            mask = mask.cuda()\n",
        "\n",
        "        # v: [bath_size, 10, 16]\n",
        "        v_masked = mask * v\n",
        "        v_masked = torch.sum(v_masked, dim=1)\n",
        "        assert v_masked.size() == torch.Size([batch_size, 16])\n",
        "\n",
        "        # Forward\n",
        "        v = F.relu(self.fc1(v_masked))\n",
        "        v = F.relu(self.fc2(v))\n",
        "        reconstruction = torch.sigmoid(self.fc3(v))\n",
        "\n",
        "        assert reconstruction.size() == torch.Size([batch_size, opt.image_size * opt.image_size])\n",
        "        return reconstruction"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CH6pouLtXQnx"
      },
      "source": [
        "class CapsuleNetwork(nn.Module):\n",
        "    '''Consists of a ReLU Convolution layer, a PrimaryCapsules layer, a DoodleCapsules\n",
        "    layer, and a Decoder layer. Section 4 of the paper.\n",
        "    '''\n",
        "\n",
        "    def __init__(self, opt):\n",
        "        super(CapsuleNetwork, self).__init__()\n",
        "        self.opt = opt\n",
        "\n",
        "        self.Conv1 = nn.Conv2d(in_channels=1, out_channels=256, kernel_size=9)\n",
        "        self.PrimaryCaps = PrimaryCapsules()\n",
        "        self.DigitCaps = DoodleCapsules(opt)\n",
        "\n",
        "        self.Decoder = DoodleDecoder(opt)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        '''\n",
        "        Args:\n",
        "            `x`: [batch_size, 1, 28, 28] MNIST samples\n",
        "        \n",
        "        Return:\n",
        "            `v`: [batch_size, 10, 16] CapsNet outputs, 16D prediction vectors of\n",
        "                10 digit capsules\n",
        "        The dimension transformation procedure of an input tensor in each layer:\n",
        "            0. Input: [batch_size, 1, 28, 28] -->\n",
        "            1. `Conv1` --> [batch_size, 256, 20, 20] --> \n",
        "            2. `PrimaryCaps` --> [batch_size, 8, 6, 6] x 32 capsules --> \n",
        "            3. Flatten, concatenate, squash --> [batch_size, 1152, 8] -->\n",
        "            4. `W_ij`s and `DigitCaps` --> [batch_size, 16, 10] -->\n",
        "            5. Length of 10 capsules --> [batch_size, 10] output probabilities\n",
        "        '''\n",
        "        # Input: [batch_size, 1, 28, 28]\n",
        "        x = x.view(-1, 1, opt.image_size, opt.image_size)\n",
        "        x = F.relu(self.Conv1(x))\n",
        "        # PrimaryCaps input: [batch_size, 256, 20, 20]\n",
        "        u = self.PrimaryCaps(x)\n",
        "        # PrimaryCaps output u: [batch_size, 1152, 8]\n",
        "        v = self.DigitCaps(u)\n",
        "        # DigitCaps output v: [batsh_size, 10, 16]\n",
        "        return v\n",
        "\n",
        "    def marginal_loss(self, v, target, l=0.5):\n",
        "        '''\n",
        "        Args:\n",
        "            `v`: [batch_size, 10, 16]\n",
        "            `target`: [batch_size, 10]\n",
        "            `l`: Scalar, lambda for down-weighing the loss for absent digit classes\n",
        "        Return:\n",
        "            `marginal_loss`: Scalar\n",
        "        \n",
        "        L_c = T_c * max(0, m_plus - norm(v_c)) ^ 2 + lambda * (1 - T_c) * max(0, norm(v_c) - m_minus) ^2\n",
        "        \n",
        "        Reference: Eq.4 in Section 3.\n",
        "        '''\n",
        "        batch_size = v.size(0)\n",
        "\n",
        "        square = v ** 2\n",
        "        square_sum = torch.sum(square, dim=2)\n",
        "        # norm: [batch_size, 10]\n",
        "        norm = torch.sqrt(square_sum)\n",
        "        assert norm.size() == torch.Size([batch_size, opt.n_classes])\n",
        "\n",
        "        # The two T_c in Eq.4\n",
        "        T_c = target.type(torch.FloatTensor)\n",
        "        zeros = Variable(torch.zeros(norm.size()))\n",
        "        # Use GPU if available\n",
        "        if self.opt.use_cuda & torch.cuda.is_available():\n",
        "            zeros = zeros.cuda()\n",
        "            T_c = T_c.cuda()\n",
        "\n",
        "        # Eq.4\n",
        "        marginal_loss = T_c * (torch.max(zeros, 0.9 - norm) ** 2) + \\\n",
        "            (1 - T_c) * l * (torch.max(zeros, norm - 0.1) ** 2)\n",
        "        marginal_loss = torch.sum(marginal_loss)\n",
        "\n",
        "        return marginal_loss\n",
        "\n",
        "    def reconstruction_loss(self, reconstruction, image):\n",
        "        '''\n",
        "        Args:\n",
        "            `reconstruction`: [batch_size, 784] Decoder outputs of images\n",
        "            `image`: [batch_size, 1, 28, 28] MNIST samples\n",
        "        Return:\n",
        "            `reconstruction_loss`: Scalar Variable\n",
        "        The reconstruction loss is measured by a squared differences\n",
        "        between the reconstruction and the original image. \n",
        "        Reference: Section 4.1\n",
        "        '''\n",
        "        batch_size = image.size(0)\n",
        "        # image: [batch_size, 784]\n",
        "        image = image.view(batch_size, -1)\n",
        "        assert image.size() == (batch_size, opt.image_size * opt.image_size)\n",
        "        \n",
        "        # Scalar Variable\n",
        "        reconstruction_loss = torch.sum((reconstruction - image) ** 2)\n",
        "        return reconstruction_loss\n",
        "\n",
        "    def loss(self, v, image, target):\n",
        "        '''\n",
        "        Args:\n",
        "            `v`: [batch_size, 10, 16] CapsNet outputs\n",
        "            `target`: [batch_size, 10] One-hot MNIST labels\n",
        "            `image`: [batch_size, 1, 28, 28] MNIST samples\n",
        "        Return:\n",
        "            `L`: Scalar Variable, total loss\n",
        "            `marginal_loss`: Scalar Variable\n",
        "            `reconstruction_loss`: Scalar Variable\n",
        "        The reconstruction loss is scaled down by 5e-4, serving as a\n",
        "        regularization method.\n",
        "        Reference: Section 4.1\n",
        "        '''\n",
        "        batch_size = image.size(0)\n",
        "\n",
        "        marginal_loss = self.marginal_loss(v, target)\n",
        "\n",
        "        # Get reconstructions from the decoder network\n",
        "        reconstruction = self.Decoder(v, target)\n",
        "        reconstruction_loss = self.reconstruction_loss(reconstruction, image)\n",
        "\n",
        "        # Scalar Variable\n",
        "        loss = (marginal_loss + 0.0005 * reconstruction_loss) / batch_size\n",
        "\n",
        "        return loss, marginal_loss / batch_size, reconstruction_loss / batch_size"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zKPXAofhnYhK"
      },
      "source": [
        "## Training QuickDraw"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QQCM0T0_1d5W"
      },
      "source": [
        "def evaluate(opt, valid_loader, model, type_data, plot=False):\n",
        "    sum_loss = 0\n",
        "    sum_marginal_loss = 0\n",
        "    sum_reconstruction_loss = 0\n",
        "    correct = 0\n",
        "    num_sample = len(valid_loader.dataset)\n",
        "    num_batch = len(valid_loader)\n",
        "\n",
        "    model.eval()\n",
        "    for data, target in valid_loader:\n",
        "        data = data.to(torch.float32)\n",
        "        target = target.to(torch.int64)\n",
        "        batch_size = data.size(0)\n",
        "        assert target.size() == torch.Size([batch_size, opt.n_classes])\n",
        "\n",
        "        with torch.no_grad():\n",
        "            data, target = Variable(data), Variable(target)\n",
        "        if opt.use_cuda & torch.cuda.is_available():\n",
        "            data, target = data.cuda(), target.cuda()\n",
        "\n",
        "        output = model(data)  # (batch_size, n_classes, 16)\n",
        "        loss, marginal_loss, reconstruction_loss = model.loss(output, data, target)\n",
        "        sum_loss += loss.item()\n",
        "        sum_marginal_loss += marginal_loss.item()\n",
        "        sum_reconstruction_loss += reconstruction_loss.item()\n",
        "\n",
        "        norms = torch.sqrt(torch.sum(output**2, dim=2))  # (batch_size, n_classes)\n",
        "        pred = norms.data.max(1, keepdim=True)[1].type(torch.LongTensor)  # (batch_size, 1)\n",
        "        label = target.max(1, keepdim=True)[1].type(torch.LongTensor)  # (batch_size, 1)\n",
        "        correct += pred.eq(label.view_as(pred)).cpu().sum().item()\n",
        "\n",
        "    if plot:\n",
        "        recons = model.Decoder(output, target)\n",
        "        recons = recons.view(batch_size, opt.image_size, opt.image_size)\n",
        "        recons = recons[0].cpu()\n",
        "        if (correct / float(num_sample)) > 0.703: plt.imshow(recons.detach(), cmap = \"gray\")\n",
        "\n",
        "    sum_loss /= num_batch\n",
        "    sum_marginal_loss /= num_batch\n",
        "    sum_reconstruction_loss /= num_batch\n",
        "    print('\\n{} loss: {:.4f}   Marginal loss: {:.4f}   Reconstruction loss: {:.4f}'.format(\n",
        "        type_data, sum_loss, sum_marginal_loss, sum_reconstruction_loss))\n",
        "    print('Accuracy: {}/{} {:.4f}\\n'.format(correct, num_sample,\n",
        "        correct / num_sample))"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jcoOvQfb1bVn"
      },
      "source": [
        "def train(opt, train_loader, valid_loader, model):\n",
        "    num_sample = len(train_loader.dataset)\n",
        "    num_batches = len(train_loader)\n",
        "    train_loss_list = []\n",
        "    loss_val = 0.\n",
        "\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=opt.lr)\n",
        "    scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, opt.gamma)\n",
        "    model.train()\n",
        "    evaluate(opt, train_loader, model, 'initial TRAIN', False) \n",
        "    evaluate(opt, valid_loader, model, 'initial VALID', False)\n",
        "\n",
        "    for epoch in range(opt.epochs):\n",
        "    \n",
        "        for batch_idx, (data, target) in enumerate(train_loader):\n",
        "            optimizer.zero_grad()\n",
        "            data = data.to(torch.float32)\n",
        "            \n",
        "            batch_size = data.size(0)\n",
        "            target = target.to(torch.int64)\n",
        "            assert target.size() == torch.Size([batch_size, opt.n_classes])\n",
        "\n",
        "            # Use GPU if available\n",
        "            with torch.no_grad():\n",
        "                data, target = Variable(data), Variable(target)\n",
        "            if opt.use_cuda & torch.cuda.is_available():\n",
        "                data, target = data.cuda(), target.cuda()\n",
        "            \n",
        "            output = model(data)\n",
        "            loss, marginal_loss, reconstruction_loss = model.loss(output, data, target)\n",
        "            loss_val = loss.item()\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        if epoch % 5 == 0: \n",
        "            train_loss_list.append(loss_val)\n",
        "        if epoch % 2 == 0:\n",
        "            print('Epoch: {}'.format(epoch))\n",
        "            print('Learning rate: {:.2e}'.format(scheduler.get_last_lr()[0]))\n",
        "            evaluate(opt, train_loader, model, 'TRAIN', False) \n",
        "            evaluate(opt, valid_loader, model, 'VALID', False) \n",
        "        torch.save({\n",
        "            'epoch': epoch,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'loss': loss,\n",
        "            }, 'guide_model.pt')\n",
        "        scheduler.step()\n",
        "    fig = plt.figure()\n",
        "    plt.plot([i for i in range(len(train_loss_list))], train_loss_list, '-')"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Holn-KGHnvZH"
      },
      "source": [
        "def get_opts():\n",
        "    parser = argparse.ArgumentParser(description='CapsuleNetwork')\n",
        "    parser.add_argument('-batch_size', type=int, default=32)\n",
        "    parser.add_argument('-lr', type=float, default=1e-6)\n",
        "    parser.add_argument('-epochs', type=int, default=40)\n",
        "    parser.add_argument('-image_size', type=int, default=28)\n",
        "    parser.add_argument('-n_classes', type=int, default=2)\n",
        "    parser.add_argument('-use_cuda', default=True)\n",
        "    parser.add_argument('-gamma', type=float, default=0.95)\n",
        "    parser.add_argument('-r', type=int, default=3)\n",
        "    opt, _ = parser.parse_known_args()\n",
        "    return opt"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "jp16Ag_f0Q7G",
        "outputId": "0005d59b-b401-46f2-fc09-109002f125dc"
      },
      "source": [
        "opt = get_opts()\n",
        "model = CapsuleNetwork(opt)\n",
        "if opt.use_cuda & torch.cuda.is_available():\n",
        "    model.cuda()\n",
        "#model.load_state_dict(torch.load('guide_model.pt'))\n",
        "train_loader = loader.train_loader\n",
        "valid_loader = loader.val_loader\n",
        "\n",
        "train(opt, train_loader, valid_loader, model)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "initial TRAIN loss: 3554.2808   Marginal loss: 0.4050   Reconstruction loss: 7107751.1477\n",
            "Accuracy: 2073/4200 0.4936\n",
            "\n",
            "\n",
            "initial VALID loss: 3532.1978   Marginal loss: 0.4050   Reconstruction loss: 7063585.2273\n",
            "Accuracy: 728/1400 0.5200\n",
            "\n",
            "Epoch: 0\n",
            "Learning rate: 1.00e-06\n",
            "\n",
            "TRAIN loss: 3554.1690   Marginal loss: 0.4050   Reconstruction loss: 7107527.5985\n",
            "Accuracy: 3010/4200 0.7167\n",
            "\n",
            "\n",
            "VALID loss: 3532.1116   Marginal loss: 0.4050   Reconstruction loss: 7063412.8409\n",
            "Accuracy: 1010/1400 0.7214\n",
            "\n",
            "Epoch: 2\n",
            "Learning rate: 9.02e-07\n",
            "\n",
            "TRAIN loss: 3552.3481   Marginal loss: 0.4050   Reconstruction loss: 7103885.8523\n",
            "Accuracy: 3234/4200 0.7700\n",
            "\n",
            "\n",
            "VALID loss: 3531.9830   Marginal loss: 0.4050   Reconstruction loss: 7063155.6705\n",
            "Accuracy: 1060/1400 0.7571\n",
            "\n",
            "Epoch: 4\n",
            "Learning rate: 8.15e-07\n",
            "\n",
            "TRAIN loss: 3552.1595   Marginal loss: 0.4050   Reconstruction loss: 7103508.5871\n",
            "Accuracy: 3384/4200 0.8057\n",
            "\n",
            "\n",
            "VALID loss: 3531.8239   Marginal loss: 0.4050   Reconstruction loss: 7062837.3409\n",
            "Accuracy: 1110/1400 0.7929\n",
            "\n",
            "Epoch: 6\n",
            "Learning rate: 7.35e-07\n",
            "\n",
            "TRAIN loss: 3555.3861   Marginal loss: 0.4050   Reconstruction loss: 7109961.8674\n",
            "Accuracy: 3382/4200 0.8052\n",
            "\n",
            "\n",
            "VALID loss: 3531.6477   Marginal loss: 0.4050   Reconstruction loss: 7062485.0341\n",
            "Accuracy: 1121/1400 0.8007\n",
            "\n",
            "Epoch: 8\n",
            "Learning rate: 6.63e-07\n",
            "\n",
            "TRAIN loss: 3553.5195   Marginal loss: 0.4050   Reconstruction loss: 7106228.5720\n",
            "Accuracy: 3396/4200 0.8086\n",
            "\n",
            "\n",
            "VALID loss: 3531.4672   Marginal loss: 0.4050   Reconstruction loss: 7062124.0568\n",
            "Accuracy: 1120/1400 0.8000\n",
            "\n",
            "Epoch: 10\n",
            "Learning rate: 5.99e-07\n",
            "\n",
            "TRAIN loss: 3551.3357   Marginal loss: 0.4050   Reconstruction loss: 7101861.0152\n",
            "Accuracy: 3401/4200 0.8098\n",
            "\n",
            "\n",
            "VALID loss: 3531.2855   Marginal loss: 0.4050   Reconstruction loss: 7061760.5455\n",
            "Accuracy: 1115/1400 0.7964\n",
            "\n",
            "Epoch: 12\n",
            "Learning rate: 5.40e-07\n",
            "\n",
            "TRAIN loss: 3549.7664   Marginal loss: 0.4050   Reconstruction loss: 7098722.4621\n",
            "Accuracy: 3438/4200 0.8186\n",
            "\n",
            "\n",
            "VALID loss: 3531.1053   Marginal loss: 0.4050   Reconstruction loss: 7061400.1477\n",
            "Accuracy: 1143/1400 0.8164\n",
            "\n",
            "Epoch: 14\n",
            "Learning rate: 4.88e-07\n",
            "\n",
            "TRAIN loss: 3554.2522   Marginal loss: 0.4050   Reconstruction loss: 7107693.9167\n",
            "Accuracy: 3458/4200 0.8233\n",
            "\n",
            "\n",
            "VALID loss: 3530.9305   Marginal loss: 0.4050   Reconstruction loss: 7061050.6705\n",
            "Accuracy: 1145/1400 0.8179\n",
            "\n",
            "Epoch: 16\n",
            "Learning rate: 4.40e-07\n",
            "\n",
            "TRAIN loss: 3551.9222   Marginal loss: 0.4050   Reconstruction loss: 7103033.9773\n",
            "Accuracy: 3442/4200 0.8195\n",
            "\n",
            "\n",
            "VALID loss: 3530.7629   Marginal loss: 0.4050   Reconstruction loss: 7060715.4318\n",
            "Accuracy: 1136/1400 0.8114\n",
            "\n",
            "Epoch: 18\n",
            "Learning rate: 3.97e-07\n",
            "\n",
            "TRAIN loss: 3552.9931   Marginal loss: 0.4050   Reconstruction loss: 7105175.8939\n",
            "Accuracy: 3421/4200 0.8145\n",
            "\n",
            "\n",
            "VALID loss: 3530.6053   Marginal loss: 0.4050   Reconstruction loss: 7060400.0795\n",
            "Accuracy: 1127/1400 0.8050\n",
            "\n",
            "Epoch: 20\n",
            "Learning rate: 3.58e-07\n",
            "\n",
            "TRAIN loss: 3553.2632   Marginal loss: 0.4050   Reconstruction loss: 7105715.9621\n",
            "Accuracy: 3394/4200 0.8081\n",
            "\n",
            "\n",
            "VALID loss: 3530.4582   Marginal loss: 0.4050   Reconstruction loss: 7060105.9659\n",
            "Accuracy: 1119/1400 0.7993\n",
            "\n",
            "Epoch: 22\n",
            "Learning rate: 3.24e-07\n",
            "\n",
            "TRAIN loss: 3551.8591   Marginal loss: 0.4050   Reconstruction loss: 7102907.7500\n",
            "Accuracy: 3374/4200 0.8033\n",
            "\n",
            "\n",
            "VALID loss: 3530.3220   Marginal loss: 0.4050   Reconstruction loss: 7059833.5227\n",
            "Accuracy: 1120/1400 0.8000\n",
            "\n",
            "Epoch: 24\n",
            "Learning rate: 2.92e-07\n",
            "\n",
            "TRAIN loss: 3549.5663   Marginal loss: 0.4050   Reconstruction loss: 7098322.2424\n",
            "Accuracy: 3357/4200 0.7993\n",
            "\n",
            "\n",
            "VALID loss: 3530.1965   Marginal loss: 0.4050   Reconstruction loss: 7059582.6250\n",
            "Accuracy: 1114/1400 0.7957\n",
            "\n",
            "Epoch: 26\n",
            "Learning rate: 2.64e-07\n",
            "\n",
            "TRAIN loss: 3550.9682   Marginal loss: 0.4050   Reconstruction loss: 7101126.0265\n",
            "Accuracy: 3314/4200 0.7890\n",
            "\n",
            "\n",
            "VALID loss: 3530.0813   Marginal loss: 0.4050   Reconstruction loss: 7059352.1023\n",
            "Accuracy: 1099/1400 0.7850\n",
            "\n",
            "Epoch: 28\n",
            "Learning rate: 2.38e-07\n",
            "\n",
            "TRAIN loss: 3554.0460   Marginal loss: 0.4050   Reconstruction loss: 7107281.5947\n",
            "Accuracy: 3275/4200 0.7798\n",
            "\n",
            "\n",
            "VALID loss: 3529.9758   Marginal loss: 0.4050   Reconstruction loss: 7059141.2159\n",
            "Accuracy: 1067/1400 0.7621\n",
            "\n",
            "Epoch: 30\n",
            "Learning rate: 2.15e-07\n",
            "\n",
            "TRAIN loss: 3549.6053   Marginal loss: 0.4050   Reconstruction loss: 7098400.1667\n",
            "Accuracy: 3278/4200 0.7805\n",
            "\n",
            "\n",
            "VALID loss: 3529.8796   Marginal loss: 0.4050   Reconstruction loss: 7058948.9091\n",
            "Accuracy: 1066/1400 0.7614\n",
            "\n",
            "Epoch: 32\n",
            "Learning rate: 1.94e-07\n",
            "\n",
            "TRAIN loss: 3552.5784   Marginal loss: 0.4050   Reconstruction loss: 7104346.3258\n",
            "Accuracy: 3260/4200 0.7762\n",
            "\n",
            "\n",
            "VALID loss: 3529.7921   Marginal loss: 0.4050   Reconstruction loss: 7058773.8523\n",
            "Accuracy: 1055/1400 0.7536\n",
            "\n",
            "Epoch: 34\n",
            "Learning rate: 1.75e-07\n",
            "\n",
            "TRAIN loss: 3550.0894   Marginal loss: 0.4050   Reconstruction loss: 7099368.3258\n",
            "Accuracy: 3215/4200 0.7655\n",
            "\n",
            "\n",
            "VALID loss: 3529.7124   Marginal loss: 0.4050   Reconstruction loss: 7058614.3523\n",
            "Accuracy: 1056/1400 0.7543\n",
            "\n",
            "Epoch: 36\n",
            "Learning rate: 1.58e-07\n",
            "\n",
            "TRAIN loss: 3549.4532   Marginal loss: 0.4050   Reconstruction loss: 7098095.9091\n",
            "Accuracy: 3180/4200 0.7571\n",
            "\n",
            "\n",
            "VALID loss: 3529.6401   Marginal loss: 0.4050   Reconstruction loss: 7058469.7045\n",
            "Accuracy: 1046/1400 0.7471\n",
            "\n",
            "Epoch: 38\n",
            "Learning rate: 1.42e-07\n",
            "\n",
            "TRAIN loss: 3549.6048   Marginal loss: 0.4050   Reconstruction loss: 7098399.1402\n",
            "Accuracy: 3172/4200 0.7552\n",
            "\n",
            "\n",
            "VALID loss: 3529.5743   Marginal loss: 0.4050   Reconstruction loss: 7058338.2614\n",
            "Accuracy: 1036/1400 0.7400\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 333
        },
        "id": "JNeQe9UX0Z0H",
        "outputId": "65871ed0-4b1f-4972-a78f-f802e8312a36"
      },
      "source": [
        "evaluate(opt, valid_loader, model, 'VALID', True)"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "VALID loss: 3529.5438   Marginal loss: 0.4050   Reconstruction loss: 7058277.3068\n",
            "Accuracy: 1038/1400 0.7414\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a9QasPoHDSO9"
      },
      "source": [
        "## Training MNIST\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BKAj4LCUAvm-"
      },
      "source": [
        "def get_MNIST_dataloader(opt):\n",
        "    # MNIST Dataset\n",
        "    train_dataset = datasets.MNIST(root='./data/',\n",
        "                                   train=True,\n",
        "                                   transform=transforms.ToTensor(),\n",
        "                                   download=True)\n",
        "\n",
        "    test_dataset = datasets.MNIST(root='./data/',\n",
        "                                  train=False,\n",
        "                                  transform=transforms.ToTensor())\n",
        "\n",
        "    # Data Loader (Input Pipeline)\n",
        "    train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
        "                                               batch_size=opt.batch_size,\n",
        "                                               shuffle=True)\n",
        "\n",
        "    test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
        "                                              batch_size=opt.batch_size,\n",
        "                                              shuffle=True)\n",
        "\n",
        "    return train_loader, test_loader"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7oGMCHfVBH-o"
      },
      "source": [
        "def train_MNIST(opt, train_loader, valid_loader, model):\n",
        "    num_sample = len(train_loader.dataset)\n",
        "    num_batches = len(train_loader)\n",
        "    train_loss_list = []\n",
        "    loss_val = 0.\n",
        "\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=opt.lr)\n",
        "    scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, opt.gamma)\n",
        "    model.train()\n",
        "    for epoch in range(opt.epochs):\n",
        "    \n",
        "        for batch_idx, (data, target) in enumerate(train_loader):\n",
        "            optimizer.zero_grad()\n",
        "            batch_size = data.size(0)\n",
        "            target = torch.one_hot((batch_size, 10), target.view(-1, 1))\n",
        "            assert target.size() == torch.Size([batch_size, opt.n_classes]), target.size()\n",
        "\n",
        "            # Use GPU if available\n",
        "            with torch.no_grad():\n",
        "                data, target = Variable(data), Variable(target)\n",
        "            if opt.use_cuda & torch.cuda.is_available():\n",
        "                data, target = data.cuda(), target.cuda()\n",
        "            \n",
        "            output = model(data)\n",
        "            loss, marginal_loss, reconstruction_loss = model.loss(output, data, target)\n",
        "            loss_val = loss.item()\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        if epoch % 5 == 0: \n",
        "            train_loss_list.append(loss_val)\n",
        "        if epoch % 2 == 0:\n",
        "            print('Epoch: {}'.format(epoch))\n",
        "            print('Learning rate: {:.2e}'.format(scheduler.get_last_lr()[0]))\n",
        "            evaluate_MNIST(opt, train_loader, model, 'TRAIN', False) \n",
        "            evaluate_MNIST(opt, valid_loader, model, 'VALID', False) \n",
        "        torch.save({\n",
        "            'epoch': epoch,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'loss': loss,\n",
        "            }, 'guide_model_mnist.pt')\n",
        "        scheduler.step()\n",
        "    fig = plt.figure()\n",
        "    plt.plot([i for i in range(len(train_loss_list))], train_loss_list, '-')"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cjrezNwwBVz-"
      },
      "source": [
        "def evaluate_MNIST(opt, valid_loader, model, type_data, plot=False):\n",
        "    sum_loss = 0\n",
        "    sum_marginal_loss = 0\n",
        "    sum_reconstruction_loss = 0\n",
        "    correct = 0\n",
        "    num_sample = len(valid_loader.dataset)\n",
        "    num_batch = len(valid_loader)\n",
        "\n",
        "    model.eval()\n",
        "    for data, target in valid_loader:\n",
        "        batch_size = data.size(0)\n",
        "        target = torch.one_hot((batch_size, 10), target.view(-1, 1))\n",
        "        assert target.size() == torch.Size([batch_size, opt.n_classes])\n",
        "\n",
        "        with torch.no_grad():\n",
        "            data, target = Variable(data), Variable(target)\n",
        "        if opt.use_cuda & torch.cuda.is_available():\n",
        "            data, target = data.cuda(), target.cuda()\n",
        "\n",
        "        output = model(data)  # (batch_size, n_classes, 16)\n",
        "        loss, marginal_loss, reconstruction_loss = model.loss(output, data, target)\n",
        "        sum_loss += loss.item()\n",
        "        sum_marginal_loss += marginal_loss.item()\n",
        "        sum_reconstruction_loss += reconstruction_loss.item()\n",
        "\n",
        "        norms = torch.sqrt(torch.sum(output**2, dim=2))  # (batch_size, n_classes)\n",
        "        pred = norms.data.max(1, keepdim=True)[1].type(torch.LongTensor)  # (batch_size, 1)\n",
        "        label = target.max(1, keepdim=True)[1].type(torch.LongTensor)  # (batch_size, 1)\n",
        "        correct += pred.eq(label.view_as(pred)).cpu().sum().item()\n",
        "\n",
        "    if plot:\n",
        "        recons = model.Decoder(output, target)\n",
        "        recons = recons.view(batch_size, opt.image_size, opt.image_size)\n",
        "        recons = recons[0].cpu()\n",
        "        if (correct / float(num_sample)) > 0.703: plt.imshow(recons.detach(), cmap = \"gray\")\n",
        "\n",
        "    sum_loss /= num_batch\n",
        "    sum_marginal_loss /= num_batch\n",
        "    sum_reconstruction_loss /= num_batch\n",
        "    print('\\n{} loss: {:.4f}   Marginal loss: {:.4f}   Reconstruction loss: {:.4f}'.format(\n",
        "        type_data, sum_loss, sum_marginal_loss, sum_reconstruction_loss))\n",
        "    print('Accuracy: {}/{} {:.4f}\\n'.format(correct, num_sample,\n",
        "        correct / num_sample))"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w6V6HNPADUh4"
      },
      "source": [
        "def get_opts_MNIST():\n",
        "    parser = argparse.ArgumentParser(description='CapsuleNetwork')\n",
        "    parser.add_argument('-batch_size', type=int, default=32)\n",
        "    parser.add_argument('-lr', type=float, default=1e-6)\n",
        "    parser.add_argument('-epochs', type=int, default=20)\n",
        "    parser.add_argument('-image_size', type=int, default=28)\n",
        "    parser.add_argument('-n_classes', type=int, default=10)\n",
        "    parser.add_argument('-use_cuda', default=True)\n",
        "    parser.add_argument('-gamma', type=float, default=0.8)\n",
        "    parser.add_argument('-r', type=int, default=3)\n",
        "    opt, _ = parser.parse_known_args()\n",
        "    return opt"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4zXW1i3p1uq8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d84e957f-c85e-490d-c92f-6ce663d1e7b9"
      },
      "source": [
        "opt = get_opts_MNIST()\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "model = CapsuleNetwork(opt)\n",
        "if opt.use_cuda & torch.cuda.is_available():\n",
        "    model.cuda()\n",
        "\n",
        "setattr(torch, 'one_hot', torch_extras.one_hot)\n",
        "train_loader, valid_loader = get_MNIST_dataloader(opt)\n",
        "train_MNIST(opt, train_loader, valid_loader, model)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch_extras/extras.py:117: UserWarning: volatile was removed (Variable.volatile is always False)\n",
            "  mask = Variable(mask, volatile=index.volatile)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jXTaG31KxcXr"
      },
      "source": [
        "evaluate_MNIST(opt, valid_loader, model, 'VALID', True)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}